{"id":929,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-probit-regression\/"},"modified":"2016-02-11T19:13:41","modified_gmt":"2016-02-11T19:13:41","slug":"probit-regression","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/probit-regression\/","title":{"rendered":"Probit Regression |  R Data Analysis Examples"},"content":{"rendered":"<p><!DOCTYPE html><\/p>\n<p><html><\/p>\n<p><head><\/p>\n<style type=\"text\/css\">.knitr.inline {\n  background-color: #f7f7f7;\n  border:solid 1px #B0B0B0;\n}\n.error {\n\tfont-weight: bold;\n\tcolor: #FF0000;\n}, \n.warning {\n\tfont-weight: bold;\n} \n.message {\n\tfont-style: italic;\n} \n.source, .output, .warning, .error, .message {\n\tpadding: 0em 1em;\n  border:solid 1px #F7F7F7;\n}\n.source {\n  background-color: #f7f7f7;\n}\n.rimage.left {\n  text-align: left;\n}\n.rimage.right {\n  text-align: right;\n}\n.rimage.center {\n  text-align: center;\n}<\/p>\n<p>.source {\n  color: #333333;\n}\n.background {\n  color: #F7F7F7;\n}<\/p>\n<p>.number {\n  color: #000000;\n}<\/p>\n<p>.functioncall {\n  color: #800054;\n  font-weight: bolder;\n}<\/p>\n<p>.string {\n  color: #9999FF;\n}<\/p>\n<p>.keyword {\n  font-weight: bolder;\n  color: black;\n}<\/p>\n<p>.argument {\n  color: #B04005;\n}<\/p>\n<p>.comment {\n  color: #2E9957;\n}<\/p>\n<p>.roxygencomment {\n  color: #707AB3;\n}<\/p>\n<p>.formalargs {\n  color: #B04005;\n}<\/p>\n<p>.eqformalargs {\n  color: #B04005;\n}<\/p>\n<p>.assignement {\n  font-weight: bolder;\n  color: #000000;\n}<\/p>\n<p>.package {\n  color: #96B525;\n}<\/p>\n<p>.slot {\n  font-style: italic;\n}<\/p>\n<p>.symbol {\n  color: #000000;\n}<\/p>\n<p>.prompt {\n  color: #333333;\n}\n<\/style>\n<p><title><\/title>\n<link href=\"\/stat\/ats_style.css\" type=\"text\/css\" rel=\"stylesheet\">\n<!--modified 7\/12\/10--ram-->\n<!-- updated 7\/18\/12 jfw -->\n<\/head><\/p>\n<p><body>\n<?php include \"stat\/header.htm\"; ?><\/p>\n<p>Probit regression, also called a probit model, is used to model dichotomous\nor binary outcome variables. In the probit model, the inverse standard normal distribution of the probability is modeled\nas a linear combination of the predictors.<\/p>\n<p>This page uses the following packages. Make sure that you can load \nthem before trying to run the examples on this page. If you do not have \na package installed, run: <code>install.packages(\"packagename\")<\/code>, or \nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">require<\/span>(aod)\r\n<span class=\"functioncall\">require<\/span>(ggplot2)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><b>Version info: <\/b>Code for this page was tested in R Under development (unstable) (2012-11-16 r61126)<br \/>\n On: 2012-12-15<br \/>\nWith: ggplot2 0.9.3; aod 1.3; knitr 0.9\n<\/code><\/p>\n<p><strong>Please Note:<\/strong>  The purpose of this page is to show how to use various data analysis commands.\nIt does not cover all aspects of the research process which researchers are expected to do.  In\nparticular, it does not cover data cleaning and checking, verification of assumptions, model\ndiagnostics and potential follow-up analyses.<\/p>\n<h2>Examples<\/h2>\n<p>Example 1: Suppose that we are interested in the factors that influence\nwhether a political candidate wins an election. The outcome (response) variable\nis binary (0\/1); win or lose. The predictor variables of interest are the\namount of money spent on the campaign, the amount of time spent campaigning\nnegatively and whether the candidate is an incumbent.<\/p>\n<p>Example 2: A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA\n(grade point average) and prestige of the undergraduate institution, effect\nadmission into graduate school. The response variable, admit\/don&#8217;t admit, is a\nbinary variable.<\/p>\n<h2>Description of the Data<\/h2>\n<p>For our data analysis below, we are going to expand on Example 2 about getting\ninto graduate school. We have generated hypothetical data, which\ncan be obtained from our website in R. Note that <em>R requires forward slashes\n<\/em>(\/) not back slashes () when specifying a file location even if the file is\non your hard drive.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">mydata &lt;- <span class=\"functioncall\">read.csv<\/span>(<span class=\"string\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/binary.csv\"<\/span>)\r\n\r\n## convert rank to a <span class=\"functioncall\">factor<\/span> (categorical variable)\r\nmydata$rank &lt;- <span class=\"functioncall\">factor<\/span>(mydata$rank)\r\n\r\n<span class=\"comment\">## view first few rows<\/span>\r\n<span class=\"functioncall\">head<\/span>(mydata)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   admit gre  gpa rank\r\n## 1     0 380 3.61    3\r\n## 2     1 660 3.67    3\r\n## 3     1 800 4.00    1\r\n## 4     1 640 3.19    4\r\n## 5     0 520 2.93    4\r\n## 6     1 760 3.00    2\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>This data set has a binary response (outcome, dependent) variable called <code>admit<\/code>.\nThere are three predictor variables: <code>gre<\/code>, <code>gpa<\/code> and <code>rank<\/code>. We will treat the\nvariables <code>gre<\/code> and <code>gpa<\/code> as continuous. The variable <code>rank<\/code> takes on the\nvalues 1 through 4. Institutions with a rank of 1 have the highest prestige,\nwhile those with a rank of 4 have the lowest.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">summary<\/span>(mydata)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##      admit            gre           gpa       rank   \r\n##  Min.   :0.000   Min.   :220   Min.   :2.26   1: 61  \r\n##  1st Qu.:0.000   1st Qu.:520   1st Qu.:3.13   2:151  \r\n##  Median :0.000   Median :580   Median :3.40   3:121  \r\n##  Mean   :0.318   Mean   :588   Mean   :3.39   4: 67  \r\n##  3rd Qu.:1.000   3rd Qu.:660   3rd Qu.:3.67          \r\n##  Max.   :1.000   Max.   :800   Max.   :4.00\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">xtabs<\/span>(~rank + admit, data = mydata)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##     admit\r\n## rank  0  1\r\n##    1 28 33\r\n##    2 97 54\r\n##    3 93 28\r\n##    4 55 12\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<h2>Analysis methods you might consider<\/h2>\n<p>Below is a list of some analysis methods you may have encountered.\nSome of the methods listed are quite reasonable while others have either\nfallen out of favor or have limitations.<\/p>\n<ul>\n<li>Probit regression, the focus of this page.<\/li>\n<li>Logistic regression. A logit model will produce results similar\n  probit regression. The choice of probit versus logit depends largely on\n  individual preferences. <\/li>\n<li>OLS regression. When used with a binary response variable, this model is known\n  as a linear probability model and can be used as a way to\n  describe conditional probabilities. However, the errors (i.e., residuals)\n  from the linear probability model violate the homoskedasticity and\n  normality of errors assumptions of OLS\n  regression, resulting in invalid standard errors and hypothesis tests. For\n  a more thorough discussion of these and other problems with the linear\n  probability model, see Long (1997, p. 38-40).<\/li>\n<li>Two-group discriminant function analysis. A multivariate method for\n  dichotomous outcome variables.<\/li>\n<li>Hotelling&#8217;s T<sup>2<\/sup>. The 0\/1 outcome is turned into the\n  grouping variable, and the former predictors are turned into outcome\n  variables. This will produce an overall test of significance but will not\n  give individual coefficients for each variable, and it is unclear the extent\n  to which each &#8220;predictor&#8221; is adjusted for the impact of the other\n  &#8220;predictors&#8221;.<\/li>\n<\/ul>\n<h2>Using the Probit Model<\/h2>\n<p>The code below estimates a probit regression model using the glm (generalized linear model) function.\nSince we stored our model output in the object &#8220;myprobit&#8221;, R will not print anything to the console. We\ncan use the <code>summary<\/code> function to get a summary of the\n  model and all the estimates.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">myprobit &lt;- <span class=\"functioncall\">glm<\/span>(admit ~ gre + gpa + rank, family = <span class=\"functioncall\">binomial<\/span>(link = <span class=\"string\">\"probit\"<\/span>), \r\n    data = mydata)\r\n\r\n<span class=\"comment\">## model summary<\/span>\r\n<span class=\"functioncall\">summary<\/span>(myprobit)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## glm(formula = admit ~ gre + gpa + rank, family = binomial(link = \"probit\"), \r\n##     data = mydata)\r\n## \r\n## Deviance Residuals: \r\n##    Min      1Q  Median      3Q     Max  \r\n## -1.616  -0.871  -0.639   1.156   2.103  \r\n## \r\n## Coefficients:\r\n##             Estimate Std. Error z value Pr(>|z|)    \r\n## (Intercept) -2.38684    0.67395   -3.54  0.00040 ***\r\n## gre          0.00138    0.00065    2.12  0.03433 *  \r\n## gpa          0.47773    0.19720    2.42  0.01541 *  \r\n## rank2       -0.41540    0.19498   -2.13  0.03313 *  \r\n## rank3       -0.81214    0.20836   -3.90  9.7e-05 ***\r\n## rank4       -0.93590    0.24527   -3.82  0.00014 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \r\n## \r\n## (Dispersion parameter for binomial family taken to be 1)\r\n## \r\n##     Null deviance: 499.98  on 399  degrees of freedom\r\n## Residual deviance: 458.41  on 394  degrees of freedom\r\n## AIC: 470.4\r\n## \r\n## Number of Fisher Scoring iterations: 4\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<ul>\n<li>In the output above, the first thing we see is the call,\n  this is R reminding us what the model we ran was, what options we specified, etc.<\/li>\n<li>Next we see the deviance residuals, which are a measure of model fit. This part\n  of output shows the distribution of the deviance residuals for individual cases used\n  in the model. Below we discuss how to use summaries of the deviance statistic to asses model fit.<\/li>\n<li>The next part of the output shows the coefficients, their standard errors,\n  the z-statistic (sometimes called a Wald z-statistic), and the associated\n  p-values.  Both <code>gre<\/code>, <code>gpa<\/code>, and the three terms for\n  <code>rank<\/code> are statistically significant. The probit\n  regression coefficients give the change in the z-score or probit\n  index for a one unit change in the predictor. <\/p>\n<ul>\n<li>For a one unit increase in <code>gre<\/code>, the z-score increases by 0.001.<\/li>\n<li>For each one unit increase in <code>gpa<\/code>, the z-score increases by 0.478.<\/li>\n<li>The indicator variables for <code>rank<\/code> have a slightly different interpretation. For example,\n    having attended an undergraduate institution of <code>rank<\/code> of 2, versus an institution with a\n    <code>rank<\/code> of 1 (the reference group), decreases the z-score by 0.415.<\/li>\n<\/ul>\n<\/li>\n<li>Below the table of coefficients are fit indices, including the null and deviance residuals and the AIC.\n    Later we show an example of how you can use these values to help assess model fit.<\/li>\n<\/ul>\n<p>We can use the <code>confint<\/code> function to obtain confidence intervals for the coefficient estimates.\nThese will be profiled confidence intervals by default, created by profiling the likelihood function.  As\nsuch, they are not necessarily symmetric.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">confint<\/span>(myprobit)\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## Waiting for profiling to be done...\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                  2.5 %    97.5 %\r\n## (Intercept) -3.7201051 -1.076328\r\n## gre          0.0001104  0.002655\r\n## gpa          0.0960655  0.862610\r\n## rank2       -0.7992114 -0.032995\r\n## rank3       -1.2230956 -0.405008\r\n## rank4       -1.4234218 -0.459539\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can test for an overall effect of <code>rank<\/code> using the <code>wald.test<\/code>\nfunction of the <code>aod<\/code> library. The\norder in which the coefficients are given in the table of coefficients is the\nsame as the order of the terms in the model. This is important because the <code>\nwald.test<\/code> function refers to the coefficients by their order in\n  the model. \nWe use the <code>wald.test<\/code> function. <code>b<\/code>\nsupplies the coefficients, while <code>Sigma<\/code> supplies the variance covariance\nmatrix of the error terms, finally <code>Terms<\/code> tells R which terms in the model\nare to be tested, in this case, terms 4, 5, and 6, are the three terms for the\nlevels of <code>rank<\/code>.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">wald.test<\/span>(b = <span class=\"functioncall\">coef<\/span>(myprobit), Sigma = <span class=\"functioncall\">vcov<\/span>(myprobit), Terms = 4:6)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## Wald test:\r\n## ----------\r\n## \r\n## Chi-squared test:\r\n## X2 = 21.4, df = 3, P(> X2) = 8.9e-05\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The chi-squared test statistic of 21.4 with three degrees of freedom is\nassociated with a p-value of less than 0.001 indicating that the overall effect of <code>\nrank<\/code> is statistically significant.<\/p>\n<p>We can also test additional hypotheses about the differences in the\ncoefficients for different levels of rank. Below we\ntest that the coefficient for <code>rank<\/code>=2 is equal to the coefficient for <code>rank<\/code>=3.\nThe first line of code below creates a vector <code>l<\/code> that defines the test we\nwant to perform. In this case, we want to test the difference (subtraction) of\nthe terms for <code>rank<\/code>=2 and <code>rank<\/code>=3 (i.e. the 4th and 5th terms in the\nmodel). To contrast these two terms, we multiply one of them by 1, and the other\nby -1. The other terms in the model are not involved in the test, so they are\nmultiplied by 0. The second line of code below uses <code>L=l<\/code> to tell R that we\nwish to base the test on the vector <code>l<\/code> (rather than using the Terms option\nas we did above).<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">l &lt;- <span class=\"functioncall\">cbind<\/span>(0, 0, 0, 1, -1, 0)\r\n<span class=\"functioncall\">wald.test<\/span>(b = <span class=\"functioncall\">coef<\/span>(myprobit), Sigma = <span class=\"functioncall\">vcov<\/span>(myprobit), L = l)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## Wald test:\r\n## ----------\r\n## \r\n## Chi-squared test:\r\n## X2 = 5.6, df = 1, P(> X2) = 0.018\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The chi-squared test statistic of 5.5 with 1 degree of freedom is associated with\na p-value of 0.019, indicating that the difference between the coefficient for <code>rank<\/code>=2\nand the coefficient for <code>rank<\/code>=3 is statistically significant.<\/p>\n<p>You can also use predicted probabilities to help you understand the model.\nTo do this, we first create a data frame containing\nthe values we want for the independent variables.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">newdata &lt;- <span class=\"functioncall\">data.frame<\/span>(gre = <span class=\"functioncall\">rep<\/span>(<span class=\"functioncall\">seq<\/span>(from = 200, to = 800, length.out = 100), \r\n    4 * 4), gpa = <span class=\"functioncall\">rep<\/span>(<span class=\"functioncall\">c<\/span>(2.5, 3, 3.5, 4), each = 100 * 4), rank = <span class=\"functioncall\">factor<\/span>(<span class=\"functioncall\">rep<\/span>(<span class=\"functioncall\">rep<\/span>(1:4, \r\n    each = 100), 4)))\r\n\r\n<span class=\"functioncall\">head<\/span>(newdata)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##     gre gpa rank\r\n## 1 200.0 2.5    1\r\n## 2 206.1 2.5    1\r\n## 3 212.1 2.5    1\r\n## 4 218.2 2.5    1\r\n## 5 224.2 2.5    1\r\n## 6 230.3 2.5    1\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Now we can predict the probabilities for our input data as well as their standard errors.\nThese are stored as new variable in the data frame with the original data, so we can\nplot the predicted probabilities for different <code>gre<\/code> scores.  We create four plots,\none for each level of <code>gpa<\/code> we used (2.5, 3, 3.5, 4) with the colour of the lines\nindicating the rank the predicted probabilities were for.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">newdata[, <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"p\"<\/span>, <span class=\"string\">\"se\"<\/span>)] &lt;- <span class=\"functioncall\">predict<\/span>(myprobit, newdata, type = <span class=\"string\">\"response\"<\/span>, se.fit = <span class=\"keyword\">TRUE<\/span>)[-3]\r\n\r\n<span class=\"functioncall\">ggplot<\/span>(newdata, <span class=\"functioncall\">aes<\/span>(x = gre, y = p, colour = rank)) + <span class=\"functioncall\">geom_line<\/span>() + <span class=\"functioncall\">facet_wrap<\/span>(~gpa)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/probit-unnamed-chunk-10.png\" alt=\"plot of chunk unnamed-chunk-10\" width=\"500px\" height=\"500px\" class=\"plot\" \/><\/div>\n<\/div>\n<p>We may also wish to see measures of how well our model fits. This can be\nparticularly useful when comparing competing models. The output produced by <code>\nsummary(mylogit)<\/code> included indices of fit (shown below the coefficients), including the null and\ndeviance residuals and the AIC. One measure of model fit is the significance of\nthe overall model. This test asks whether the model with predictors fits\nsignificantly better than a model with just an intercept (i.e. a null model).\nThe test statistic is the difference between the residual deviance for the model\nwith predictors and the null model. The test statistic is distributed\nchi-squared with degrees of freedom equal to the differences in degrees of freedom between\nthe current and the null model (i.e. the number of predictor variables in the\nmodel). To find the difference in deviance for the two models (i.e. the test\nstatistic) we can compute the change in deviance, and test it using a chi square test&#8212;the\nchange in deviance distributed as chi square on the change in degrees\n  of freedom.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## change <span class=\"keyword\">in<\/span> deviance<\/span>\r\n<span class=\"functioncall\">with<\/span>(myprobit, null.deviance - deviance)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 41.56\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\">\r\n<span class=\"comment\">## change <span class=\"keyword\">in<\/span> degrees of freedom<\/span>\r\n<span class=\"functioncall\">with<\/span>(myprobit, df.null - df.residual)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 5\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\">\r\n<span class=\"comment\">## chi square test p-value<\/span>\r\n<span class=\"functioncall\">with<\/span>(myprobit, <span class=\"functioncall\">pchisq<\/span>(null.deviance - deviance, df.null - df.residual, lower.tail = <span class=\"keyword\">FALSE<\/span>))\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 7.219e-08\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The chi-square of 41.56 with 5 degrees of freedom and an associated p-value of less than 0.001 tells us that our model as a whole fits\nsignificantly better than an empty model. This is sometimes called a likelihood\nratio test (the deviance residual is -2*log likelihood). To see the\n  model&#8217;s log likelihood, we type:<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">logLik<\/span>(myprobit)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## 'log Lik.' -229.2 (df=6)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<h2>Things to consider<\/h2>\n<ul>\n<li>Empty cells or small cells: You should check for empty or small\n  cells by doing a crosstab between categorical predictors and the outcome\n  variable. If a cell has very few cases (a small cell), the model may\n  become unstable or it might not run at all. <\/li>\n<li>Separation or quasi-separation (also called perfect prediction), a\n  condition in which the outcome does not vary at some levels of the\n  independent variables. See our page <a href=\"https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faqwhat-is-complete-or-quasi-complete-separation-in-logisticprobit-regression-and-how-do-we-deal-with-them\/\">\n  FAQ: What is complete or quasi-complete separation in logistic\/probit\n  regression and how do we deal with them?<\/a> for information on models with\n  perfect prediction.<\/li>\n<li>Sample size: Both probit and logit models require more cases than\n  OLS regression because they use maximum likelihood estimation techniques. It\n  is sometimes possible to estimate models for binary outcomes in datasets\n  with only a small number of cases using exact logistic regression. It is also important to keep in mind that\n  when the outcome is rare, even if the overall dataset is large, it can be\n  difficult to estimate a probit model.<\/li>\n<li>Pseudo-R-squared: Many different measures of psuedo-R-squared\n  exist. They all attempt to provide information similar to that provided by\n  R-squared in OLS regression; however, none of them can be interpreted\n  exactly as R-squared in OLS regression is interpreted. For a discussion of\n  various pseudo-R-squareds see Long and Freese (2006) or our FAQ page\n  <a href=\"https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faq-what-are-pseudo-r-squareds\/\">What are\n  pseudo R-squareds?<\/a><\/li>\n<li>Diagnostics: The diagnostics for probit regression are different\n  from those for OLS regression. The diagnostics for probit models are similar\n  to those for logit models. For a discussion of model diagnostics for\n  logistic regression, see Hosmer and Lemeshow (2000, Chapter 5).<\/li>\n<\/ul>\n<h2>References<\/h2>\n<ul>\n<li>Hosmer, D. &#038; Lemeshow, S. (2000).  Applied Logistic Regression (Second Edition).\n    New York: John Wiley &#038; Sons, Inc.<\/li>\n<li>Long, J. Scott (1997). Regression Models for Categorical and Limited Dependent Variables.\n\tThousand Oaks, CA: Sage Publications.<\/li>\n<\/ul>\n<h2>See Also<\/h2>\n<ul>\n<li>R Online Manual: <a href=\"http:\/\/stat.ethz.ch\/R-manual\/R-patched\/library\/stats\/html\/glm.html\">\n      glm<\/a><\/li>\n<li><a href=\"\/examples\/alr2\/\">Applied Logistic\n  Regression (Second Edition)<\/a> by David Hosmer and Stanley Lemeshow\n  <\/li>\n<li><a href=\"\/books\/#Logistic Regression and Related Methods\">\n  Stat Books for Loan, Logistic Regression and Limited Dependent Variables<\/a>\n  <\/li>\n<li>Everitt, B. S. and Hothorn, T.\n\t<a href=\"http:\/\/cran.r-project.org\/web\/packages\/HSAUR\/vignettes\/preface.pdf\">\n\tA Handbook of Statistical Analyses Using R<\/a>\n  <\/li>\n<\/ul>\n<p><?php include \"stat\/footer.htm\"; ?><\/p>\n<p><\/body>\n<\/html><\/p>\n","protected":false},"excerpt":{"rendered":"<p>Probit regression, also called a probit model, is used to model dichotomous or binary outcome variables. In the probit model, the inverse standard normal distribution of the probability is modeled&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/probit-regression\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":11,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-929","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/929","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=929"}],"version-history":[{"count":1,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/929\/revisions"}],"predecessor-version":[{"id":6594,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/929\/revisions\/6594"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=929"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}