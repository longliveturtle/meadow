{"id":938,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-zero-truncated-negative-binomial\/"},"modified":"2024-06-24T11:09:47","modified_gmt":"2024-06-24T18:09:47","slug":"zero-truncated-negative-binomial","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/zero-truncated-negative-binomial\/","title":{"rendered":"Zero-Truncated Negative Binomial |  R Data Analysis Examples"},"content":{"rendered":"<p>Zero-truncated negative binomial regression is used to model count data for which the value zero cannot occur and for which over dispersion\nexists.<\/p>\n<p>This page uses the following packages. Make sure that you can load\nthem before trying to run the examples on this page. If you do not have\na package installed, run: <code>install.packages(\"packagename\")<\/code>, or\nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">require<\/span>(foreign)\r\n<span class=\"functioncall\">require<\/span>(ggplot2)\r\n<span class=\"functioncall\">require<\/span>(VGAM)\r\n<span class=\"functioncall\">require<\/span>(boot)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><b>Version info: <\/b>Code for this page was tested in R Under development (unstable) (2012-11-16 r61126)\nOn: 2012-12-15\nWith: boot 1.3-7; VGAM 0.9-0; ggplot2 0.9.3; foreign 0.8-51; knitr 0.9\n<\/code><\/p>\n<p><strong>Please Note:<\/strong> The purpose of this page is to show how to use various data analysis commands.\nIt does not cover all aspects of the research process which researchers are expected to do. In\nparticular, it does not cover data cleaning and verification, verification of assumptions, model\ndiagnostics and potential follow-up analyses.<\/p>\n<h2>Examples of zero-truncated negative binomial<\/h2>\n<p>Example 1. A study of length of hospital stay, in days, as a function\nof age, kind of health insurance and whether or not the patient died while in the hospital.\nLength of hospital stay is recorded as a minimum of at least one day.<\/p>\n<p>Example 2. A study of the number of journal articles published by\ntenured faculty as a function of discipline (fine arts, science, social science,\nhumanities, medical, etc). To get tenure faculty must publish, therefore,\nthere are no tenured faculty with zero publications.<\/p>\n<p>Example 3. A study by the county traffic court on the number of tickets received by teenagers\nas predicted by school performance, amount of driver training and gender. Only individuals\nwho have received at least one citation are in the traffic court files.<\/p>\n<h2>Description of the data<\/h2>\n<p>Let&#8217;s pursue Example 1 from above.<\/p>\n<p>We have a <em>hypothetical<\/em> data file, <code>ztp.dta<\/code> with 1,493 observations.\nThe length of hospital stay variable is <code>stay<\/code>.\nThe variable <code>age<\/code> gives the age group from 1 to 9 which will be treated as\ninterval in this example. The variables <code>hmo<\/code> and <code>died<\/code> are binary indicator variables\nfor HMO insured patients and patients who died while in the hospital, respectively.<\/p>\n<p>Let&#8217;s look at the data. These are the same data as were used in the\n<a href=\"\/r\/dae\/zero-truncated-poisson\/\">ztp<\/a> example.\nWe import the Stata dataset using the <code>foreign<\/code> package.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">dat &lt;- <span class=\"functioncall\">read.dta<\/span>(<span class=\"string\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/ztp.dta\"<\/span>)\r\n\r\ndat &lt;- <span class=\"functioncall\">within<\/span>(dat, {\r\n    hmo &lt;- <span class=\"functioncall\">factor<\/span>(hmo)\r\n    died &lt;- <span class=\"functioncall\">factor<\/span>(died)\r\n})\r\n\r\n<span class=\"functioncall\">summary<\/span>(dat)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##       stay            age       hmo      died   \r\n##  Min.   : 1.00   Min.   :1.00   0:1254   0:981  \r\n##  1st Qu.: 4.00   1st Qu.:4.00   1: 239   1:512  \r\n##  Median : 8.00   Median :5.00                   \r\n##  Mean   : 9.73   Mean   :5.23                   \r\n##  3rd Qu.:13.00   3rd Qu.:6.00                   \r\n##  Max.   :74.00   Max.   :9.00\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Now let&#8217;s look at some graphs of the data conditional on various\ncombinations of the variables to get a sense of how the variables work together.\nWe will use the <code>ggplot2<\/code> package. First we can look at histograms of\n<code>stay<\/code> broken down by <code>hmo<\/code> on the rows and <code>died<\/code> on the columns.\nWe also include the marginal distributions, thus the lower right corner represents\nthe overall histogram. We use a log base 10 scale to approximate the canonical link function of\nthe negative binomial distribution (natural logarithm).<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(dat, <span class=\"functioncall\">aes<\/span>(stay)) + <span class=\"functioncall\">geom_histogram<\/span>() + <span class=\"functioncall\">scale_x_log10<\/span>() + <span class=\"functioncall\">facet_grid<\/span>(hmo ~ \r\n    died, margins = <span class=\"keyword\">TRUE<\/span>, scales = <span class=\"string\">\"free_y\"<\/span>)\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztnb-unnamed-chunk-4.png\" alt=\"plot of chunk unnamed-chunk-4\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>From the histograms, it looks like the density of the distribution,\ndoes vary across levels of <code>hmo<\/code> and <code>died<\/code>, with\nshorter stays for those in HMOs (1) and shorter for those who did die,\nincluding what seems to be an inflated number of 1 day stays.\nTo examine how <code>stay<\/code> varies across age groups, we can use conditional\nviolin plots which show a kernel density estimate of the distribution of stay\nmirrored (hence the violin) and conditional on each age group. To further understand\nthe raw data going into each density estimate, we add raw data on top of the violin plots\nwith a small amount of random noise (jitter) to alleviate over plotting. Finally, to get a\nsense of the overall trend, we add a locally weighted regression line.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(dat, <span class=\"functioncall\">aes<\/span>(<span class=\"functioncall\">factor<\/span>(age), stay)) +\r\n  <span class=\"functioncall\">geom_violin<\/span>() +\r\n  <span class=\"functioncall\">geom_jitter<\/span>(size=1.5) +\r\n  <span class=\"functioncall\">scale_y_log10<\/span>() +\r\n  <span class=\"functioncall\">stat_smooth<\/span>(<span class=\"functioncall\">aes<\/span>(x = age, y = stay, group=1), method=<span class=\"string\">\"loess\"<\/span>)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztnb-unnamed-chunk-5.png\" alt=\"plot of chunk unnamed-chunk-5\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>The distribution of length of stay does not seem to vary much across age groups.\nThis observation from the raw data is corroborated by the relatively flat loess line.\nFinally let&#8217;s look at the proportion of people who lived or died across age groups\nby whether or not they are in HMOs.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(dat, <span class=\"functioncall\">aes<\/span>(age, fill=died)) +\r\n  <span class=\"functioncall\">geom_histogram<\/span>(binwidth=.5, position=<span class=\"string\">\"fill\"<\/span>) +\r\n  <span class=\"functioncall\">facet_grid<\/span>(hmo ~ ., margins=<span class=\"keyword\">TRUE<\/span>)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztnb-unnamed-chunk-6.png\" alt=\"plot of chunk unnamed-chunk-6\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>For the lowest ages, a smaller proportion of people in HMOs died, but\nfor higher ages, there does not seem to be a huge difference, with a\nslightly higher proportion in HMOs dying if anything. Overall, as\nage group increases, the proportion of those dying increases, as expected.<\/p>\n<h2>Analysis methods you might consider<\/h2>\n<p>Below is a list of some analysis methods you may have encountered.\nSome of the methods listed are quite reasonable while others have either fallen out of favor or\nhave limitations.<\/p>\n<ul>\n<li>Zero-truncated Negative Binomial Regression &#8211; The focus of this web page.<\/li>\n<li>Zero-truncated Poisson Regression &#8211; Useful if you have no overdispersion in\nSee the Data Analysis Example for <a href=\"\/r\/dae\/zero-truncated-poisson\/\">ztp<\/a>.<\/li>\n<li>Poisson Regression &#8211; Ordinary Poisson regression will have difficulty with\nzero-truncated data. It will try to predict zero counts even though there are\nno zero values.<\/li>\n<li>Negative Binomial Regression &#8211; Ordinary Negative Binomial regression will have difficulty with\nzero-truncated data. It will try to predict zero counts even though there are\nno zero values.<\/li>\n<li>OLS Regression &#8211; You could try to analyze these data using OLS regression. However, count\ndata are highly non-normal and are not well estimated by OLS regression.<\/li>\n<\/ul>\n<h2>Zero-truncated negative binomial regression<\/h2>\n<p>To fit the zero-truncated negative binomial model, we use the <code>vglm<\/code> function\nin the <code>VGAM<\/code> package. This function fits a very flexible class of models\ncalled vector generalized linear models to a wide range of assumed distributions.\nIn our case, we believe the data come from the negative binomial distribution,\nbut without zeros. Thus the values are strictly positive poisson,\nfor which we use the positive negative binomial family via the\n<code>posnegbinomial<\/code> function passed to <code>vglm<\/code>.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">m1 &lt;- <span class=\"functioncall\">vglm<\/span>(stay ~ age + hmo + died, family = <span class=\"functioncall\">posnegbinomial<\/span>(), data = dat)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] \"head(w)\"\r\n##      [,1]\r\n## [1,]    1\r\n## [2,]    1\r\n## [3,]    1\r\n## [4,]    1\r\n## [5,]    1\r\n## [6,]    1\r\n## [1] \"head(y)\"\r\n##   [,1]\r\n## 1    4\r\n## 2    9\r\n## 3    3\r\n## 4    9\r\n## 5    1\r\n## 6    4\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">summary<\/span>(m1)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## vglm(formula = stay ~ age + hmo + died, family = posnegbinomial(), \r\n##     data = dat)\r\n## \r\n## Pearson Residuals:\r\n##             Min    1Q Median   3Q Max\r\n## log(munb)  -1.4 -0.70  -0.23 0.45 9.8\r\n## log(size) -14.1 -0.27   0.45 0.76 1.0\r\n## \r\n## Coefficients:\r\n##               Estimate Std. Error z value\r\n## (Intercept):1    2.408      0.072    33.6\r\n## (Intercept):2    0.569      0.055    10.4\r\n## age             -0.016      0.013    -1.2\r\n## hmo1            -0.147      0.059    -2.5\r\n## died1           -0.218      0.046    -4.7\r\n## \r\n## Number of linear predictors:  2 \r\n## \r\n## Names of linear predictors: log(munb), log(size)\r\n## \r\n## Dispersion Parameter for posnegbinomial family:   1\r\n## \r\n## Log-likelihood: -4755 on 2981 degrees of freedom\r\n## \r\n## Number of iterations: 4\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The output looks very much like the output from an OLS regression:<\/p>\n<ul>\n<li>It begins by echoing the function call showing us what we modeled.<\/li>\n<li>Next comes the spread of the residuals, there is at least one high\nresidual because the max is much higher than the min.<\/li>\n<li>Following the residuals are the parameter estimates and standard errors. The\nz values (frac{Estimate}{SE}) are also printed. No p values are given, although\nif one wished to assume that the estimates followed the normal distribution, one\ncould easily compute the probability of obtaining that z value. The first intercept\nis what we know as the typical intercept. The second is the over dispersion parameter,\n(alpha).<\/li>\n<li>The number of linear predictors is two, one for the expected mean (lambda) and one for the\nover dispersion.<\/li>\n<li>Next the dispersion parameter is printed, assumed to be one after accounting for overdispersion.<\/li>\n<li>Finally the log likelihood <code class=\"knitr inline\">-4755.2796<\/code> is printed along with the number of iterations\nneeded to reach convergence.<\/li>\n<\/ul>\n<p>Now let&#8217;s look at a plot of the residuals versus fitted values. We add random horizontal\nnoise as well as 50 percent transparency to alleviate over plotting and better see where\n<i>most<\/i> residuals fall. Note that these residuals are for the mean prediction.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">output &lt;- <span class=\"functioncall\">data.frame<\/span>(resid = <span class=\"functioncall\">resid<\/span>(m1)[, 1], fitted = <span class=\"functioncall\">fitted<\/span>(m1))\r\n<span class=\"functioncall\">ggplot<\/span>(output, <span class=\"functioncall\">aes<\/span>(fitted, resid)) + <span class=\"functioncall\">geom_jitter<\/span>(position = <span class=\"functioncall\">position_jitter<\/span>(width = 0.25), \r\n    alpha = 0.5) + <span class=\"functioncall\">stat_smooth<\/span>(method = <span class=\"string\">\"loess\"<\/span>)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztnb-unnamed-chunk-8.png\" alt=\"plot of chunk unnamed-chunk-8\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>The mean is around zero across all the fitted levels it looks like. However,\nthere are some values that look rather extreme. To see if these have much influence,\nwe can fit lines using quantile regression, these lines represent the 75th, 50th, and 25th\npercentiles.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(output, <span class=\"functioncall\">aes<\/span>(fitted, resid)) +\r\n  <span class=\"functioncall\">geom_jitter<\/span>(position=<span class=\"functioncall\">position_jitter<\/span>(width=.25), alpha=.5) +\r\n  <span class=\"functioncall\">stat_quantile<\/span>(method=<span class=\"string\">\"rq\"<\/span>)\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## Smoothing formula not specified. Using: y ~ x\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztnb-unnamed-chunk-9.png\" alt=\"plot of chunk unnamed-chunk-9\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>Here we see the spread narrowing at higher levels. Let&#8217;s cut the data\ninto intervals and check box plots for each. We will get the breaks\nfrom the algorithm for a histogram.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">output &lt;- <span class=\"functioncall\">within<\/span>(output, {\r\n  broken &lt;- <span class=\"functioncall\">cut<\/span>(fitted, <span class=\"functioncall\">hist<\/span>(fitted, plot=<span class=\"keyword\">FALSE<\/span>)$breaks)\r\n})\r\n\r\n<span class=\"functioncall\">ggplot<\/span>(output, <span class=\"functioncall\">aes<\/span>(broken, resid)) +\r\n <span class=\"functioncall\">geom_boxplot<\/span>() +\r\n <span class=\"functioncall\">geom_jitter<\/span>(alpha=.25)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztnb-unnamed-chunk-10.png\" alt=\"plot of chunk unnamed-chunk-10\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>The variance seems to decrease slightly at higher fitted values, except for the\nvery last category (this shown by the hinges of the boxplots).<\/p>\n<ul>\n<li>The value of the coefficient for <code>age<\/code>,\n<code class=\"knitr inline\">-0.0157<\/code> suggests that the log count of\nstay decreases by <code class=\"knitr inline\">0.0157<\/code> for each year increase in age.<\/li>\n<li>The coefficient for <code>hmo<\/code>,\n<code class=\"knitr inline\">-0.1471<\/code> indicates that the log count of stay\nfor HMO patient is <code class=\"knitr inline\">0.1471<\/code> less than for non-HMO patients.<\/li>\n<li>The log count of stay for patients who died while in the hospital was\n<code class=\"knitr inline\">0.2178<\/code> less than those patients who did not die.<\/li>\n<li>The value of the constant <code class=\"knitr inline\">2.4083<\/code> is\nthe log count of the stay when all of the predictors equal zero.<\/li>\n<li>The value of the second intercept, the over dispersion parameter, (alpha)\nis <code class=\"knitr inline\">0.5686<\/code>.<\/li>\n<\/ul>\n<p>To test whether we need to estimate over dispersion, we could fit a zero-truncated\nPoisson model and compare the two.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">m2 &lt;- <span class=\"functioncall\">vglm<\/span>(formula = stay ~ age + hmo + died, family = <span class=\"functioncall\">pospoisson<\/span>(), data = dat)\r\n\r\n<span class=\"comment\">## change <span class=\"keyword\">in<\/span> deviance<\/span>\r\n(dLL &lt;- 2 * (<span class=\"functioncall\">logLik<\/span>(m1) - <span class=\"functioncall\">logLik<\/span>(m2)))\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 4307\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## p-value, 1 df---the overdispersion parameter<\/span>\r\n<span class=\"functioncall\">pchisq<\/span>(dLL, df = 1, lower.tail = <span class=\"keyword\">FALSE<\/span>)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 0\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Based on this, we would conclude that the negative binomial model is\na better fit to the data.<\/p>\n<p>We can get confidence intervals for the parameters and the\nexponentiated parameters using bootstrapping. For the negative\nbinomial model, these would be incident risk ratios.\nWe use the <code>boot<\/code> package.\nFirst, we get the coefficients from our original model to\nuse as start values for the model to speed up the time it takes to estimate. Then\nwe write a short function that takes data and indices as input and returns the\nparameters we are interested in. Finally, we pass that\nto the <code>boot<\/code> function and do 1200 replicates, using snow to distribute across\nfour cores. Note that you should adjust the number of cores to whatever your machine\nhas. Also, for final results, one may wish to increase the number of replications to\nhelp ensure stable results.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">dput<\/span>(<span class=\"functioncall\">round<\/span>(<span class=\"functioncall\">coef<\/span>(m1),3))\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## structure(c(2.408, 0.569, -0.016, -0.147, -0.218), .Names = c(\"(Intercept):1\", \r\n## \"(Intercept):2\", \"age\", \"hmo1\", \"died1\"))\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\">f &lt;- <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(data, i, newdata) {\r\n  <span class=\"functioncall\">require<\/span>(VGAM)\r\n  m &lt;- <span class=\"functioncall\">vglm<\/span>(formula = stay ~ age + hmo + died, family = <span class=\"functioncall\">posnegbinomial<\/span>(),\r\n    data = data[i, ], coefstart = <span class=\"functioncall\">c<\/span>(2.408, 0.569, -0.016, -0.147, -0.218))\r\n  mparams &lt;- <span class=\"functioncall\">as.vector<\/span>(<span class=\"functioncall\">t<\/span>(<span class=\"functioncall\">coef<\/span>(<span class=\"functioncall\">summary<\/span>(m))[, 1:2]))\r\n  yhat &lt;- <span class=\"functioncall\">predict<\/span>(m, newdata, type = <span class=\"string\">\"response\"<\/span>)\r\n  <span class=\"functioncall\">return<\/span>(<span class=\"functioncall\">c<\/span>(mparams, yhat))\r\n}\r\n\r\n<span class=\"comment\">## newdata <span class=\"keyword\">for<\/span> prediction<\/span>\r\nnewdata &lt;- <span class=\"functioncall\">expand.grid<\/span>(age = 1:9, hmo = <span class=\"functioncall\">factor<\/span>(0:1), died = <span class=\"functioncall\">factor<\/span>(0:1))\r\nnewdata$yhat &lt;- <span class=\"functioncall\">predict<\/span>(m1, newdata, type = <span class=\"string\">\"response\"<\/span>)\r\n\r\n<span class=\"functioncall\">set.seed<\/span>(10)\r\nres &lt;- <span class=\"functioncall\">boot<\/span>(dat, f, R = 1200, newdata = newdata, parallel = <span class=\"string\">\"snow\"<\/span>, ncpus = 4)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] \"head(w)\"\r\n##      [,1]\r\n## [1,]    1\r\n## [2,]    1\r\n## [3,]    1\r\n## [4,]    1\r\n## [5,]    1\r\n## [6,]    1\r\n## [1] \"head(y)\"\r\n##   [,1]\r\n## 1    4\r\n## 2    9\r\n## 3    3\r\n## 4    9\r\n## 5    1\r\n## 6    4\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The results are alternating parameter estimates and standard\nerrors for the parameters (the first 10).\nThat is, the first row has the first parameter estimate\nfrom our model. The second has the standard error for the\nfirst parameter. The third column contains the bootstrapped\nstandard errors.<\/p>\n<p>Now we can get the confidence intervals for all the parameters.\nWe start on the original scale with percentile and basic bootstrap CIs.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## basic parameter estimates with percentile and bias adjusted CIs<\/span>\r\nparms &lt;- <span class=\"functioncall\">t<\/span>(<span class=\"functioncall\">sapply<\/span>(<span class=\"functioncall\">c<\/span>(1, 3, 5, 7, 9), <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(i) {\r\n  out &lt;- <span class=\"functioncall\">boot.ci<\/span>(res, index = <span class=\"functioncall\">c<\/span>(i, i + 1), type = <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"perc\"<\/span>, <span class=\"string\">\"basic\"<\/span>))\r\n  <span class=\"functioncall\">with<\/span>(out, <span class=\"functioncall\">c<\/span>(Est = t0, pLL = percent[4], pUL = percent[5],\r\n    basicLL = basic[4], basicLL = basic[5]))\r\n}))\r\n\r\n<span class=\"comment\">## add row names<\/span>\r\n<span class=\"functioncall\">row.names<\/span>(parms) &lt;- <span class=\"functioncall\">names<\/span>(<span class=\"functioncall\">coef<\/span>(m1))\r\n<span class=\"comment\">## print results<\/span>\r\nparms\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                    Est      pLL      pUL  basicLL  basicLL\r\n## (Intercept):1  2.40833  2.26288  2.55285  2.26381  2.55377\r\n## (Intercept):2  0.56864  0.43812  0.70414  0.43314  0.69916\r\n## age           -0.01569 -0.04233  0.01089 -0.04228  0.01095\r\n## hmo1          -0.14706 -0.26276 -0.03931 -0.25481 -0.03135\r\n## died1         -0.21777 -0.32846 -0.11476 -0.32078 -0.10708\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The bootstrapped confidence intervals are wider than would be expected using a\nnormal based approximation. The bootstrapped CIs are more consistent with\nthe CIs from Stata when using robust standard errors.<\/p>\n<p>Now we can estimate the incident risk ratio (IRR) for the negative binomial model.\nThis is done using almost identical code as before,\nbut passing a transformation function to the <code>h<\/code> argument of\n<code>boot.ci<\/code>, in this case, <code>exp<\/code> to exponentiate.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## exponentiated parameter estimates with percentile and bias adjusted CIs<\/span>\r\nexpparms &lt;- <span class=\"functioncall\">t<\/span>(<span class=\"functioncall\">sapply<\/span>(<span class=\"functioncall\">c<\/span>(1, 3, 5, 7, 9), <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(i) {\r\n  out &lt;- <span class=\"functioncall\">boot.ci<\/span>(res, index = <span class=\"functioncall\">c<\/span>(i, i + 1), type = <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"perc\"<\/span>, <span class=\"string\">\"basic\"<\/span>), h = exp)\r\n  <span class=\"functioncall\">with<\/span>(out, <span class=\"functioncall\">c<\/span>(Est = t0, pLL = percent[4], pUL = percent[5],\r\n    basicLL = basic[4], basicLL = basic[5]))\r\n}))\r\n\r\n<span class=\"comment\">## add row names<\/span>\r\n<span class=\"functioncall\">row.names<\/span>(expparms) &lt;- <span class=\"functioncall\">names<\/span>(<span class=\"functioncall\">coef<\/span>(m1))\r\n<span class=\"comment\">## print results<\/span>\r\nexpparms\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                   Est    pLL     pUL basicLL basicLL\r\n## (Intercept):1 11.1154 9.6107 12.8436  9.3871 12.6200\r\n## (Intercept):2  1.7659 1.5498  2.0221  1.5096  1.9819\r\n## age            0.9844 0.9585  1.0110  0.9579  1.0103\r\n## hmo1           0.8632 0.7689  0.9615  0.7650  0.9576\r\n## died1          0.8043 0.7200  0.8916  0.7170  0.8886\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The results are consistent with what we initially viewed graphically,\n<code>age<\/code> does not have a significant effect, but <code>hmo<\/code> and <code>died<\/code> both do.\nIn order to better understand our results and model, let&#8217;s plot some predicted values.\nBecause all of our predictors were categorical (<code>hmo<\/code> and <code>died<\/code>)\nor had a small number of unique values (<code>age<\/code>) we will get predicted values for\nall possible combinations. This was actually done earlier\nwhen we bootstrapped the parameter estimates by creating a new data set\nusing the <code>expand.grid<\/code> function, then estimating the predicted values\nusing the <code>predict<\/code> function. Now we can plot that data.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(newdata, <span class=\"functioncall\">aes<\/span>(x = age, y = yhat, colour = hmo))  +\r\n  <span class=\"functioncall\">geom_point<\/span>() +\r\n  <span class=\"functioncall\">geom_line<\/span>() +\r\n  <span class=\"functioncall\">facet_wrap<\/span>(~ died)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztnb-unnamed-chunk-15.png\" alt=\"plot of chunk unnamed-chunk-15\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>If we really wanted to compare the predicted values, we could bootstrap\nconfidence intervals around the predicted estimates. These confidence\nintervals are not for the predicted value themselves, but that that is the\nmean predicted value (i.e., for the estimate, not a new individual).\nBecause fitting these models is slow, we included the predicted values\nearlier when we bootstrapped the model parameters. We will go back to\nthe bootstrap output now and get the confidence intervals for the\npredicted values.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## get the bootstrapped percentile CIs<\/span>\r\nyhat &lt;- <span class=\"functioncall\">t<\/span>(<span class=\"functioncall\">sapply<\/span>(10 + (1:<span class=\"functioncall\">nrow<\/span>(newdata)), <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(i) {\r\n  out &lt;- <span class=\"functioncall\">boot.ci<\/span>(res, index = i, type = <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"perc\"<\/span>))\r\n  <span class=\"functioncall\">with<\/span>(out, <span class=\"functioncall\">c<\/span>(Est = t0, pLL = percent[4], pUL = percent[5]))\r\n}))\r\n\r\n<span class=\"comment\">## merge CIs with predicted values<\/span>\r\nnewdata &lt;- <span class=\"functioncall\">cbind<\/span>(newdata, yhat)\r\n\r\n<span class=\"comment\">## graph with CIs<\/span>\r\n<span class=\"functioncall\">ggplot<\/span>(newdata, <span class=\"functioncall\">aes<\/span>(x = age, y = yhat, colour = hmo, fill = hmo))  +\r\n  <span class=\"functioncall\">geom_ribbon<\/span>(<span class=\"functioncall\">aes<\/span>(ymin = pLL, ymax = pUL), alpha = .25) +\r\n  <span class=\"functioncall\">geom_point<\/span>() +\r\n  <span class=\"functioncall\">geom_line<\/span>() +\r\n  <span class=\"functioncall\">facet_wrap<\/span>(~ died)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztnb-unnamed-chunk-16.png\" alt=\"plot of chunk unnamed-chunk-16\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<h2>Things to consider<\/h2>\n<ul>\n<li>Count data often use exposure variable to indicate the number of times the event\ncould have happened. You can incorporate exposure into your model by using the <b>offset()<\/b>\noption.<\/li>\n<li>It is not recommended that zero-truncated negative models be applied to\nsmall samples. What constitutes a small sample does not seem to be clearly defined\nin the literature.<\/li>\n<li>Pseudo-R-squared values differ from OLS R-squareds, please see\n<a href=\"https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faq-what-are-pseudo-r-squareds\/\">FAQ: What are\npseudo R-squareds?<\/a> for a discussion on this issue.<\/li>\n<\/ul>\n<h2>See Also<\/h2>\n<ul>\n<li><a href=\"\/r\/dae\/zero-truncated-poisson\/\">Zero-truncated Poisson regression in R<\/a>\nuseful when there is not overdispersion.<\/li>\n<\/ul>\n<h2>References<\/h2>\n<ul>\n<li>Yee, T. W. (2008). The VGAM package. <i>R News 8(2)<\/i>, 28 39.\nURL http:\/\/CRAN.R-project.org\/doc\/Rnews\/<\/li>\n<li>Yee, T. W., Hastie, T. J. (2003). Reduced-rank vector generalized linear models.\n<i>Statistical Modelling 3(1)<\/i>, 15 41.<\/li>\n<li>Yee, T. W., Wild, C. J. (1996). Vector generalized additive models.\n<i>J. Roy. Statist. Soc. Ser. B 58(3)<\/i>, 481 493.<\/li>\n<li>Long, J. Scott (1997). Regression Models for Categorical and Limited Dependent Variables.\nThousand Oaks, CA: Sage Publications.<\/li>\n<\/ul>\n<p><!--?php include \"stat\/footer.htm\"; ?--><\/p>\n","protected":false},"excerpt":{"rendered":"<p>Zero-truncated negative binomial regression is used to model count data for which the value zero cannot occur and for which over dispersion exists. This page uses the following packages. Make&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/zero-truncated-negative-binomial\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":20,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-938","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/938","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=938"}],"version-history":[{"count":6,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/938\/revisions"}],"predecessor-version":[{"id":37448,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/938\/revisions\/37448"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=938"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}