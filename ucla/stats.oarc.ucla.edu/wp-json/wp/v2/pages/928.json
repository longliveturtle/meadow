{"id":928,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-poisson-regression\/"},"modified":"2021-10-20T12:34:21","modified_gmt":"2021-10-20T19:34:21","slug":"poisson-regression","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/poisson-regression\/","title":{"rendered":"Poisson Regression |  R Data Analysis Examples"},"content":{"rendered":"<p>Poisson regression is used to model count variables.<\/p>\n<p>This page uses the following packages. Make sure that you can load\nthem before trying to run the examples on this page. If you do not have\na package installed, run: <code>install.packages(\"packagename\")<\/code>, or\nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div id=\"unnamed-chunk-2\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">require<\/span><span class=\"hl std\">(ggplot2)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(sandwich)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(msm)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><strong>Version info: <\/strong>Code for this page was tested in R version 3.1.1 (2014-07-10)\nOn: 2014-08-11\nWith: sandwich 2.3-1; boot 1.3-11; knitr 1.6; pscl 1.04.4; vcd 1.3-1; gam 1.09.1; coda 0.16-1; mvtnorm 1.0-0; GGally 0.4.7; plyr 1.8.1; MASS 7.3-33; Hmisc 3.14-4; Formula 1.1-2; survival 2.37-7; psych 1.4.5; reshape2 1.4; msm 1.4; phia 0.1-5; RColorBrewer 1.0-5; effects 3.0-0; colorspace 1.2-4; lattice 0.20-29; pequod 0.0-3; car 2.0-20; ggplot2 1.0.0\n<\/code><\/p>\n<p><strong>Please note:<\/strong> The purpose of this page is to show how to use various data\nanalysis commands. It does not cover all aspects of the research process which\nresearchers are expected to do. In particular, it does not cover data\ncleaning and checking, verification of assumptions, model diagnostics or\npotential follow-up analyses.<\/p>\n<h2>Examples of Poisson regression<\/h2>\n<p>Example 1. The number of persons killed by mule or horse kicks in the\nPrussian army per year. Ladislaus Bortkiewicz collected data from 20 volumes of\n<i>Preussischen Statistik<\/i>. These data were collected on 10 corps of\nthe Prussian army in the late 1800s over the course of 20 years.<\/p>\n<p>Example 2. The number of people in line in front of you at the grocery store.\nPredictors may include the number of items currently offered at a special\ndiscounted price and whether a special event (e.g., a holiday, a big sporting\nevent) is three or fewer days away.<\/p>\n<p>Example 3. The number of awards earned by students at one high school.\nPredictors of the number of awards earned include the type of program in which the\nstudent was enrolled (e.g., vocational, general or academic) and the score on their\nfinal exam in math.<\/p>\n<h2>Description of the data<\/h2>\n<p>For the purpose of illustration, we have simulated a data set for Example 3 above.\nIn this example, <code>num_awards<\/code> is the outcome variable and indicates the\nnumber of awards earned by students at a high school in a year, <code>math<\/code> is a continuous\npredictor variable and represents students&#8217; scores on their math final exam, and <code>prog<\/code> is a categorical predictor variable with\nthree levels indicating the type of program in which the students were\nenrolled. It is coded as 1 = &#8220;General&#8221;, 2 = &#8220;Academic&#8221; and 3 = &#8220;Vocational&#8221;.\nLet&#8217;s start with loading the data and looking at some descriptive statistics.<\/p>\n<div id=\"unnamed-chunk-3\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">p<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">read.csv<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/poisson_sim.csv\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl std\">p<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">within<\/span><span class=\"hl std\">(p, {<\/span>\r\n  <span class=\"hl std\">prog<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">factor<\/span><span class=\"hl std\">(prog,<\/span> <span class=\"hl kwc\">levels<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">3<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">labels<\/span><span class=\"hl std\">=<\/span><span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"General\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"Academic\"<\/span><span class=\"hl std\">,<\/span> \r\n<span class=\"hl str\">                                                     \"Vocational\"<\/span><span class=\"hl std\">))<\/span>\r\n  <span class=\"hl std\">id<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">factor<\/span><span class=\"hl std\">(id)<\/span>\r\n<span class=\"hl std\">})<\/span>\r\n<span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(p)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##        id        num_awards           prog          math     \r\n##  1      :  1   Min.   :0.00   General   : 45   Min.   :33.0  \r\n##  2      :  1   1st Qu.:0.00   Academic  :105   1st Qu.:45.0  \r\n##  3      :  1   Median :0.00   Vocational: 50   Median :52.0  \r\n##  4      :  1   Mean   :0.63                    Mean   :52.6  \r\n##  5      :  1   3rd Qu.:1.00                    3rd Qu.:59.0  \r\n##  6      :  1   Max.   :6.00                    Max.   :75.0  \r\n##  (Other):194\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Each variable has 200 valid observations and their distributions seem quite\nreasonable. The <em>unconditional<\/em> mean and variance of our outcome variable\nare not extremely different. Our model assumes that these values, conditioned on\nthe predictor variables, will be equal (or at least roughly so).<\/p>\n<p>We can use the <code>tapply<\/code> function to display the summary statistics by program\ntype. The table below shows the average numbers of awards by program type\nand seems to suggest that program type is a good candidate for\npredicting the number of awards, our outcome variable, because the mean value of\nthe outcome appears to vary by <code>prog<\/code>. Additionally, the\nmeans and variances within each level of <code>prog<\/code>&#8211;the <em>conditional<\/em>\nmeans and variances&#8211;are similar. A conditional histogram separated out by\nprogram type is plotted to show the distribution.<\/p>\n<div id=\"unnamed-chunk-4\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">with<\/span><span class=\"hl std\">(p,<\/span> <span class=\"hl kwd\">tapply<\/span><span class=\"hl std\">(num_awards, prog,<\/span> <span class=\"hl kwa\">function<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span><span class=\"hl std\">) {<\/span>\r\n  <span class=\"hl kwd\">sprintf<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"M (SD) = %1.2f (%1.2f)\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(x),<\/span> <span class=\"hl kwd\">sd<\/span><span class=\"hl std\">(x))<\/span>\r\n<span class=\"hl std\">}))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                General               Academic             Vocational \r\n## \"M (SD) = 0.20 (0.40)\" \"M (SD) = 1.00 (1.28)\" \"M (SD) = 0.24 (0.52)\"\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(p,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(num_awards,<\/span> <span class=\"hl kwc\">fill<\/span> <span class=\"hl std\">= prog))<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_histogram<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">binwidth<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">.5<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">position<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"dodge\"<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/poissonreg-unnamed-chunk-4.png\" alt=\"Histogram of number of awards by program type\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<h2>Analysis methods you might consider<\/h2>\n<p>Below is a list of some analysis methods you may have\nencountered. Some of the methods listed are quite reasonable, while others have\neither fallen out of favor or have limitations.<\/p>\n<ul>\n<li>Poisson regression &#8211; Poisson regression is often used for modeling count\ndata. Poisson regression has a number of extensions useful for count models.<\/li>\n<li>Negative binomial regression &#8211; Negative binomial regression can be used for over-dispersed\ncount data, that is when the conditional variance exceeds the conditional\nmean. It can be considered as a generalization of Poisson regression since\nit has the same mean structure as Poisson regression and it has an extra\nparameter to model the over-dispersion. If the conditional distribution of\nthe outcome variable is over-dispersed, the confidence intervals for coefficients in Negative binomial\nregression are likely to be wider as compared to those from a Poisson regression.<\/li>\n<li>Zero-inflated regression model &#8211; Zero-inflated models attempt to account\nfor excess zeros. In other words, two kinds of zeros are thought to\nexist in the data, &#8220;true zeros&#8221; and &#8220;excess zeros&#8221;. Zero-inflated\nmodels estimate two equations simultaneously, one for the count model and one for the\nexcess zeros.<\/li>\n<li>OLS regression &#8211; Count outcome variables are sometimes log-transformed\nand analyzed using OLS regression. Many issues arise with this\napproach, including loss of data due to undefined values generated by taking\nthe log of zero (which is undefined) and biased estimates.<\/li>\n<\/ul>\n<h2>Poisson regression<\/h2>\n<p>At this point, we are ready to perform our Poisson model analysis using\nthe <code>glm<\/code> function. We fit the model and store it in the object <code>m1<\/code>\nand get a summary of the model at the same time.<\/p>\n<div id=\"unnamed-chunk-5\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(m1<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">glm<\/span><span class=\"hl std\">(num_awards<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">prog<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">math,<\/span> <span class=\"hl kwc\">family<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"poisson\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">data<\/span><span class=\"hl std\">=p))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## glm(formula = num_awards ~ prog + math, family = \"poisson\", data = p)\r\n## \r\n## Deviance Residuals: \r\n##    Min      1Q  Median      3Q     Max  \r\n## -2.204  -0.844  -0.511   0.256   2.680  \r\n## \r\n## Coefficients:\r\n##                Estimate Std. Error z value Pr(&gt;|z|)    \r\n## (Intercept)     -5.2471     0.6585   -7.97  1.6e-15 ***\r\n## progAcademic     1.0839     0.3583    3.03   0.0025 ** \r\n## progVocational   0.3698     0.4411    0.84   0.4018    \r\n## math             0.0702     0.0106    6.62  3.6e-11 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n## \r\n## (Dispersion parameter for poisson family taken to be 1)\r\n## \r\n##     Null deviance: 287.67  on 199  degrees of freedom\r\n## Residual deviance: 189.45  on 196  degrees of freedom\r\n## AIC: 373.5\r\n## \r\n## Number of Fisher Scoring iterations: 6\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Cameron and Trivedi (2009) recommended using robust standard errors for the\nparameter estimates to control for mild violation of the distribution\nassumption that the variance equals the mean.\nWe use R package <code>sandwich<\/code> below to obtain the robust standard errors and\ncalculated the p-values accordingly. Together with the p-values, we have also\ncalculated the 95% confidence interval using the parameter estimates and their\nrobust standard errors.<\/p>\n<div id=\"unnamed-chunk-6\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">cov.m1<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">vcovHC<\/span><span class=\"hl std\">(m1,<\/span> <span class=\"hl kwc\">type<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"HC0\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl std\">std.err<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">sqrt<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">diag<\/span><span class=\"hl std\">(cov.m1))<\/span>\r\n<span class=\"hl std\">r.est<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">Estimate<\/span><span class=\"hl std\">=<\/span> <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(m1),<\/span> <span class=\"hl str\">\"Robust SE\"<\/span> <span class=\"hl std\">= std.err,<\/span>\r\n<span class=\"hl str\">\"Pr(&gt;|z|)\"<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">2<\/span> <span class=\"hl opt\">*<\/span> <span class=\"hl kwd\">pnorm<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">abs<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(m1)<\/span><span class=\"hl opt\">\/<\/span><span class=\"hl std\">std.err),<\/span> <span class=\"hl kwc\">lower.tail<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">FALSE<\/span><span class=\"hl std\">),<\/span>\r\n<span class=\"hl kwc\">LL<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(m1)<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl num\">1.96<\/span> <span class=\"hl opt\">*<\/span> <span class=\"hl std\">std.err,<\/span>\r\n<span class=\"hl kwc\">UL<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(m1)<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl num\">1.96<\/span> <span class=\"hl opt\">*<\/span> <span class=\"hl std\">std.err)<\/span>\r\n\r\n<span class=\"hl std\">r.est<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                Estimate Robust SE  Pr(&gt;|z|)      LL       UL\r\n## (Intercept)    -5.24712   0.64600 4.567e-16 -6.5133 -3.98097\r\n## progAcademic    1.08386   0.32105 7.355e-04  0.4546  1.71311\r\n## progVocational  0.36981   0.40042 3.557e-01 -0.4150  1.15463\r\n## math            0.07015   0.01044 1.784e-11  0.0497  0.09061\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Now let&#8217;s look at the output of function <code>glm<\/code> more closely.<\/p>\n<ul>\n<li>The output begins\nwith echoing the function call. The information on deviance residuals\nis displayed next. Deviance residuals are approximately normally distributed\nif the model is specified correctly.In our example, it shows a little\nbit of skeweness since median is not quite zero.<\/li>\n<li>Next come the Poisson regression coefficients for each of the variables\nalong with the standard errors, z-scores, p-values\nand 95% confidence intervals for the coefficients. The coefficient for\n<code>math<\/code> is .07. This means that the expected\nlog count for a one-unit increase in <code>math<\/code> is .07. The indicator variable\n<code>progAcademic<\/code> compares between <strong>prog = &#8220;Academic&#8221;<\/strong>\nand <strong>prog = &#8220;General&#8221;<\/strong>, the expected log count for <strong>prog =\n&#8220;Academic&#8221;<\/strong> increases by about 1.1. The\nindicator variable <code>prog.Vocational<\/code> is the expected\ndifference in log count ((approx .37)) between <strong>prog = &#8220;Vocational&#8221;<\/strong> and\nthe reference group (<strong>prog = &#8220;General&#8221;<\/strong>).<\/li>\n<li>The information on deviance is also provided. We can use the residual\ndeviance to perform a goodness of fit test for the overall model. The\nresidual deviance is the difference between the deviance of the current\nmodel and the maximum deviance of the ideal model where the predicted values\nare identical to the observed. Therefore, if the residual difference is\nsmall enough, the goodness of fit test will not be significant, indicating\nthat the model fits the data. We conclude that the model fits reasonably\nwell because the goodness-of-fit chi-squared test is not statistically\nsignificant. If the test had been statistically significant, it would\nindicate that the data do not fit the model well. In that situation,\nwe may try to determine if there are omitted predictor variables, if\nour linearity assumption holds and\/or if there is an issue of\nover-dispersion.<\/li>\n<\/ul>\n<div id=\"unnamed-chunk-7\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">with<\/span><span class=\"hl std\">(m1,<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">res.deviance<\/span> <span class=\"hl std\">= deviance,<\/span> <span class=\"hl kwc\">df<\/span> <span class=\"hl std\">= df.residual,<\/span>\r\n  <span class=\"hl kwc\">p<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">pchisq<\/span><span class=\"hl std\">(deviance, df.residual,<\/span> <span class=\"hl kwc\">lower.tail<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">FALSE<\/span><span class=\"hl std\">)))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##      res.deviance  df      p\r\n## [1,]        189.4 196 0.6182\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can also test the overall effect of <code>prog<\/code> by comparing the deviance\nof the full model with the deviance of the model excluding <code>prog<\/code>.\nThe two degree-of-freedom chi-square test indicates that <code>prog<\/code>, taken\ntogether, is a statistically significant predictor of <code>num_awards<\/code>.<\/p>\n<div id=\"unnamed-chunk-8\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## update m1 model dropping prog<\/span>\r\n<span class=\"hl std\">m2<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">update<\/span><span class=\"hl std\">(m1, .<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">.<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">prog)<\/span>\r\n<span class=\"hl com\">## test model differences with chi square test<\/span>\r\n<span class=\"hl kwd\">anova<\/span><span class=\"hl std\">(m2, m1,<\/span> <span class=\"hl kwc\">test<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"Chisq\"<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## Analysis of Deviance Table\r\n## \r\n## Model 1: num_awards ~ math\r\n## Model 2: num_awards ~ prog + math\r\n##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    \r\n## 1       198        204                         \r\n## 2       196        189  2     14.6  0.00069 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Sometimes, we might want to present the regression results as incident rate\nratios and their standard errors, together with the confidence interval. To\ncompute the standard error for the incident rate ratios, we will use the\nDelta method. To this end, we make use the function <code>deltamethod<\/code>\nimplemented in R package <code>msm<\/code>.<\/p>\n<div id=\"unnamed-chunk-9\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">s<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">deltamethod<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">list<\/span><span class=\"hl std\">(<\/span><span class=\"hl opt\">~<\/span> <span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(x1),<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(x2),<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(x3),<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(x4)),<\/span> \r\n                                                <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(m1), cov.m1)<\/span>\r\n\r\n<span class=\"hl com\">## exponentiate old estimates dropping the p values<\/span>\r\n<span class=\"hl std\">rexp.est<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(r.est[,<\/span> <span class=\"hl opt\">-<\/span><span class=\"hl num\">3<\/span><span class=\"hl std\">])<\/span>\r\n<span class=\"hl com\">## replace SEs with estimates for exponentiated coefficients<\/span>\r\n<span class=\"hl std\">rexp.est[,<\/span> <span class=\"hl str\">\"Robust SE\"<\/span><span class=\"hl std\">]<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">s<\/span>\r\n\r\n<span class=\"hl std\">rexp.est<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                Estimate Robust SE       LL      UL\r\n## (Intercept)    0.005263   0.00340 0.001484 0.01867\r\n## progAcademic   2.956065   0.94904 1.575551 5.54620\r\n## progVocational 1.447458   0.57959 0.660335 3.17284\r\n## math           1.072672   0.01119 1.050955 1.09484\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The output above indicates that the incident rate for <strong>prog = &#8220;Academic&#8221;<\/strong> is 2.96\ntimes the incident rate for the reference group (<strong>prog = &#8220;General&#8221;<\/strong>). Likewise,\nthe incident rate for <strong>prog = &#8220;Vocational&#8221;<\/strong> is 1.45 times the incident rate for the\nreference group holding the other variables at constant. The percent change in the incident rate of\n<code>num_awards<\/code> is by 7% for every unit increase in <code>math<\/code>.\nFor additional information on the various metrics in which the results can be\npresented, and the interpretation of such, please see <i>Regression Models for\nCategorical Dependent Variables Using Stata, Second Edition<\/i> by J. Scott Long\nand Jeremy Freese (2006).<\/p>\n<p>Sometimes, we might want to look at the expected marginal means. For\nexample, what are the expected counts for each program type holding math\nscore at its overall mean? To answer this question, we can make use of\nthe <code>predict<\/code> function. First off, we will make a small data set\nto apply the <code>predict<\/code> function to it.<\/p>\n<div id=\"unnamed-chunk-10\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">(s1<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">data.frame<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">math<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(p<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">math),<\/span>\r\n  <span class=\"hl kwc\">prog<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">factor<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">3<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">levels<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">3<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">labels<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">levels<\/span><span class=\"hl std\">(p<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">prog))))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##    math       prog\r\n## 1 52.65    General\r\n## 2 52.65   Academic\r\n## 3 52.65 Vocational\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">predict<\/span><span class=\"hl std\">(m1, s1,<\/span> <span class=\"hl kwc\">type<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"response\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">se.fit<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">TRUE<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## $fit\r\n##      1      2      3 \r\n## 0.2114 0.6249 0.3060 \r\n## \r\n## $se.fit\r\n##       1       2       3 \r\n## 0.07050 0.08628 0.08834 \r\n## \r\n## $residual.scale\r\n## [1] 1\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>In the output above, we see that the predicted number of events for level 1\nof <code>prog<\/code> is about .21, holding <code>math<\/code> at its mean. The predicted\nnumber of events for level 2 of <code>prog<\/code> is higher at .62, and the\npredicted number of events for level 3 of <code>prog<\/code> is about .31. The ratios\nof these predicted counts ((frac{.625}{.211} = 2.96), (frac{.306}{.211} = 1.45)) match\nwhat we saw looking at the IRR.<\/p>\n<p>We can also graph the predicted number of events with the commands below.\nThe graph indicates that the most awards are predicted for those in the academic\nprogram (prog = 2), especially if the student has a high math score. The\nlowest number of predicted awards is for those students in the general program (prog\n= 1). The graph overlays the lines of expected values onto the actual points,\nalthough a small amount of random noise was added vertically to lessen\noverplotting.<\/p>\n<div id=\"unnamed-chunk-11\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## calculate and store predicted values<\/span>\r\n<span class=\"hl std\">p<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">phat<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">predict<\/span><span class=\"hl std\">(m1,<\/span> <span class=\"hl kwc\">type<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"response\"<\/span><span class=\"hl std\">)<\/span>\r\n\r\n<span class=\"hl com\">## order by program and then by math<\/span>\r\n<span class=\"hl std\">p<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">p[<\/span><span class=\"hl kwd\">with<\/span><span class=\"hl std\">(p,<\/span> <span class=\"hl kwd\">order<\/span><span class=\"hl std\">(prog, math)), ]<\/span>\r\n\r\n<span class=\"hl com\">## create the plot<\/span>\r\n<span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(p,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span> <span class=\"hl std\">= math,<\/span> <span class=\"hl kwc\">y<\/span> <span class=\"hl std\">= phat,<\/span> <span class=\"hl kwc\">colour<\/span> <span class=\"hl std\">= prog))<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_point<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">y<\/span> <span class=\"hl std\">= num_awards),<\/span> <span class=\"hl kwc\">alpha<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">.5<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">position<\/span><span class=\"hl std\">=<\/span><span class=\"hl kwd\">position_jitter<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">h<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">.2<\/span><span class=\"hl std\">))<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_line<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">size<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">labs<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"Math Score\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">y<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"Expected number of awards\"<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/poissonreg-unnamed-chunk-11.png\" alt=\"Line plots of predicted number of awards by program type\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<h2>Things to consider<\/h2>\n<ul>\n<li>When there seems to be an issue of dispersion, we should first check if\nour model is appropriately specified, such as omitted variables and\nfunctional forms. For example, if we omitted the predictor variable <code>prog\n<\/code>in the example above, our model would seem to have a problem with\nover-dispersion. In other words, a misspecified model could present\na symptom like an over-dispersion problem.<\/li>\n<li>Assuming that the model is correctly specified, the assumption that the\nconditional variance is equal to the conditional mean should be checked.\nThere are several tests including the likelihood ratio test of\nover-dispersion parameter alpha by running the same model using negative\nbinomial distribution. R package\n<a href=\"http:\/\/cran.r-project.org\/web\/packages\/pscl\/index.html\">pscl<\/a> (Political Science Computational Laboratory, Stanford University)\nprovides many functions for binomial and count data including <code>odTest<\/code>\nfor testing over-dispersion.<\/li>\n<li>One common cause of over-dispersion is excess zeros, which in turn are\ngenerated by an additional data generating process. In this situation,\nzero-inflated model should be considered.<\/li>\n<li>If the data generating process does not allow for any 0s (such as the\nnumber of days spent in the hospital), then a zero-truncated model may be\nmore appropriate.<\/li>\n<li>Count data often have an exposure variable, which indicates the number\nof times the event could have happened. This variable should be\nincorporated into a Poisson model with the use of the <code>offset<\/code> option.<\/li>\n<li>The outcome variable in a Poisson regression cannot have negative numbers, and the exposure\ncannot have 0s.<\/li>\n<li>Many different measures of pseudo-R-squared exist.\nThey all attempt to provide information similar to that provided by\nR-squared in OLS regression, even though none of them can be interpreted\nexactly as R-squared in OLS regression is interpreted. For a discussion of\nvarious pseudo-R-squares, see Long and Freese (2006) or our FAQ page\n<a href=\"https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faq-what-are-pseudo-r-squareds\/\">What are pseudo R-squareds?<\/a>.<\/li>\n<li>Poisson regression is estimated via maximum likelihood estimation. It\nusually requires a large sample size.<\/li>\n<\/ul>\n<h2>References<\/h2>\n<ul>\n<li>Cameron, A. C. and Trivedi, P. K. 2009. <i>Microeconometrics Using\nStata<\/i>. College Station, TX: Stata Press.<\/li>\n<li>Cameron, A. C. and Trivedi, P. K. 1998. <i>Regression\nAnalysis of Count Data<\/i>. New York: Cambridge Press.<\/li>\n<li>Cameron, A. C. Advances in Count Data Regression Talk for the\nApplied Statistics Workshop, March 28, 2009.\n<a href=\"http:\/\/cameron.econ.ucdavis.edu\/racd\/count.html\">\nhttp:\/\/cameron.econ.ucdavis.edu\/racd\/count.html<\/a> .<\/li>\n<li>Dupont, W. D. 2002. <i>Statistical Modeling for Biomedical\nResearchers: A Simple Introduction to the Analysis of Complex Data.<\/i>\nNew York: Cambridge Press.<\/li>\n<li>Long, J. S. 1997. <i>Regression Models for Categorical and Limited Dependent Variables.<\/i>\nThousand Oaks, CA: Sage Publications.<\/li>\n<li>Long, J. S. and Freese, J. 2006. <i>Regression Models for\nCategorical Dependent Variables Using Stata, Second Edition<\/i>.\nCollege Station, TX: Stata Press.<\/li>\n<\/ul>\n","protected":false},"excerpt":{"rendered":"<p>Poisson regression is used to model count variables. This page uses the following packages. Make sure that you can load them before trying to run the examples on this page&#8230;.<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/poisson-regression\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":10,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-928","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/928","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=928"}],"version-history":[{"count":6,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/928\/revisions"}],"predecessor-version":[{"id":35897,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/928\/revisions\/35897"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=928"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}