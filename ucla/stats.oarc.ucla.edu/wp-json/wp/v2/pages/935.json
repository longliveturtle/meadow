{"id":935,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-truncated-regression\/"},"modified":"2016-02-11T19:13:41","modified_gmt":"2016-02-11T19:13:41","slug":"truncated-regression","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/truncated-regression\/","title":{"rendered":"Truncated Regression |  R Data Analysis Examples"},"content":{"rendered":"<p><!DOCTYPE html><\/p>\n<p><html><\/p>\n<p><head><\/p>\n<style type=\"text\/css\">\n.knitr.inline {\n  background-color: #f7f7f7;\n  border:solid 1px #B0B0B0;\n}\n.error {\n\tfont-weight: bold;\n\tcolor: #FF0000;\n},\n.warning {\n\tfont-weight: bold;\n}\n.message {\n\tfont-style: italic;\n}\n.source, .output, .warning, .error, .message {\n\tpadding: 0em 1em;\n  border:solid 1px #F7F7F7;\n}\n.source {\n  background-color: #f5f5f5;\n}\n.rimage.left {\n  text-align: left;\n}\n.rimage.right {\n  text-align: right;\n}\n.rimage.center {\n  text-align: center;\n}\n.hl.num {\n  color: #AF0F91;\n}\n.hl.str {\n  color: #317ECC;\n}\n.hl.com {\n  color: #AD95AF;\n  font-style: italic;\n}\n.hl.opt {\n  color: #000000;\n}\n.hl.std {\n  color: #585858;\n}\n.hl.kwa {\n  color: #295F94;\n  font-weight: bold;\n}\n.hl.kwb {\n  color: #B05A65;\n}\n.hl.kwc {\n  color: #55aa55;\n}\n.hl.kwd {\n  color: #BC5A65;\n  font-weight: bold;\n}\n<\/style>\n<p><title><\/title>\n<link href=\"\/stat\/ats_style.css\" type=\"text\/css\" rel=\"stylesheet\">\n<!-- modified 9\/02\/12 jfw -->\n<!-- modified 8\/20\/14 jlr -->\n<\/head><\/p>\n<p><body>\n<?php include \"stat\/header.htm\"; ?><\/p>\n<p>Truncated regression is used to model dependent variables for which some of the\nobservations are not included in the analysis because of the value of the\ndependent variable.<\/p>\n<p>This page uses the following packages. Make sure that you can load \nthem before trying to run the examples on this page. If you do not have \na package installed, run: <code>install.packages(\"packagename\")<\/code>, or \nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div class=\"chunk\" id=\"unnamed-chunk-2\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">require<\/span><span class=\"hl std\">(foreign)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(ggplot2)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(truncreg)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(boot)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><strong>Version info: <\/strong>Code for this page was tested in R version 3.1.1 (2014-07-10)<br \/>\n On: 2014-08-21<br \/>\nWith: boot 1.3-11; truncreg 0.2-1; maxLik 1.2-0; miscTools 0.6-16; ggplot2 1.0.0; foreign 0.8-61; knitr 1.6\n<\/code><\/p>\n<p><strong>Please note:<\/strong> The purpose of this page is to show how to use various data\nanalysis commands. It does not cover all aspects of the research process which\nresearchers are expected to do. In particular, it does not cover data\ncleaning and checking, verification of assumptions, model diagnostics or\npotential follow-up analyses. <\/p>\n<h2>Examples of truncated regression<\/h2>\n<p>Example 1. A study of students in a special GATE (gifted and talented education) program\nwishes to model achievement as a function of language skills and the type of\nprogram in which the student is currently enrolled. A major concern is\nthat students are required to have a minimum achievement score of 40 to enter\nthe special program. Thus, the sample is truncated at an achievement score\nof 40.<\/p>\n<p>Example 2. A researcher has data for a sample of Americans whose income is\nabove the poverty line. Hence, the lower part of the distribution of\nincome is truncated. If the researcher had a sample of Americans whose\nincome was at or below the poverty line, then the upper part of the income\ndistribution would be truncated. In other words, truncation is a result of\nsampling only part of the distribution of the outcome variable.<\/p>\n<h2>Description of the data<\/h2>\n<p>Let&#8217;s pursue Example 1 from above. We have a hypothetical data file,\n<code>truncreg.dta<\/code>, with 178 observations. The\noutcome variable is called <code>achiv<\/code>, and the language test score\nvariable is called <code>langscore<\/code>. The variable <code>prog<\/code> is a\ncategorical predictor variable with three levels indicating the type\nof program in which the students were enrolled.<\/p>\n<p>Let&#8217;s look at the data. It is always a good idea to start with descriptive\nstatistics.<\/p>\n<div class=\"chunk\" id=\"unnamed-chunk-3\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">dat<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">read.dta<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">&quot;https:\/\/stats.idre.ucla.edu\/stat\/data\/truncreg.dta&quot;<\/span><span class=\"hl std\">)<\/span>\r\n\r\n<span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(dat)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##        id            achiv        langscore          prog    \r\n##  Min.   :  3.0   Min.   :41.0   Min.   :31.0   general : 40  \r\n##  1st Qu.: 55.2   1st Qu.:47.0   1st Qu.:47.5   academic:101  \r\n##  Median :102.5   Median :52.0   Median :56.0   vocation: 37  \r\n##  Mean   :103.6   Mean   :54.2   Mean   :54.0                 \r\n##  3rd Qu.:151.8   3rd Qu.:63.0   3rd Qu.:61.8                 \r\n##  Max.   :200.0   Max.   :76.0   Max.   :67.0\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\"># histogram of achiv coloured by program type<\/span>\r\n<span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(dat,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(achiv,<\/span> <span class=\"hl kwc\">fill<\/span> <span class=\"hl std\">= prog))<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_histogram<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">binwidth<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">3<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/truncreg-unnamed-chunk-3.png\" alt=\"plot of chunk unnamed-chunk-3\" width=\"500px\" height=\"500px\" alt=\"histogram of achiv by prog\" class=\"plot\" \/><\/div>\n<\/div>\n<div class=\"chunk\" id=\"unnamed-chunk-4\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\"># boxplot of achiv by program type<\/span>\r\n<span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(dat,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(prog, achiv))<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_boxplot<\/span><span class=\"hl std\">()<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_jitter<\/span><span class=\"hl std\">()<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/truncreg-unnamed-chunk-4.png\" alt=\"plot of chunk unnamed-chunk-4\" width=\"500px\" height=\"500px\" alt=\"boxplot with jittered data values of achiv by prog\" class=\"plot\" \/><\/div>\n<\/div>\n<div class=\"chunk\" id=\"unnamed-chunk-5\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(dat,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span> <span class=\"hl std\">= langscore,<\/span> <span class=\"hl kwc\">y<\/span> <span class=\"hl std\">= achiv))<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_point<\/span><span class=\"hl std\">()<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">stat_smooth<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">method<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">&quot;loess&quot;<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">facet_grid<\/span><span class=\"hl std\">(.<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">prog,<\/span> <span class=\"hl kwc\">margins<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">TRUE<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/truncreg-unnamed-chunk-5.png\" alt=\"plot of chunk unnamed-chunk-5\" width=\"500px\" height=\"500px\" alt=\"scatterplot of langscore and achiv by prog with loess lines\" class=\"plot\" \/><\/div>\n<\/div>\n<h2>Analysis methods you might consider<\/h2>\n<p>Below is a list of some analysis methods you may have\nencountered. Some of the methods listed are quite reasonable, while others have\neither fallen out of favor or have limitations.<\/p>\n<ul>\n<li>OLS regression &#8211; You could analyze these data using OLS regression.&nbsp;\n  OLS regression will not adjust the estimates of the coefficients to take\n  into account the effect of truncating the sample at 40, and the coefficients\n  may be severely biased.&nbsp; This can be conceptualized as a model\n  specification error (Heckman, 1979).<\/li>\n<li>Truncated regression &#8211; Truncated regression addresses the bias\n  introduced when using OLS regression with truncated data.&nbsp; Note that with truncated regression, the\n  variance of the outcome variable is reduced compared to the distribution\n  that is not truncated. Also, if the lower part of the distribution is\n  truncated, then the mean of the truncated variable will be greater than the\n  mean from the untruncated variable; if the truncation is from above, the\n  mean of the truncated variable will be less than the untruncated variable.<\/li>\n<li>These types of models can also be conceptualized as Heckman\n  selection models, which are used to correct for sampling selection bias.<\/li>\n<li>Censored regression &#8211; Sometimes the concepts of truncation and censoring\n  are confused. With censored data we have all of the observations, but we don&#8217;t know the &#8220;true&#8221; values of\n  some of them. With truncation, some of the observations are not included in the analysis\n  because of the value of the outcome variable. It would be inappropriate to analyze\n  the data in our example using a censored regression model.<\/li>\n<\/ul>\n<h2>Truncated regression<\/h2>\n<p>Below we use the <code>truncreg<\/code> function in the <code>truncreg<\/code> package\nto estimate a truncated regression model. The <code>point<\/code> argument indicates\nwhere the data are truncated, and the direction indicates whether it is\nleft or right truncated.<\/p>\n<div class=\"chunk\" id=\"unnamed-chunk-6\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">m<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">truncreg<\/span><span class=\"hl std\">(achiv<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">langscore<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">prog,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= dat,<\/span> <span class=\"hl kwc\">point<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">40<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">direction<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">&quot;left&quot;<\/span><span class=\"hl std\">)<\/span>\r\n\r\n<span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(m)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## truncreg(formula = achiv ~ langscore + prog, data = dat, point = 40, \r\n##     direction = \"left\")\r\n## \r\n## \r\n## Coefficients :\r\n##              Estimate Std. Error t-value Pr(>|t|)    \r\n## (Intercept)    11.299      6.772    1.67    0.095 .  \r\n## langscore       0.713      0.114    6.23  4.8e-10 ***\r\n## progacademic    4.063      2.054    1.98    0.048 *  \r\n## progvocation   -1.144      2.670   -0.43    0.668    \r\n## sigma           8.754      0.666   13.13  < 2e-16 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n## \r\n## Log-Likelihood: -591 on 5 Df\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<ul>\n<li>In the table of coefficients, we have the truncated regression coefficients,\n  the standard error of the coefficients, the Wald z-tests (coefficient\/se),\n  and the p-value associated with each z-test (shown as t-values).<\/li>\n<li>The ancillary statistic \/sigma is equivalent to the standard error of\n  estimate in OLS regression. The value of 8.76 can be compared to the\n  standard deviation of achievement which was 8.96. This shows a modest reduction.\n  The output also contains an estimate of the standard error of sigma.<\/li>\n<li>The variable <code>langscore<\/code> is statistically significant. A unit\n  increase in language score leads to a .71 increase in predicted achievement.\n  One of the indicator variables for <code>prog<\/code> is also statistically\n  significant. Compared to general programs, academic programs are about 4.07 higher.\n  To determine if <code>prog<\/code> itself is statistically significant,\n  we can test models with it in and out for the two degree-of-freedom test of this variable.<\/li>\n<\/ul>\n<div class=\"chunk\" id=\"unnamed-chunk-7\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\"># update old model dropping prog<\/span>\r\n<span class=\"hl std\">m2<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">update<\/span><span class=\"hl std\">(m, .<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">.<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">prog)<\/span>\r\n\r\n<span class=\"hl kwd\">pchisq<\/span><span class=\"hl std\">(<\/span><span class=\"hl opt\">-<\/span><span class=\"hl num\">2<\/span> <span class=\"hl opt\">*<\/span> <span class=\"hl std\">(<\/span><span class=\"hl kwd\">logLik<\/span><span class=\"hl std\">(m2)<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl kwd\">logLik<\/span><span class=\"hl std\">(m)),<\/span> <span class=\"hl kwc\">df<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">2<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">lower.tail<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">FALSE<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## 'log Lik.' 0.0252 (df=3)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The two degree-of-freedom chi-square test indicates that <code>prog<\/code> is a\nstatistically significant predictor of <code>achiv<\/code>.  We can get the\nexpected means for each program at the mean of <code>langscore<\/code> by\nreparameterizing the model.<\/p>\n<div class=\"chunk\" id=\"unnamed-chunk-8\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\"># create mean centered langscore to use later<\/span>\r\n<span class=\"hl std\">dat<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">within<\/span><span class=\"hl std\">(dat, {mlangscore<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">langscore<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(langscore)})<\/span>\r\n\r\n<span class=\"hl std\">malt<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">truncreg<\/span><span class=\"hl std\">(achiv<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl num\">0<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">mlangscore<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">prog,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= dat,<\/span> <span class=\"hl kwc\">point<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">40<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(malt)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## truncreg(formula = achiv ~ 0 + mlangscore + prog, data = dat, \r\n##     point = 40)\r\n## \r\n## \r\n## Coefficients :\r\n##              Estimate Std. Error t-value Pr(>|t|)    \r\n## mlangscore      0.713      0.114    6.22  4.8e-10 ***\r\n## proggeneral    49.789      1.897   26.24  < 2e-16 ***\r\n## progacademic   53.853      1.150   46.82  < 2e-16 ***\r\n## progvocation   48.653      2.140   22.73  < 2e-16 ***\r\n## sigma           8.755      0.667   13.13  < 2e-16 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n## \r\n## Log-Likelihood: -591 on 5 Df\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Notice all that has changed is the intercept is gone and the program scores\nare now the expected values when <code>langscore<\/code> is at its mean for each\ntype of program.<\/p>\n<p>We could also calculate the bootstrapped confidence intervals if\nwe wanted to. First, we define a function that returns the parameters\nof interest, and then use the <code>boot<\/code> function to run the bootstrap.<\/p>\n<div class=\"chunk\" id=\"unnamed-chunk-9\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">f<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwa\">function<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">data<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">i<\/span><span class=\"hl std\">) {<\/span>\r\n  <span class=\"hl kwd\">require<\/span><span class=\"hl std\">(truncreg)<\/span>\r\n  <span class=\"hl std\">m<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">truncreg<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">formula<\/span> <span class=\"hl std\">= achiv<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">langscore<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">prog,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= data[i, ],<\/span> <span class=\"hl kwc\">point<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">40<\/span><span class=\"hl std\">)<\/span>\r\n  <span class=\"hl kwd\">as.vector<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">t<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(m)<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">coefficients[,<\/span> <span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">2<\/span><span class=\"hl std\">]))<\/span>\r\n<span class=\"hl std\">}<\/span>\r\n\r\n<span class=\"hl kwd\">set.seed<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">10<\/span><span class=\"hl std\">)<\/span>\r\n\r\n<span class=\"hl std\">(res<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">boot<\/span><span class=\"hl std\">(dat, f,<\/span> <span class=\"hl kwc\">R<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">1200<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">parallel<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">&quot;snow&quot;<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">ncpus<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">4<\/span><span class=\"hl std\">))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## ORDINARY NONPARAMETRIC BOOTSTRAP\r\n## \r\n## \r\n## Call:\r\n## boot(data = dat, statistic = f, R = 1200, parallel = \"snow\", \r\n##     ncpus = 4)\r\n## \r\n## \r\n## Bootstrap Statistics :\r\n##      original    bias    std. error\r\n## t1*    11.299  0.303557      5.9327\r\n## t2*     6.772 -0.055936      0.8620\r\n## t3*     0.713 -0.004048      0.0965\r\n## t4*     0.114 -0.000685      0.0137\r\n## t5*     4.063 -0.053784      2.0333\r\n## t6*     2.054 -0.001430      0.2411\r\n## t7*    -1.144  0.021373      2.8721\r\n## t8*     2.670  0.012132      0.2944\r\n## t9*     8.754 -0.109523      0.5500\r\n## t10*    0.666 -0.010924      0.0754\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We could use the bootstrapped standard error to get a normal approximation\nfor a significance test and confidence intervals for every parameter. However,\ninstead we will get the percentile and bias adjusted 95 percent confidence\nintervals, using the <code>boot.ci<\/code> function.<\/p>\n<div class=\"chunk\" id=\"unnamed-chunk-10\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\"># basic parameter estimates with percentile and bias adjusted CIs<\/span>\r\n<span class=\"hl std\">parms<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">t<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">sapply<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">1<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">3<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">5<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">7<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">9<\/span><span class=\"hl std\">),<\/span> <span class=\"hl kwa\">function<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">i<\/span><span class=\"hl std\">) {<\/span>\r\n    <span class=\"hl std\">out<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">boot.ci<\/span><span class=\"hl std\">(res,<\/span> <span class=\"hl kwc\">index<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(i, i<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">),<\/span> <span class=\"hl kwc\">type<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">&quot;perc&quot;<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">&quot;bca&quot;<\/span><span class=\"hl std\">))<\/span>\r\n    <span class=\"hl kwd\">with<\/span><span class=\"hl std\">(out,<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">Est<\/span> <span class=\"hl std\">= t0,<\/span> <span class=\"hl kwc\">pLL<\/span> <span class=\"hl std\">= percent[<\/span><span class=\"hl num\">4<\/span><span class=\"hl std\">],<\/span> <span class=\"hl kwc\">pUL<\/span> <span class=\"hl std\">= percent[<\/span><span class=\"hl num\">5<\/span><span class=\"hl std\">],<\/span> <span class=\"hl kwc\">bcaLL<\/span> <span class=\"hl std\">= bca[<\/span><span class=\"hl num\">4<\/span><span class=\"hl std\">],<\/span>\r\n        <span class=\"hl kwc\">bcaLL<\/span> <span class=\"hl std\">= bca[<\/span><span class=\"hl num\">5<\/span><span class=\"hl std\">]))<\/span>\r\n<span class=\"hl std\">}))<\/span>\r\n\r\n<span class=\"hl com\"># add row names<\/span>\r\n<span class=\"hl kwd\">row.names<\/span><span class=\"hl std\">(parms)<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">names<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(m))<\/span>\r\n<span class=\"hl com\"># print results<\/span>\r\n<span class=\"hl std\">parms<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                 Est    pLL    pUL   bcaLL  bcaLL\r\n## (Intercept)  11.299 -1.258 22.297 -3.7231 21.320\r\n## langscore     0.713  0.539  0.916  0.5508  0.944\r\n## progacademic  4.063  0.058  8.011  0.0842  8.043\r\n## progvocation -1.144 -6.805  4.277 -6.8436  4.250\r\n## sigma         8.754  7.674  9.792  7.8896 10.110\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The conclusions are the same as from the default model tests. You can compute\na rough estimate of the degree of association for the overall model,\nby correlating <code>achiv<\/code> with the predicted value and squaring the result.<\/p>\n<div class=\"chunk\" id=\"unnamed-chunk-11\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">dat<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">yhat<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">fitted<\/span><span class=\"hl std\">(m)<\/span>\r\n\r\n<span class=\"hl com\"># correlation<\/span>\r\n<span class=\"hl std\">(r<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">with<\/span><span class=\"hl std\">(dat,<\/span> <span class=\"hl kwd\">cor<\/span><span class=\"hl std\">(achiv, yhat)))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 0.552\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\"># rough variance accounted for<\/span>\r\n<span class=\"hl std\">r<\/span><span class=\"hl opt\">^<\/span><span class=\"hl num\">2<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 0.305\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The calculated value of .31 is rough estimate of the R<sup>2<\/sup> you would find in an OLS\nregression. The squared correlation between the observed and predicted\nacademic aptitude values is about 0.31, indicating that these predictors\naccounted for over 30% of the variability in the outcome variable.<\/p>\n<h2>Things to consider<\/h2>\n<ul>\n<li>The <code>truncreg<\/code> function is designed to work when the\n  truncation is on the outcome variable in the model. It is possible to\n  have samples that are truncated based on one or more predictors. For\n  example, modeling college GPA as a function of high school GPA (HSGPA)\n  and SAT scores involves a sample that is truncated based on the predictors,\n  i.e., only students with higher HSGPA and SAT scores are admitted into the college.<\/li>\n<li>You need to be careful about what value is used  as the truncation value,\n  because it affects the estimation of the coefficients and standard errors.\n  In the example above, if we had used <code>point = 39<\/code> instead of\n  <code>point = 40<\/code>, the results would have been slightly different.\n  It does not matter that there were no values of 40 in our sample.<\/li>\n<\/ul>\n<h2>References<\/h2>\n<ul>\n<li>Greene, W. H. (2003). <i>Econometric Analysis, Fifth Edition<\/i>.\n  Upper Saddle River, NJ: Prentice Hall.<\/li>\n<li>Heckman, J. J. (1979). Sample selection bias as a\n  specification error. <i>Econometrica<\/i>, Volume 47, Number 1, pages 153 - 161.<\/li>\n<li>Long, J. S. (1997). <i>Regression Models for Categorical and Limited\n  Dependent Variables.<\/i> Thousand Oaks, CA: Sage Publications.<\/li>\n<\/ul>\n<p><?php include \"stat\/footer.htm\"; ?>\n<\/body>\n<\/html><\/p>\n","protected":false},"excerpt":{"rendered":"<p>Truncated regression is used to model dependent variables for which some of the observations are not included in the analysis because of the value of the dependent variable. This page&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/truncated-regression\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":17,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-935","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/935","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=935"}],"version-history":[{"count":1,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/935\/revisions"}],"predecessor-version":[{"id":6608,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/935\/revisions\/6608"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=935"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}