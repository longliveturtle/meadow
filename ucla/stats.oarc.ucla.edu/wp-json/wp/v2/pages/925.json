{"id":925,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-multinomial-logistic-regression\/"},"modified":"2019-01-30T10:59:23","modified_gmt":"2019-01-30T18:59:23","slug":"multinomial-logistic-regression","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/multinomial-logistic-regression\/","title":{"rendered":"Multinomial Logistic Regression |  R Data Analysis Examples"},"content":{"rendered":"<p><!--[mathjax]--><\/p>\n<p>Multinomial logistic regression is used to model nominal outcome variables,\nin which the log odds of the outcomes are modeled as a linear\ncombination of the predictor variables.<\/p>\n<p>This page uses the following packages. Make sure that you can load\nthem before trying to run the examples on this page. If you do not have\na package installed, run: <code>install.packages(\"packagename\")<\/code>, or\nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div id=\"unnamed-chunk-2\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">require<\/span><span class=\"hl std\">(foreign)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(nnet)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(ggplot2)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(reshape2)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><strong>Version info: <\/strong>Code for this page was tested in R version 3.1.0 (2014-04-10)\nOn: 2014-06-13\nWith: reshape2 1.2.2; ggplot2 0.9.3.1; nnet 7.3-8; foreign 0.8-61; knitr 1.5\n<\/code><\/p>\n<p><strong>Please note:<\/strong> The purpose of this page is to show how to use various\ndata analysis commands. It does not cover all aspects of the research process\nwhich researchers are expected to do. In particular, it does not cover data\ncleaning and checking, verification of assumptions, model diagnostics or\npotential follow-up analyses.<\/p>\n<h2>Examples of multinomial logistic regression<\/h2>\n<p>Example 1. People&#8217;s occupational choices might be influenced\nby their parents&#8217; occupations and their own education level. We can study the\nrelationship of one&#8217;s occupation choice with education level and father&#8217;s\noccupation. The occupational choices will be the outcome variable which\nconsists of categories of occupations.<\/p>\n<p>Example 2. A biologist may be interested in food choices that alligators make.\nAdult alligators might have different preferences from young ones.\nThe outcome variable here will be the types of food, and the predictor\nvariables might be size of the alligators and other environmental variables.<\/p>\n<p>Example 3. Entering high school students make program choices among\ngeneral program, vocational program and academic program.\nTheir choice might be modeled using their writing score\nand their social economic status.<\/p>\n<h2>Description of the data<\/h2>\n<p>For our data analysis example, we will expand the third example using\nthe <code>hsbdemo<\/code> data set. Let&#8217;s first read in the data.<\/p>\n<div id=\"unnamed-chunk-3\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">ml<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">read.dta<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/hsbdemo.dta\"<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The data set contains variables on 200 students. The outcome variable\nis <code>prog<\/code>, program type. The predictor variables are social economic status,\n<code>ses<\/code>, a three-level categorical variable and writing score, <code>write<\/code>,\na continuous variable. Let&#8217;s start with getting some descriptive\nstatistics of the variables of interest.<\/p>\n<div id=\"unnamed-chunk-4\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">with<\/span><span class=\"hl std\">(ml,<\/span> <span class=\"hl kwd\">table<\/span><span class=\"hl std\">(ses, prog))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##         prog\r\n## ses      general academic vocation\r\n##   low         16       19       12\r\n##   middle      20       44       31\r\n##   high         9       42        7\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">with<\/span><span class=\"hl std\">(ml,<\/span> <span class=\"hl kwd\">do.call<\/span><span class=\"hl std\">(rbind,<\/span> <span class=\"hl kwd\">tapply<\/span><span class=\"hl std\">(write, prog,<\/span> <span class=\"hl kwa\">function<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span><span class=\"hl std\">)<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">M<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(x),<\/span> <span class=\"hl kwc\">SD<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">sd<\/span><span class=\"hl std\">(x)))))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##              M    SD\r\n## general  51.33 9.398\r\n## academic 56.26 7.943\r\n## vocation 46.76 9.319\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<h2>Analysis methods you might consider<\/h2>\n<ul>\n<li>Multinomial logistic regression, the focus of this page.<\/li>\n<li>Multinomial probit regression, similar to multinomial logistic\nregression with independent normal error terms.<\/li>\n<li>Multiple-group discriminant function analysis. A multivariate method for\nmultinomial outcome variables<\/li>\n<li>Multiple logistic regression analyses, one for each pair of outcomes:\nOne problem with this approach is that each analysis is potentially run on a\ndifferent sample. The other problem is that without constraining the\nlogistic models, we can end up with the probability of choosing all possible\noutcome categories greater than 1.<\/li>\n<li>Collapsing number of categories to two and then doing a logistic\nregression: This approach suffers from loss of information and changes the\noriginal research questions to very different ones.<\/li>\n<li>Ordinal logistic regression: If the outcome variable is truly ordered\nand if it also satisfies the assumption of proportional odds, then switching\nto ordinal logistic regression will make the model more parsimonious.<\/li>\n<li>Alternative-specific multinomial probit regression, which allows\ndifferent error structures therefore allows to relax the IIA assumption.\nThis requires that the data structure be choice-specific.<\/li>\n<li>Nested logit model, another way to relax the IIA assumption, also\nrequires the data structure be choice-specific.<\/li>\n<\/ul>\n<h2>Multinomial logistic regression<\/h2>\n<p>Below we use the <code>multinom<\/code> function from the <code>nnet<\/code>\npackage to estimate a multinomial logistic\nregression model. There are other functions in other R packages capable of\nmultinomial regression. We chose the <code>multinom<\/code> function because it does\nnot require the data to be reshaped (as the <code>mlogit<\/code> package does) and to\nmirror the example code found in Hilbe&#8217;s <em>Logistic Regression\nModels<\/em>. <\/p>\n<p>First, we need to choose the level of our outcome that we wish to use as our baseline and specify this in\nthe <code>relevel<\/code> function. Then, we run our model using <code>multinom<\/code>.\nThe <code>multinom<\/code> package does not include p-value calculation for the regression\ncoefficients, so we calculate p-values using Wald tests (here z-tests).<\/p>\n<div id=\"unnamed-chunk-5\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">ml<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">prog2<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">relevel<\/span><span class=\"hl std\">(ml<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">prog,<\/span> <span class=\"hl kwc\">ref<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"academic\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl std\">test<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">multinom<\/span><span class=\"hl std\">(prog2<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">ses<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">write,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= ml)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## # weights:  15 (8 variable)\r\n## initial  value 219.722458 \r\n## iter  10 value 179.982880\r\n## final  value 179.981726 \r\n## converged\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(test)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## Call:\r\n## multinom(formula = prog2 ~ ses + write, data = ml)\r\n## \r\n## Coefficients:\r\n##          (Intercept) sesmiddle seshigh    write\r\n## general        2.852   -0.5333 -1.1628 -0.05793\r\n## vocation       5.218    0.2914 -0.9827 -0.11360\r\n## \r\n## Std. Errors:\r\n##          (Intercept) sesmiddle seshigh   write\r\n## general        1.166    0.4437  0.5142 0.02141\r\n## vocation       1.164    0.4764  0.5956 0.02222\r\n## \r\n## Residual Deviance: 360 \r\n## AIC: 376\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">z<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(test)<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">coefficients<\/span><span class=\"hl opt\">\/<\/span><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(test)<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">standard.errors<\/span>\r\n<span class=\"hl std\">z<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##          (Intercept) sesmiddle seshigh  write\r\n## general        2.445   -1.2018  -2.261 -2.706\r\n## vocation       4.485    0.6117  -1.650 -5.113\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\"># 2-tailed z test<\/span>\r\n<span class=\"hl std\">p<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">(<\/span><span class=\"hl num\">1<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl kwd\">pnorm<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">abs<\/span><span class=\"hl std\">(z),<\/span> <span class=\"hl num\">0<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">))<\/span> <span class=\"hl opt\">*<\/span> <span class=\"hl num\">2<\/span>\r\n<span class=\"hl std\">p<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##          (Intercept) sesmiddle seshigh     write\r\n## general    1.448e-02    0.2294 0.02374 6.819e-03\r\n## vocation   7.299e-06    0.5408 0.09895 3.176e-07\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<ul>\n<li>We first see that some output is generated by running the model, even\nthough we are assigning the model to a new R object. This model-running\noutput includes some iteration history and includes the final negative\nlog-likelihood 179.981726. This value multiplied by two is then seen in the\nmodel summary as the Residual Deviance and it can be used in comparisons of\nnested models, but we won&#8217;t show an example of comparing models on this\npage.<\/li>\n<li>The model summary output has a block of coefficients and a block of\nstandard errors. Each of these blocks has one row of values corresponding to\na model equation. Focusing on the block of coefficients, we can look at the\nfirst row comparing <code>prog = \"general\"<\/code> to our baseline <code>prog = \"academic\"<\/code>\nand the second row comparing <code>prog = \"vocation\"<\/code> to our\nbaseline <code>prog = \"academic\"<\/code>. If we consider our coefficients from the\nfirst row to be \\(b_1\\) and our coefficients from the second row to be \\(b_2\\), we\ncan write our model equations:$$ln\\left(\\frac{P(prog=general)}{P(prog=academic)}\\right) = b_{10} + b_{11}(ses=2) + b_{12}(ses=3) + b_{13}write$$\n$$ln\\left(\\frac{P(prog=vocation)}{P(prog=academic)}\\right) = b_{20} + b_{21}(ses=2) + b_{22}(ses=3) + b_{23}write$$<\/p>\n<ul>\n<li>\\(b_{13}\\) A one-unit increase in the variable <code>write<\/code> is associated with\nthe decrease in the log odds of being in general program vs. academic\nprogram in the amount of .058 .<\/li>\n<li>\\(b_{23}\\) A one-unit increase in the variable <code>write<\/code> is associated with\nthe decrease in the log odds of being in vocation program vs. academic\nprogram. in the amount of .1136 .<\/li>\n<li>\\(b_{12}\\) The log odds of being in general program vs. in academic program\nwill decrease by 1.163 if moving from <code>ses=\"low\"<\/code> to <code>ses=\"high\"<\/code>.<\/li>\n<li>\\(b_{11}\\) The log odds of being in general program vs. in academic program\nwill decrease by 0.533 if moving from <code>ses=\"low\"<\/code>to <code>ses=\"middle\"<\/code>,\nalthough this coefficient is not significant.<\/li>\n<li>\\(b_{22}\\) The log odds of being in vocation program vs. in academic program\nwill decrease by 0.983 if moving from <code>ses=\"low\"<\/code> to <code>ses=\"high\"<\/code>.<\/li>\n<li>\\(b_{21}\\) The log odds of being in vocation program vs. in academic program\nwill increase by 0.291 if moving from <code>ses=\"low\"<\/code> to <code>ses=\"middle\"<\/code>,\nalthough this coefficient is not significant.<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>The ratio of the probability of choosing one outcome category over the\nprobability of choosing the baseline category is often referred as relative risk\n(and it is sometimes referred to as <em>odds<\/em>, described in the regression parameters above). The relative risk is the right-hand side linear equation exponentiated, leading to the fact that the exponentiated regression\ncoefficients are relative risk ratios for a unit change in the predictor\nvariable. We can exponentiate the coefficients from our model to see these\nrisk ratios.<\/p>\n<div id=\"unnamed-chunk-6\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## extract the coefficients from the model and exponentiate<\/span>\r\n<span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(test))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##          (Intercept) sesmiddle seshigh  write\r\n## general        17.33    0.5867  0.3126 0.9437\r\n## vocation      184.61    1.3383  0.3743 0.8926\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<ul>\n<li>The relative risk ratio for a one-unit increase in the variable <code>write<\/code>\nis .9437 for being in general program vs. academic program.<\/li>\n<li>The relative risk ratio switching from <code>ses<\/code> = 1 to 3 is .3126 for\nbeing in general program vs. academic program.<\/li>\n<\/ul>\n<p>You can also use predicted probabilities to help you understand the model.\nYou can calculate predicted probabilities for each of our outcome levels using the\n<code>fitted<\/code> function. We can start by generating the predicted probabilities\nfor the observations in our dataset and viewing the first few rows<\/p>\n<div id=\"unnamed-chunk-7\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">head<\/span><span class=\"hl std\">(pp<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">fitted<\/span><span class=\"hl std\">(test))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   academic general vocation\r\n## 1   0.1483  0.3382   0.5135\r\n## 2   0.1202  0.1806   0.6992\r\n## 3   0.4187  0.2368   0.3445\r\n## 4   0.1727  0.3508   0.4765\r\n## 5   0.1001  0.1689   0.7309\r\n## 6   0.3534  0.2378   0.4088\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Next, if we want to examine the changes in predicted probability associated\nwith one of our two variables, we can create small datasets varying one variable\nwhile holding the other constant. We will first do this holding <code>write<\/code> at\nits mean and examining the predicted probabilities for each level of <code>ses<\/code>.<\/p>\n<div id=\"unnamed-chunk-8\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">dses<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">data.frame<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">ses<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"low\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"middle\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"high\"<\/span><span class=\"hl std\">),<\/span> <span class=\"hl kwc\">write<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(ml<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">write))<\/span>\r\n<span class=\"hl kwd\">predict<\/span><span class=\"hl std\">(test,<\/span> <span class=\"hl kwc\">newdata<\/span> <span class=\"hl std\">= dses,<\/span> <span class=\"hl str\">\"probs\"<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   academic general vocation\r\n## 1   0.4397  0.3582   0.2021\r\n## 2   0.4777  0.2283   0.2939\r\n## 3   0.7009  0.1785   0.1206\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Another way to understand the model using the predicted probabilities is to\nlook at the averaged predicted probabilities for different values of the\ncontinuous predictor variable <code>write<\/code> within each level of <code>ses<\/code>.<\/p>\n<div id=\"unnamed-chunk-9\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">dwrite<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">data.frame<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">ses<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">rep<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"low\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"middle\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"high\"<\/span><span class=\"hl std\">),<\/span> <span class=\"hl kwc\">each<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">41<\/span><span class=\"hl std\">),<\/span> <span class=\"hl kwc\">write<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">rep<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">30<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">70<\/span><span class=\"hl std\">),<\/span>\r\n    <span class=\"hl num\">3<\/span><span class=\"hl std\">))<\/span>\r\n\r\n<span class=\"hl com\">## store the predicted probabilities for each value of ses and write<\/span>\r\n<span class=\"hl std\">pp.write<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(dwrite,<\/span> <span class=\"hl kwd\">predict<\/span><span class=\"hl std\">(test,<\/span> <span class=\"hl kwc\">newdata<\/span> <span class=\"hl std\">= dwrite,<\/span> <span class=\"hl kwc\">type<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"probs\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">se<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">TRUE<\/span><span class=\"hl std\">))<\/span>\r\n\r\n<span class=\"hl com\">## calculate the mean probabilities within each level of ses<\/span>\r\n<span class=\"hl kwd\">by<\/span><span class=\"hl std\">(pp.write[,<\/span> <span class=\"hl num\">3<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">5<\/span><span class=\"hl std\">], pp.write<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">ses, colMeans)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## pp.write$ses: high\r\n## academic  general vocation \r\n##   0.6164   0.1808   0.2028 \r\n## -------------------------------------------------------- \r\n## pp.write$ses: low\r\n## academic  general vocation \r\n##   0.3973   0.3278   0.2749 \r\n## -------------------------------------------------------- \r\n## pp.write$ses: middle\r\n## academic  general vocation \r\n##   0.4256   0.2011   0.3733\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Sometimes, a couple of plots can convey a good deal amount of information.\nUsing the predictions we generated for the pp.write object above, we can plot the predicted probabilities against the writing score by the\nlevel of <code>ses<\/code> for different levels of the outcome variable.<\/p>\n<div id=\"unnamed-chunk-10\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## melt data set to long for ggplot2<\/span>\r\n<span class=\"hl std\">lpp<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">melt<\/span><span class=\"hl std\">(pp.write,<\/span> <span class=\"hl kwc\">id.vars<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"ses\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"write\"<\/span><span class=\"hl std\">),<\/span> <span class=\"hl kwc\">value.name<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"probability\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl kwd\">head<\/span><span class=\"hl std\">(lpp)<\/span>  <span class=\"hl com\"># view first few rows<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   ses write variable probability\r\n## 1 low    30 academic     0.09844\r\n## 2 low    31 academic     0.10717\r\n## 3 low    32 academic     0.11650\r\n## 4 low    33 academic     0.12646\r\n## 5 low    34 academic     0.13705\r\n## 6 low    35 academic     0.14828\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## plot predicted probabilities across write values for each level of ses<\/span>\r\n<span class=\"hl com\">## facetted by program type<\/span>\r\n<span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(lpp,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span> <span class=\"hl std\">= write,<\/span> <span class=\"hl kwc\">y<\/span> <span class=\"hl std\">= probability,<\/span> <span class=\"hl kwc\">colour<\/span> <span class=\"hl std\">= ses))<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl kwd\">geom_line<\/span><span class=\"hl std\">()<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl kwd\">facet_grid<\/span><span class=\"hl std\">(variable<\/span> <span class=\"hl opt\">~<\/span>\r\n    <span class=\"hl std\">.,<\/span> <span class=\"hl kwc\">scales<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"free\"<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img loading=\"lazy\" decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/mlogit-unnamed-chunk-10.png\" alt=\"Predicted probabilities plot\" width=\"500\" height=\"500\" \/><\/div>\n<\/div>\n<h2>Things to consider<\/h2>\n<ul>\n<li>The Independence of Irrelevant Alternatives (IIA) assumption: Roughly,\nthe IIA assumption means that adding or deleting alternative outcome\ncategories does not affect the odds among the remaining outcomes. There are\nalternative modeling methods, such as alternative-specific multinomial probit model, or nested logit model to relax the IIA assumption.<\/li>\n<li>Diagnostics and model fit: Unlike logistic regression where there are many\nstatistics for performing model diagnostics, it is not as straightforward to\ndo diagnostics with multinomial logistic regression models. For the purpose of detecting outliers or influential data points, one can\nrun separate logit models and use the diagnostics tools on each model.<\/li>\n<li>Sample size: Multinomial regression uses a maximum likelihood estimation\nmethod, it requires a large sample size. It also uses multiple equations.\nThis implies that it requires an even larger sample size than ordinal or\nbinary logistic regression.<\/li>\n<li>Complete or quasi-complete separation: Complete separation means that\nthe outcome variable separate a predictor variable completely, leading\nperfect prediction by the predictor variable.<\/li>\n<li>Perfect prediction means that only one value of a predictor variable\nis associated with only one value of the response variable. But you can tell\nfrom the output of the regression coefficients that something is wrong. You\ncan then do a two-way tabulation of the outcome variable with the\nproblematic variable to confirm this and then rerun the model without the\nproblematic variable.<\/li>\n<li>Empty cells or small cells: You should check for empty or small cells\nby doing a cross-tabulation between categorical predictors and the outcome\nvariable. If a cell has very few cases (a small cell), the model may become\nunstable or it might not even run at all.<\/li>\n<\/ul>\n<h2>See also<\/h2>\n<ul>\n<li><a href=\"\/examples\/alr2\/\">Applied\nLogistic Regression (Second Edition)<\/a> by David Hosmer and Stanley\nLemeshow<\/li>\n<li><a href=\"\/examples\/icda\/\">An\nIntroduction to Categorical Data Analysis<\/a> by Alan Agresti<\/li>\n<li>Logistic Regression Models by Joseph M. Hilbe<\/li>\n<\/ul>\n","protected":false},"excerpt":{"rendered":"<p>Multinomial logistic regression is used to model nominal outcome variables, in which the log odds of the outcomes are modeled as a linear combination of the predictor variables. This page&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/multinomial-logistic-regression\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":7,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-925","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/925","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=925"}],"version-history":[{"count":10,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/925\/revisions"}],"predecessor-version":[{"id":27111,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/925\/revisions\/27111"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=925"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}