{"id":934,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-tobit-models\/"},"modified":"2017-02-15T10:18:49","modified_gmt":"2017-02-15T18:18:49","slug":"tobit-models","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/tobit-models\/","title":{"rendered":"Tobit Models |  R Data Analysis Examples"},"content":{"rendered":"<p>The tobit model, also called a censored regression model, is designed to estimate\nlinear relationships between variables when there is either left- or right-censoring\nin the dependent variable (also known as censoring from below and above,\nrespectively). Censoring from above takes place when cases with a value at or\nabove some threshold, all take on the value of that threshold, so\nthat the true value might be equal to the threshold, but it might also be higher.\nIn the case of censoring from below, values those that fall at or below some\nthreshold are censored. <\/p>\n<p>This page uses the following packages. Make sure that you can load \nthem before trying to run the examples on this page. If you do not have \na package installed, run: <code>install.packages(\"packagename\")<\/code>, or \nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">require<\/span>(ggplot2)\r\n<span class=\"functioncall\">require<\/span>(GGally)\r\n<span class=\"functioncall\">require<\/span>(VGAM)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><b>Version info: <\/b>Code for this page was tested in R Under development (unstable) (2012-11-16 r61126)<br \/>\n On: 2012-12-15<br \/>\nWith: VGAM 0.9-0; GGally 0.4.2; reshape 0.8.4; plyr 1.8; ggplot2 0.9.3; knitr 0.9\n<\/code><\/p>\n<p><strong>Please Note:<\/strong>  The purpose of this page is to show how to use various data analysis commands.\nIt does not cover all aspects of the research process which researchers are expected to do.  In\nparticular, it does not cover data cleaning and checking, verification of assumptions, model\ndiagnostics and potential follow-up analyses.<\/p>\n<h2>Examples of Tobit Analysis<\/h2>\n<p>Example 1.\nIn the 1980s there was a federal law restricting speedometer readings to no more than 85 mph.  So if\nyou wanted to try and predict a vehicle&#8217;s top-speed from a combination of horse-power and engine size,\nyou would get a reading no higher than 85, regardless of how fast the vehicle was really traveling.\nThis is a classic case of right-censoring (censoring from above) of the data.  The only thing we are certain of is that\nthose vehicles were traveling at least 85 mph.<\/p>\n<p>Example 2. A research project\nis studying the level of lead in home drinking water as a function of the age of\na house and family income. The water testing kit cannot detect lead\nconcentrations below 5 parts per billion (ppb). The EPA considers levels above\n15 ppb to be dangerous. These data are an example of left-censoring (censoring\nfrom below).<\/p>\n<p>Example 3. Consider the situation in which we have a measure of academic\naptitude (scaled 200-800) which we want to model using reading and math test\nscores, as well as, the type\nof program the student is enrolled in (academic, general, or vocational).  The problem here is that students who answer all questions on\nthe academic aptitude test correctly receive a score of 800, even though it is likely that these\nstudents are not &#8220;truly&#8221; equal in aptitude. The same is true of students who\nanswer all of the questions incorrectly. All such students would have a score of\n200, although they may not all be of equal aptitude.<\/p>\n<h2>Description of the Data<\/h2>\n<p>For our data analysis below, we are going to expand on Example 3 from above.\nWe have generated hypothetical data, which can be obtained\nfrom our website from within R. Note that R requires forward slashes, not back slashes\nwhen specifying a file location even if the file is on your hard drive.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">dat &lt;- <span class=\"functioncall\">read.csv<\/span>(<span class=\"string\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/tobit.csv\"<\/span>)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The dataset contains 200 observations.\nThe academic aptitude variable is <code>apt<\/code>, the reading and math test scores are\n<code>read<\/code> and <code>math<\/code> respectively.  The variable <code>prog<\/code>\nis the type of program the student is in, it is a categorical (nominal) variable that takes on three\nvalues, academic (<code>prog<\/code> = 1), general (<code>prog<\/code> = 2), and\nvocational (<code>prog<\/code> = 3). The variable <code>id<\/code> is an identification variable.<\/p>\n<p>Now let&#8217;s look at the data descriptively. Note that in this dataset, the lowest\nvalue of <code>apt<\/code> is 352. That is, no students received a score of 200 (the lowest\nscore possible), meaning that even though censoring from below was possible, it\ndoes not occur in the dataset.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">summary<\/span>(dat)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##        id             read           math              prog    \r\n##  Min.   :  1.0   Min.   :28.0   Min.   :33.0   academic  : 45  \r\n##  1st Qu.: 50.8   1st Qu.:44.0   1st Qu.:45.0   general   :105  \r\n##  Median :100.5   Median :50.0   Median :52.0   vocational: 50  \r\n##  Mean   :100.5   Mean   :52.2   Mean   :52.6                   \r\n##  3rd Qu.:150.2   3rd Qu.:60.0   3rd Qu.:59.0                   \r\n##  Max.   :200.0   Max.   :76.0   Max.   :75.0                   \r\n##       apt     \r\n##  Min.   :352  \r\n##  1st Qu.:576  \r\n##  Median :633  \r\n##  Mean   :640  \r\n##  3rd Qu.:705  \r\n##  Max.   :800\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\">\r\n<span class=\"comment\"># <span class=\"keyword\">function<\/span> that gives the density of normal distribution<\/span>\r\n<span class=\"comment\"># <span class=\"keyword\">for<\/span> given mean and sd, scaled to be on a count metric<\/span>\r\n<span class=\"comment\"># <span class=\"keyword\">for<\/span> the histogram: count = density * sample size * bin width<\/span>\r\nf &lt;- <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(x, var, bw = 15) {\r\n  <span class=\"functioncall\">dnorm<\/span>(x, mean = <span class=\"functioncall\">mean<\/span>(var), <span class=\"functioncall\">sd<\/span>(var)) * <span class=\"functioncall\">length<\/span>(var)  * bw\r\n}\r\n\r\n<span class=\"comment\"># setup base plot<\/span>\r\np &lt;- <span class=\"functioncall\">ggplot<\/span>(dat, <span class=\"functioncall\">aes<\/span>(x = apt, fill=prog))\r\n\r\n<span class=\"comment\"># histogram, coloured by proportion <span class=\"keyword\">in<\/span> different programs<\/span>\r\n<span class=\"comment\"># with a normal distribution overlayed<\/span>\r\np + <span class=\"functioncall\">stat_bin<\/span>(binwidth=15) +\r\n  <span class=\"functioncall\">stat_function<\/span>(fun = f, size = 1,\r\n    args = <span class=\"functioncall\">list<\/span>(var = dat$apt))\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/tobit-unnamed-chunk-4.png\" alt=\"plot of chunk unnamed-chunk-4\" width=\"500px\" height=\"500px\" alt=\"histogram filled by porportion in each program type\" class=\"plot\" \/><\/div>\n<\/div>\n<p>Looking at the above histogram, we can\nsee the censoring in the values of <code>apt<\/code>, that is, there are far\nmore cases with scores of 750 to 800 than\none would expect looking at the rest of the distribution. Below is an alternative histogram\nthat further highlights the excess of cases where <code>apt<\/code>=800. In the\nhistogram below, the <code>breaks<\/code> option produces a histogram where each\nunique value of <code>apt<\/code> has its own bar (by setting breaks equal to a vector\ncontaining values from the minimum of <code>apt<\/code> to the maximum of <code>apt<\/code>). Because\n<code>apt<\/code> is continuous, most values of <code>apt<\/code> are unique in the dataset,\nalthough close to the center of the distribution there are a few values of <code>apt<\/code>\nthat have two or three cases. The spike on the far right of the\nhistogram is the bar for cases where <code>apt<\/code>=800, the height of this bar\nrelative to all the others clearly shows the excess number of cases with this value.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">p + <span class=\"functioncall\">stat_bin<\/span>(binwidth = 1) + <span class=\"functioncall\">stat_function<\/span>(fun = f, size = 1, args = <span class=\"functioncall\">list<\/span>(var = dat$apt, \r\n    bw = 1))\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/tobit-unnamed-chunk-5.png\" alt=\"plot of chunk unnamed-chunk-5\" width=\"500px\" height=\"500px\" alt=\"another histogram with finer grain\" class=\"plot\" \/><\/div>\n<\/div>\n<p>Next we&#8217;ll explore the bivariate relationships in our dataset.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">cor<\/span>(dat[, <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"read\"<\/span>, <span class=\"string\">\"math\"<\/span>, <span class=\"string\">\"apt\"<\/span>)])\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##        read   math    apt\r\n## read 1.0000 0.6623 0.6451\r\n## math 0.6623 1.0000 0.7333\r\n## apt  0.6451 0.7333 1.0000\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\">\r\n<span class=\"comment\"># plot matrix<\/span>\r\n<span class=\"functioncall\">ggpairs<\/span>(dat[, <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"read\"<\/span>, <span class=\"string\">\"math\"<\/span>, <span class=\"string\">\"apt\"<\/span>)])\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/tobit-unnamed-chunk-6.png\" alt=\"plot of chunk unnamed-chunk-6\" width=\"500px\" height=\"500px\" alt=\"scatter plot matrix\" class=\"plot\" \/><\/div>\n<\/div>\n<p>In the first row of the scatterplot matrix shown above, we see the\nscatterplots showing the relationship between <code>read<\/code> and <code>apt<\/code>,\nas well as <code>math<\/code> and <code>apt<\/code>. Note the collection of cases at\nthe top these two scatterplots, this is due to the censoring in\nthe distribution of <code>apt<\/code>.<\/p>\n<h2>Analysis methods you might consider<\/h2>\n<p>Below is a list of some analysis methods you may have encountered.\nSome of the methods listed are quite reasonable while others have either\nfallen out of favor or have limitations.<\/p>\n<ul>\n<li>Tobit regression, the focus of this page.<\/li>\n<li>OLS Regression &#8211; You could analyze these data using OLS regression. OLS\n\tregression will treat the 800 as actual values and not as the lower limit of the top academic\n\taptitude. A limitation of this approach is that when the variable is censored, OLS provides inconsistent\n\testimates of the parameters, meaning that the coefficients from the analysis\n\twill not necessarily approach the &#8220;true&#8221; population parameters as the sample size\n\tincreases. See Long (1997, chapter 7) for a more detailed discussion of problems of\n\tusing OLS regression with censored data.<\/li>\n<li>Truncated Regression &#8211; There is sometimes confusion about the difference\n\tbetween truncated data and censored data. With censored\n\tvariables, all of the observations are in the dataset, but we don&#8217;t know the &#8220;true&#8221;\n\tvalues of some of them. With truncation some of the observations are not\n\tincluded in the analysis because of the value of the variable. When a\n\tvariable is censored, regression models for truncated data provide inconsistent estimates of the parameters. See Long\n\t(1997, chapter 7) for a more detailed discussion of problems of using\n\tregression models for truncated data to analyze censored data.<\/li>\n<\/ul>\n<h2>Tobit regression<\/h2>\n<p>Below we run the tobit model, using the <code>vglm<\/code> function of the VGAM package.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">summary<\/span>(m &lt;- <span class=\"functioncall\">vglm<\/span>(apt ~ read + math + prog, <span class=\"functioncall\">tobit<\/span>(Upper = 800), data = dat))\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## vglm(formula = apt ~ read + math + prog, family = tobit(Upper = 800), \r\n##     data = dat)\r\n## \r\n## Pearson Residuals:\r\n##          Min    1Q Median   3Q Max\r\n## mu      -2.6 -0.76 -0.051 0.79 4.1\r\n## log(sd) -1.1 -0.62 -0.369 0.25 5.4\r\n## \r\n## Coefficients:\r\n##                Estimate Std. Error z value\r\n## (Intercept):1     209.6     32.457     6.5\r\n## (Intercept):2       4.2      0.053    79.4\r\n## read                2.7      0.618     4.4\r\n## math                5.9      0.705     8.4\r\n## proggeneral       -12.7     12.355    -1.0\r\n## progvocational    -46.1     13.770    -3.4\r\n## \r\n## Number of linear predictors:  2 \r\n## \r\n## Names of linear predictors: mu, log(sd)\r\n## \r\n## Dispersion Parameter for tobit family:   1\r\n## \r\n## Log-likelihood: -1041 on 394 degrees of freedom\r\n## \r\n## Number of iterations: 4\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<ul>\n<li>In the output above, the first thing we see is the call, this is R\n  reminding us what the model we ran was, what options we specified, etc.<\/li>\n<li>The table labeled coefficients gives the coefficients, their standard errors,\n  and the z-statistic. No p-values are included in the summary table, but we show how to calculate\n  them below.  Tobit regression coefficients are interpreted in the similar\n  manner to OLS regression coefficients; however, the linear effect is on the\n  uncensored latent variable, not the observed outcome. See McDonald and Moffitt (1980) for more details.<\/p>\n<ul>\n<li>For a one unit increase in <code>read<\/code>, there is a <code class=\"knitr inline\">2.6981<\/code>\n    point increase in the predicted value of <code>apt<\/code>.<\/li>\n<li>A one unit increase in <code>math<\/code> is associated with a <code class=\"knitr inline\">5.9146<\/code>\n    unit increase in the predicted value of <code>apt<\/code>.<\/li>\n<li>The terms for <code>prog<\/code> have a slightly different interpretation. The predicted value of\n    <code>apt<\/code> is <code class=\"knitr inline\">-46.1419<\/code> points lower for\n    students in a vocational program than for students in an academic program.<\/li>\n<li>The coefficient labeled &#8220;(Intercept):1&#8221; is the intercept or constant for the model.<\/li>\n<li>The coefficient labeled &#8220;(Intercept):2&#8221; is an ancillary statistic.  If we\n    exponentiate this value, we get a statistic that is analogous to the square\n    root of the residual variance in OLS regression. The value of <code class=\"knitr inline\">65.6773<\/code>\n    can compared to the standard deviation of academic aptitude which was 99.21,\n    a substantial reduction.<\/li>\n<\/ul>\n<\/li>\n<li>The final log likelihood, <code class=\"knitr inline\">-1041.0629<\/code>, is shown\n  toward the bottom of the output, it can be used in comparisons of nested models.<\/li>\n<\/ul>\n<p>Below we calculate the p-values for each of the coefficients in the model. We\ncalculate the p-value for each coefficient using the z values and then display in\na table with the coefficients. The coefficients for <code>read<\/code>, <code>math<\/code>,\nand <code>prog<\/code> = 3 (vocational) are statistically significant.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">ctable &lt;- <span class=\"functioncall\">coef<\/span>(<span class=\"functioncall\">summary<\/span>(m))\r\npvals &lt;- 2 * <span class=\"functioncall\">pt<\/span>(<span class=\"functioncall\">abs<\/span>(ctable[, <span class=\"string\">\"z value\"<\/span>]), <span class=\"functioncall\">df.residual<\/span>(m), lower.tail = <span class=\"keyword\">FALSE<\/span>)\r\n<span class=\"functioncall\">cbind<\/span>(ctable, pvals)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                Estimate Std. Error z value      pvals\r\n## (Intercept):1   209.555   32.45655   6.456  3.157e-10\r\n## (Intercept):2     4.185    0.05268  79.432 1.408e-244\r\n## read              2.698    0.61808   4.365  1.625e-05\r\n## math              5.915    0.70480   8.392  8.673e-16\r\n## proggeneral     -12.716   12.35467  -1.029  3.040e-01\r\n## progvocational  -46.142   13.76971  -3.351  8.831e-04\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can test the significant of program type overall by fitting a model\nwithout program in it and using a likelihood ratio test.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">m2 &lt;- <span class=\"functioncall\">vglm<\/span>(apt ~ read + math, <span class=\"functioncall\">tobit<\/span>(Upper = 800), data = dat)\r\n\r\n(p &lt;- <span class=\"functioncall\">pchisq<\/span>(2 * (<span class=\"functioncall\">logLik<\/span>(m) - <span class=\"functioncall\">logLik<\/span>(m2)), df = 2, lower.tail = <span class=\"keyword\">FALSE<\/span>))\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 0.003155\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The LRT with two degrees of freedom is associated with\na p-value of <code class=\"knitr inline\">0.0032<\/code>, indicating that the overall effect\nof <code>prog<\/code> is statistically significant.<\/p>\n<p>Below we calculate the upper and lower 95% confidence intervals for the coefficients.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">b &lt;- <span class=\"functioncall\">coef<\/span>(m)\r\nse &lt;- <span class=\"functioncall\">sqrt<\/span>(<span class=\"functioncall\">diag<\/span>(<span class=\"functioncall\">vcov<\/span>(m)))\r\n\r\n<span class=\"functioncall\">cbind<\/span>(LL = b - <span class=\"functioncall\">qnorm<\/span>(0.975) * se, UL = b + <span class=\"functioncall\">qnorm<\/span>(0.975) * se)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                     LL      UL\r\n## (Intercept):1  145.941 273.169\r\n## (Intercept):2    4.081   4.288\r\n## read             1.487   3.909\r\n## math             4.533   7.296\r\n## proggeneral    -36.930  11.499\r\n## progvocational -73.130 -19.154\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We may also wish to examine how well our model fits the data. One way\nto start is with plots of the residuals to assess their absolute as well\nas relative (pearson) values and assumptions such as normality and homogeneity\nof variance.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">dat$yhat &lt;- <span class=\"functioncall\">fitted<\/span>(m)[,1]\r\ndat$rr &lt;- <span class=\"functioncall\">resid<\/span>(m, type = <span class=\"string\">\"response\"<\/span>)\r\ndat$rp &lt;- <span class=\"functioncall\">resid<\/span>(m, type = <span class=\"string\">\"pearson\"<\/span>)[,1]\r\n\r\n<span class=\"functioncall\">par<\/span>(mfcol = <span class=\"functioncall\">c<\/span>(2, 3))\r\n\r\n<span class=\"functioncall\">with<\/span>(dat, {\r\n  <span class=\"functioncall\">plot<\/span>(yhat, rr, main = <span class=\"string\">\"Fitted vs Residuals\"<\/span>)\r\n  <span class=\"functioncall\">qqnorm<\/span>(rr)\r\n  <span class=\"functioncall\">plot<\/span>(yhat, rp, main = <span class=\"string\">\"Fitted vs Pearson Residuals\"<\/span>)\r\n  <span class=\"functioncall\">qqnorm<\/span>(rp)\r\n  <span class=\"functioncall\">plot<\/span>(apt, rp, main = <span class=\"string\">\"Actual vs Pearson Residuals\"<\/span>)\r\n  <span class=\"functioncall\">plot<\/span>(apt, yhat, main = <span class=\"string\">\"Actual vs Fitted\"<\/span>)\r\n})\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/tobit-unnamed-chunk-11.png\" alt=\"plot of chunk unnamed-chunk-11\" width=\"500px\" height=\"500px\" alt=\"Diagnostic plots of actual versus residual values\" class=\"plot\" \/><\/div>\n<\/div>\n<p>The graph in the bottom right was the predicted, or fitted, values\nplotted against the actual. This can be\nparticularly useful when comparing competing models. We can calculate\nthe correlation between these two as well as the squared correlation,\nto get a sense of how accurate our model predicts the data and how\nmuch of the variance in the outcome is accounted for by the model.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\"># correlation<\/span>\r\n(r &lt;- <span class=\"functioncall\">with<\/span>(dat, <span class=\"functioncall\">cor<\/span>(yhat, apt)))\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 0.7825\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\">\r\n<span class=\"comment\"># variance accounted <span class=\"keyword\">for<\/span><\/span>\r\nr^2\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 0.6123\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The correlation between the predicted and observed values of <code>apt<\/code> is\n <code class=\"knitr inline\">0.7825<\/code>. If we square this value, we get the multiple squared\ncorrelation, this indicates predicted values share\n <code class=\"knitr inline\">61.23%<\/code>\nof their variance with <code>apt<\/code>.<\/p>\n<h2>References<\/h2>\n<p>Long, J. S. 1997. <i>Regression Models for Categorical and Limited Dependent Variables.<\/i>\n\t\tThousand Oaks, CA: Sage Publications.<\/p>\n<p>McDonald, J. F. and Moffitt, R. A.  1980.  The Uses of Tobit Analysis.  <i>The Review of Economics and Statistics<\/i>\nVol 62(2):  318-321.<\/p>\n<p>Tobin, J. 1958. Estimation of relationships for limited dependent variables.\n\t\t<i>Econometrica<\/i> 26: 24-36.<\/p>\n<p><?php include \"stat\/footer.htm\"; ?>\n<\/body>\n<\/html><\/p>\n","protected":false},"excerpt":{"rendered":"<p>The tobit model, also called a censored regression model, is designed to estimate linear relationships between variables when there is either left- or right-censoring in the dependent variable (also known&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/tobit-models\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":16,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-934","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/934","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=934"}],"version-history":[{"count":2,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/934\/revisions"}],"predecessor-version":[{"id":25606,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/934\/revisions\/25606"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=934"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}