{"id":939,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-zero-truncated-poisson\/"},"modified":"2017-01-04T12:00:30","modified_gmt":"2017-01-04T19:00:30","slug":"zero-truncated-poisson","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/zero-truncated-poisson\/","title":{"rendered":"Zero-Truncated Poisson |  R Data Analysis Examples"},"content":{"rendered":"<p>Zero-truncated poisson regression is used to model count data for which the value zero cannot occur.<\/p>\n<p>This page uses the following packages. Make sure that you can load\nthem before trying to run the examples on this page. If you do not have\na package installed, run: <code>install.packages(\"packagename\")<\/code>, or\nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">require<\/span>(foreign)\r\n<span class=\"functioncall\">require<\/span>(ggplot2)\r\n<span class=\"functioncall\">require<\/span>(VGAM)\r\n<span class=\"functioncall\">require<\/span>(boot)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><b>Version info: <\/b>Code for this page was tested in R Under development (unstable) (2012-11-16 r61126)\nOn: 2012-12-15\nWith: boot 1.3-7; VGAM 0.9-0; ggplot2 0.9.3; foreign 0.8-51; knitr 0.9\n<\/code><\/p>\n<p><strong>Please Note:<\/strong> The purpose of this page is to show how to use various data analysis commands.\nIt does not cover all aspects of the research process which researchers are expected to do. In\nparticular, it does not cover data cleaning and verification, verification of assumptions, model\ndiagnostics and potential follow-up analyses.<\/p>\n<h2>Examples of zero-truncated Poisson regression<\/h2>\n<p>Example 1. A study of length of hospital stay, in days, as a function\nof age, kind of health insurance and whether or not the patient died while in the hospital.\nLength of hospital stay is recorded as a minimum of at least one day.<\/p>\n<p>Example 2. A study of the number of journal articles published by\ntenured faculty as a function of discipline (fine arts, science, social science,\nhumanities, medical, etc). To get tenure faculty must publish, therefore,\nthere are no tenured faculty with zero publications.<\/p>\n<p>Example 3. A study by the county traffic court on the number of tickets received by teenagers\nas predicted by school performance, amount of driver training and gender. Only individuals\nwho have received at least one citation are in the traffic court files.<\/p>\n<h2>Description of the data<\/h2>\n<p>Let&#8217;s pursue Example 1 from above.<\/p>\n<p>We have a <strong>hypothetical<\/strong> data file, <code>ztp.dta<\/code> with 1,493 observations.\nThe length of hospital stay variable is <code>stay<\/code>.\nThe variable <code>age<\/code> gives the age group from 1 to 9 which will be treated as\ninterval in this example. The variables <code>hmo<\/code> and <code>died<\/code> are binary indicator variables\nfor HMO insured patients and patients who died while in the hospital, respectively.<\/p>\n<p>Let&#8217;s look at the data. We import the Stata dataset using\nthe <code>foreign<\/code> package.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">dat &lt;- <span class=\"functioncall\">read.dta<\/span>(<span class=\"string\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/ztp.dta\"<\/span>)\r\n\r\ndat &lt;- <span class=\"functioncall\">within<\/span>(dat, {\r\n    hmo &lt;- <span class=\"functioncall\">factor<\/span>(hmo)\r\n    died &lt;- <span class=\"functioncall\">factor<\/span>(died)\r\n})\r\n\r\n<span class=\"functioncall\">summary<\/span>(dat)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##       stay            age       hmo      died   \r\n##  Min.   : 1.00   Min.   :1.00   0:1254   0:981  \r\n##  1st Qu.: 4.00   1st Qu.:4.00   1: 239   1:512  \r\n##  Median : 8.00   Median :5.00                   \r\n##  Mean   : 9.73   Mean   :5.23                   \r\n##  3rd Qu.:13.00   3rd Qu.:6.00                   \r\n##  Max.   :74.00   Max.   :9.00\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Now let&#8217;s look at some graphs of the data conditional on various\ncombinations of the variables to get a sense of how the variables work together.\nWe will use the <code>ggplot2<\/code> package. First we can look at histograms of\n<code>stay<\/code> broken down by <code>hmo<\/code> on the rows and <code>died<\/code> on the columns.\nWe also include the marginal distributions, thus the lower right corner represents\nthe overall histogram. We use a log base 10 scale to approximate the canonical link function of\nthe poisson distribution (natural logarithm).<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(dat, <span class=\"functioncall\">aes<\/span>(stay)) +\r\n  <span class=\"functioncall\">geom_histogram<\/span>() +\r\n  <span class=\"functioncall\">scale_x_log10<\/span>() +\r\n  <span class=\"functioncall\">facet_grid<\/span>(hmo ~ died, margins=<span class=\"keyword\">TRUE<\/span>, scales=<span class=\"string\">\"free_y\"<\/span>)\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## stat_bin: binwidth defaulted to range\/30. Use 'binwidth = x' to adjust\r\n## this.\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztp-unnamed-chunk-4.png\" alt=\"plot of chunk unnamed-chunk-4\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>From the histograms, it looks like the density of the distribution,\ndoes vary across levels of <code>hmo<\/code> and <code>died<\/code>, with\nshorter stays for those in HMOs (1) and shorter for those who did die,\nincluding what seems to be an inflated number of 1 day stays.\nTo examine how <code>stay<\/code> varies across age groups, we can use conditional\nviolin plots which show a kernel density estimate of the distribution of stay\nmirrored (hence the violin) and conditional on each age group. To further understand\nthe raw data going into each density estimate, we add raw data on top of the violin plots\nwith a small amount of random noise (jitter) to alleviate over plotting. Finally, to get a\nsense of the overall trend, we add a locally weighted regression line.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(dat, <span class=\"functioncall\">aes<\/span>(<span class=\"functioncall\">factor<\/span>(age), stay)) +\r\n  <span class=\"functioncall\">geom_violin<\/span>() +\r\n  <span class=\"functioncall\">geom_jitter<\/span>(size=1.5) +\r\n  <span class=\"functioncall\">scale_y_log10<\/span>() +\r\n  <span class=\"functioncall\">stat_smooth<\/span>(<span class=\"functioncall\">aes<\/span>(x = age, y = stay, group=1), method=<span class=\"string\">\"loess\"<\/span>)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztp-unnamed-chunk-5.png\" alt=\"plot of chunk unnamed-chunk-5\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>The distribution of length of stay does not seem to vary much across age groups.\nThis observation from the raw data is corroborated by the relatively flat loess line.\nFinally let&#8217;s look at the proportion of people who lived or died across age groups\nby whether or not they are in HMOs.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(dat, <span class=\"functioncall\">aes<\/span>(age, fill=died)) +\r\n  <span class=\"functioncall\">geom_histogram<\/span>(binwidth=.5, position=<span class=\"string\">\"fill\"<\/span>) +\r\n  <span class=\"functioncall\">facet_grid<\/span>(hmo ~ ., margins=<span class=\"keyword\">TRUE<\/span>)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztp-unnamed-chunk-6.png\" alt=\"plot of chunk unnamed-chunk-6\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>For the lowest ages, a smaller proportion of people in HMOs died, but\nfor higher ages, there does not seem to be a huge difference, with a\nslightly higher proportion in HMOs dying if anything. Overall, as\nage group increases, the proportion of those dying increases, as expected.<\/p>\n<h2>Analysis methods you might consider<\/h2>\n<p>Below is a list of some analysis methods you may have encountered.\nSome of the methods listed are quite reasonable while others have either fallen out of favor or\nhave limitations.<\/p>\n<ul>\n<li>Zero-truncated Poisson Regression &#8211; The focus of this web page.<\/li>\n<li>Zero-truncated Negative Binomial Regression &#8211; If you have overdispersion in addition to\nzero truncation. See the Data Analysis Example for <a href=\"\/r\/dae\/zero-truncated-negative-binomial\/\">ztnb<\/a>.<\/li>\n<li>Poisson Regression &#8211; Ordinary Poisson regression will have difficulty with\nzero-truncated data. It will try to predict zero counts even though there are\nno zero values.<\/li>\n<li>Negative Binomial Regression &#8211; Ordinary Negative Binomial regression will have difficulty with\nzero-truncated data. It will try to predict zero counts even though there are\nno zero values.<\/li>\n<li>OLS Regression &#8211; You could try to analyze these data using OLS regression. However, count\ndata are highly non-normal and are not well estimated by OLS regression.<\/li>\n<\/ul>\n<h2>Zero-truncated Poisson regression<\/h2>\n<p>To fit the zero-truncated poisson model, we use the <code>vglm<\/code> function\nin the <code>VGAM<\/code> package. This function fits a very flexible class of models\ncalled vector generalized linear models to a wide range of assumed distributions.\nIn our case, we believe the data are poisson, but without zeros. Thus the values are\nstrictly positive poisson, for which we use the positive poisson family via the\n<code>pospoisson<\/code> function passed to <code>vglm<\/code>.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">m1 &lt;- <span class=\"functioncall\">vglm<\/span>(stay ~ age + hmo + died, family = <span class=\"functioncall\">pospoisson<\/span>(), data = dat)\r\n<span class=\"functioncall\">summary<\/span>(m1)\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## vglm(formula = stay ~ age + hmo + died, family = pospoisson(), \r\n##     data = dat)\r\n## \r\n## Pearson Residuals:\r\n##             Min   1Q Median   3Q Max\r\n## log(lambda)  -3 -1.7  -0.59 0.98  21\r\n## \r\n## Coefficients:\r\n##             Estimate Std. Error z value\r\n## (Intercept)    2.436      0.027    89.1\r\n## age           -0.014      0.005    -2.9\r\n## hmo1          -0.136      0.024    -5.7\r\n## died1         -0.204      0.018   -11.1\r\n## \r\n## Number of linear predictors:  1 \r\n## \r\n## Name of linear predictor: log(lambda) \r\n## \r\n## Dispersion Parameter for pospoisson family:   1\r\n## \r\n## Log-likelihood: -6909 on 1489 degrees of freedom\r\n## \r\n## Number of iterations: 4\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The output looks very much like the output from an OLS regression:<\/p>\n<ul>\n<li>It begins by echoing the function call showing us what we modeled.<\/li>\n<li>Next comes the spread of the residuals, there is at least one high\nresidual because the max is much higher than the min.<\/li>\n<li>Following the residuals are the parameter estimates and standard errors. The\nz values (frac{Estimate}{SE}) are also printed. No p values are given, although\nif one wished to assume that the estimates followed the normal distribution, one\ncould easily compute the probability of obtaining that z value.<\/li>\n<li>The number of linear predictors is printed because <b>vglm<\/b> can fit far more\ncomplex models, for this page, this will always be one, the expected mean, (lambda).<\/li>\n<li>Next the dispersion parameter is printed, for the Poisson, this is\nassumed to be 1 and is not estimated.<\/li>\n<li>Finally the log likelihood <code class=\"knitr inline\">-6908.7991<\/code> is printed along with the number of iterations\nneeded to reach convergence.<\/li>\n<\/ul>\n<p>Now let&#8217;s look at a plot of the residuals versus fitted values. We add random horizontal\nnoise as well as 50 percent transparency to alleviate over plotting and better see where\n<i>most<\/i> residuals fall.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">output &lt;- <span class=\"functioncall\">data.frame<\/span>(resid = <span class=\"functioncall\">resid<\/span>(m1), fitted = <span class=\"functioncall\">fitted<\/span>(m1))\r\n<span class=\"functioncall\">ggplot<\/span>(output, <span class=\"functioncall\">aes<\/span>(fitted, resid)) +\r\n  <span class=\"functioncall\">geom_jitter<\/span>(position=<span class=\"functioncall\">position_jitter<\/span>(width=.25), alpha=.5) +\r\n  <span class=\"functioncall\">stat_smooth<\/span>(method=<span class=\"string\">\"loess\"<\/span>)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztp-unnamed-chunk-8.png\" alt=\"plot of chunk unnamed-chunk-8\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>The mean is around zero across all the fitted levels it looks like. However,\nthere are some values that look rather extreme. To see if these have much influence,\nwe can fit lines using quantile regression, these lines represent the 75th, 50th, and 25th\npercentiles.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">ggplot<\/span>(output, <span class=\"functioncall\">aes<\/span>(fitted, resid)) +\r\n  <span class=\"functioncall\">geom_jitter<\/span>(position=<span class=\"functioncall\">position_jitter<\/span>(width=.25), alpha=.5) +\r\n  <span class=\"functioncall\">stat_quantile<\/span>(method=<span class=\"string\">\"rq\"<\/span>)\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## Smoothing formula not specified. Using: y ~ x\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztp-unnamed-chunk-9.png\" alt=\"plot of chunk unnamed-chunk-9\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>Here we see the spread narrowing at higher levels. Let&#8217;s cut the data\ninto intervals and check box plots for each. We will get the breaks\nfrom the algorithm for a histogram.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">output &lt;- <span class=\"functioncall\">within<\/span>(output, {\r\n  broken &lt;- <span class=\"functioncall\">cut<\/span>(fitted, <span class=\"functioncall\">hist<\/span>(fitted, plot=<span class=\"keyword\">FALSE<\/span>)$breaks)\r\n})\r\n\r\n<span class=\"functioncall\">ggplot<\/span>(output, <span class=\"functioncall\">aes<\/span>(broken, resid)) +\r\n <span class=\"functioncall\">geom_boxplot<\/span>() +\r\n <span class=\"functioncall\">geom_jitter<\/span>(alpha=.25)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztp-unnamed-chunk-10.png\" alt=\"plot of chunk unnamed-chunk-10\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>Now that feel a little more confident the model fits okay, let&#8217;s look\nat the coefficients.<\/p>\n<ul>\n<li>The value of the coefficient for <code>age<\/code>,\n<code class=\"knitr inline\">-0.0144<\/code> suggests that the log count of\nstay decreases by <code class=\"knitr inline\">0.0144<\/code> for each year increase in age.<\/li>\n<li>The coefficient for <code>hmo<\/code>,\n<code class=\"knitr inline\">-0.1359<\/code> indicates that the log count of stay\nfor HMO patient is <code class=\"knitr inline\">0.1359<\/code> less than for non-HMO patients.<\/li>\n<li>The log count of stay for patients who died while in the hospital was\n<code class=\"knitr inline\">0.2038<\/code> less than those patients who did not die.<\/li>\n<li>Finally, the value of the constant <code class=\"knitr inline\">2.4358<\/code> is\nthe log count of the stay when all of the predictors equal zero.<\/li>\n<\/ul>\n<p>We can get confidence intervals for the parameters and the\nexponentiated parameters using bootstrapping. For the Poisson model, these would\nbe incident risk ratios. We use the <code>boot<\/code> package.\nFirst, we get the coefficients from our original model to\nuse as start values for the model to speed up the time it takes to estimate. Then\nwe write a short function that takes data and indices as input and returns the\nparameters we are interested in. Finally, we pass that\nto the <code>boot<\/code> function and do 1200 replicates, using snow to distribute across\nfour cores. Note that you should adjust the number of cores to whatever your machine\nhas. Also, for final results, one may wish to increase the number of replications to\nhelp ensure stable results.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"functioncall\">dput<\/span>(<span class=\"functioncall\">round<\/span>(<span class=\"functioncall\">coef<\/span>(m1),3))\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## structure(c(2.436, -0.014, -0.136, -0.204), .Names = c(\"(Intercept)\", \r\n## \"age\", \"hmo1\", \"died1\"))\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\">f &lt;- <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(data, i) {\r\n  <span class=\"functioncall\">require<\/span>(VGAM)\r\n  m &lt;- <span class=\"functioncall\">vglm<\/span>(formula = stay ~ age + hmo + died, family = <span class=\"functioncall\">pospoisson<\/span>(),\r\n    data = data[i, ], coefstart = <span class=\"functioncall\">c<\/span>(2.436, -0.014, -0.136, -0.204))\r\n  <span class=\"functioncall\">as.vector<\/span>(<span class=\"functioncall\">t<\/span>(<span class=\"functioncall\">coef<\/span>(<span class=\"functioncall\">summary<\/span>(m))[, 1:2]))\r\n}\r\n\r\n<span class=\"functioncall\">set.seed<\/span>(10)\r\nres &lt;- <span class=\"functioncall\">boot<\/span>(dat, f, R = 1200, parallel = <span class=\"string\">\"snow\"<\/span>, ncpus = 4)\r\n\r\n<span class=\"comment\">## print results<\/span>\r\nres\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## ORDINARY NONPARAMETRIC BOOTSTRAP\r\n## \r\n## \r\n## Call:\r\n## boot(data = dat, statistic = f, R = 1200, parallel = \"snow\", \r\n##     ncpus = 4)\r\n## \r\n## \r\n## Bootstrap Statistics :\r\n##      original     bias    std. error\r\n## t1*  2.435808  2.124e-04   6.962e-02\r\n## t2*  0.027332  8.324e-06   5.621e-04\r\n## t3* -0.014442 -5.729e-05   1.212e-02\r\n## t4*  0.005035  2.490e-06   9.896e-05\r\n## t5* -0.135903  1.162e-03   5.105e-02\r\n## t6*  0.023742  1.313e-05   7.497e-04\r\n## t7* -0.203771 -1.487e-03   4.984e-02\r\n## t8*  0.018373  3.854e-05   3.556e-04\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The results are alternating parameter estimates and standard\nerrors. That is, the first row has the first parameter estimate\nfrom our model. The second has the standard error for the\nfirst parameter. The third column contains the bootstrapped\nstandard errors.<\/p>\n<p>Now we can get the confidence intervals for all the parameters.\nWe start on the original scale with percentile and basic bootstrap CIs.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## basic parameter estimates with percentile and bias adjusted CIs<\/span>\r\nparms &lt;- <span class=\"functioncall\">t<\/span>(<span class=\"functioncall\">sapply<\/span>(<span class=\"functioncall\">c<\/span>(1, 3, 5, 7), <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(i) {\r\n  out &lt;- <span class=\"functioncall\">boot.ci<\/span>(res, index = <span class=\"functioncall\">c<\/span>(i, i + 1), type = <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"perc\"<\/span>, <span class=\"string\">\"basic\"<\/span>))\r\n  <span class=\"functioncall\">with<\/span>(out, <span class=\"functioncall\">c<\/span>(Est = t0, pLL = percent[4], pUL = percent[5],\r\n    basicLL = basic[4], basicLL = basic[5]))\r\n}))\r\n\r\n<span class=\"comment\">## add row names<\/span>\r\n<span class=\"functioncall\">row.names<\/span>(parms) &lt;- <span class=\"functioncall\">names<\/span>(<span class=\"functioncall\">coef<\/span>(m1))\r\n<span class=\"comment\">## print results<\/span>\r\nparms\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                  Est      pLL       pUL  basicLL  basicLL\r\n## (Intercept)  2.43581  2.29933  2.577805  2.29381  2.57228\r\n## age         -0.01444 -0.04003  0.009798 -0.03868  0.01114\r\n## hmo1        -0.13590 -0.23875 -0.038074 -0.23373 -0.03306\r\n## died1       -0.20377 -0.30530 -0.106277 -0.30126 -0.10224\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The bootstrapped confidence intervals are wider than would be expected using a\nnormal based approximation. The bootstrapped CIs are more consistent with\nthe CIs from Stata when using robust standard errors.<\/p>\n<p>Now we can estimate the incident risk ratio (IRR) for the Poisson model.\nThis is done using almost identical code as before,\nbut passing a transformation function to the <code>h<\/code> argument of\n<code>boot.ci<\/code>, in this case, <code>exp<\/code> to exponentiate.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## exponentiated parameter estimates with percentile and bias adjusted CIs<\/span>\r\nexpparms &lt;- <span class=\"functioncall\">t<\/span>(<span class=\"functioncall\">sapply<\/span>(<span class=\"functioncall\">c<\/span>(1, 3, 5, 7), <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(i) {\r\n  out &lt;- <span class=\"functioncall\">boot.ci<\/span>(res, index = <span class=\"functioncall\">c<\/span>(i, i + 1), type = <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"perc\"<\/span>, <span class=\"string\">\"basic\"<\/span>), h = exp)\r\n  <span class=\"functioncall\">with<\/span>(out, <span class=\"functioncall\">c<\/span>(Est = t0, pLL = percent[4], pUL = percent[5],\r\n    basicLL = basic[4], basicLL = basic[5]))\r\n}))\r\n\r\n<span class=\"comment\">## add row names<\/span>\r\n<span class=\"functioncall\">row.names<\/span>(expparms) &lt;- <span class=\"functioncall\">names<\/span>(<span class=\"functioncall\">coef<\/span>(m1))\r\n<span class=\"comment\">## print results<\/span>\r\nexpparms\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                 Est    pLL     pUL basicLL basicLL\r\n## (Intercept) 11.4250 9.9675 13.1682  9.6819 12.8826\r\n## age          0.9857 0.9608  1.0098  0.9615  1.0106\r\n## hmo1         0.8729 0.7876  0.9626  0.7832  0.9582\r\n## died1        0.8156 0.7369  0.8992  0.7321  0.8944\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The results are consistent with what we initially viewed graphically,\n<code>age<\/code> does not have a significant effect, but <code>hmo<\/code> and <code>died<\/code> both do.\nIn order to better understand our results and model, let&#8217;s plot some predicted values.\nBecause all of our predictors were categorical (<code>hmo<\/code> and <code>died<\/code>)\nor had a small number of unique values (<code>age<\/code>) we will get predicted values for\nall possible combinations. First we create a new data set using the <code>expand.grid<\/code>\nfunction, then estimate the predicted values using the <code>predict<\/code> function, and\nfinally plot them.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\">newdata &lt;- <span class=\"functioncall\">expand.grid<\/span>(age = 1:9, hmo = <span class=\"functioncall\">factor<\/span>(0:1), died = <span class=\"functioncall\">factor<\/span>(0:1))\r\nnewdata$yhat &lt;- <span class=\"functioncall\">predict<\/span>(m1, newdata, type = <span class=\"string\">\"response\"<\/span>)\r\n\r\n<span class=\"functioncall\">ggplot<\/span>(newdata, <span class=\"functioncall\">aes<\/span>(x = age, y = yhat, colour = hmo))  +\r\n  <span class=\"functioncall\">geom_point<\/span>() +\r\n  <span class=\"functioncall\">geom_line<\/span>() +\r\n  <span class=\"functioncall\">facet_wrap<\/span>(~ died)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztp-unnamed-chunk-14.png\" alt=\"plot of chunk unnamed-chunk-14\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>If we really wanted to compare the predicted values, we could bootstrap\nconfidence intervals around the predicted estimates. These confidence\nintervals are not for the predicted value themselves, but that that is the\nmean predicted value (i.e., for the estimate, not a new individual).\nIf we wanted to be efficient, we could have done this with our prior\nbootstrap so we only fit the models once. However, it is fast enough\nwe just rerun the bootstrap rather than combine them.<\/p>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## <span class=\"keyword\">function<\/span> to return predicted values<\/span>\r\nfpred &lt;- <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(data, i, newdata) {\r\n  <span class=\"functioncall\">require<\/span>(VGAM)\r\n  m &lt;- <span class=\"functioncall\">vglm<\/span>(formula = stay ~ age + hmo + died, family = <span class=\"functioncall\">pospoisson<\/span>(),\r\n    data = data[i, ], coefstart = <span class=\"functioncall\">c<\/span>(2.436, -0.014, -0.136, -0.204))\r\n  <span class=\"functioncall\">predict<\/span>(m, newdata, type = <span class=\"string\">\"response\"<\/span>)\r\n}\r\n\r\n<span class=\"comment\">## set seed and run bootstrap with 1,200 draws<\/span>\r\n<span class=\"functioncall\">set.seed<\/span>(10)\r\nrespred &lt;- <span class=\"functioncall\">boot<\/span>(dat, fpred, R = 1200, newdata = newdata,\r\n  parallel = <span class=\"string\">\"snow\"<\/span>, ncpus = 4)\r\n\r\n<span class=\"comment\">## get the bootstrapped percentile CIs<\/span>\r\nyhat &lt;- <span class=\"functioncall\">t<\/span>(<span class=\"functioncall\">sapply<\/span>(1:<span class=\"functioncall\">nrow<\/span>(newdata), <span class=\"functioncall\"><span class=\"keyword\">function<\/span><\/span>(i) {\r\n  out &lt;- <span class=\"functioncall\">boot.ci<\/span>(respred, index = i, type = <span class=\"functioncall\">c<\/span>(<span class=\"string\">\"perc\"<\/span>))\r\n  <span class=\"functioncall\">with<\/span>(out, <span class=\"functioncall\">c<\/span>(Est = t0, pLL = percent[4], pUL = percent[5]))\r\n}))\r\n\r\n<span class=\"comment\">## merge CIs with predicted values<\/span>\r\nnewdata &lt;- <span class=\"functioncall\">cbind<\/span>(newdata, yhat)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<div class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"comment\">## graph with CIs<\/span>\r\n<span class=\"functioncall\">ggplot<\/span>(newdata, <span class=\"functioncall\">aes<\/span>(x = age, y = yhat, colour = hmo, fill = hmo))  +\r\n  <span class=\"functioncall\">geom_ribbon<\/span>(<span class=\"functioncall\">aes<\/span>(ymin = pLL, ymax = pUL), alpha = .25) +\r\n  <span class=\"functioncall\">geom_point<\/span>() +\r\n  <span class=\"functioncall\">geom_line<\/span>() +\r\n  <span class=\"functioncall\">facet_wrap<\/span>(~ died)\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ztp-unnamed-chunk-16.png\" alt=\"plot of chunk unnamed-chunk-16\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<h2>Things to consider<\/h2>\n<ul>\n<li>Count data often use exposure variable to indicate the number of times the event\ncould have happened. You can incorporate exposure into your model by using the <code>exposure()<\/code>\noption.<\/li>\n<li>It is not recommended that zero-truncated poisson models be applied to\nsmall samples. What constitutes a small sample does not seem to be clearly defined\nin the literature.<\/li>\n<li>Pseudo-R-squared values differ from OLS R-squareds, please see\n<a href=\"https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faq-what-are-pseudo-r-squareds\/\">FAQ: What are\npseudo R-squareds?<\/a> for a discussion on this issue.<\/li>\n<\/ul>\n<h2>See Also<\/h2>\n<ul>\n<li><a href=\"\/r\/dae\/zero-truncated-negative-binomial\/\">Zero-truncated negative binomial regression in R<\/a>\nuseful when there is overdispersion.<\/li>\n<\/ul>\n<h2>References<\/h2>\n<ul>\n<li>Yee, T. W. (2008). The VGAM package. <i>R News 8(2)<\/i>, 28 39.\nURL http:\/\/CRAN.R-project.org\/doc\/Rnews\/<\/li>\n<li>Yee, T. W., Hastie, T. J. (2003). Reduced-rank vector generalized linear models.\n<i>Statistical Modelling 3(1)<\/i>, 15 41.<\/li>\n<li>Yee, T. W., Wild, C. J. (1996). Vector generalized additive models.\n<i>J. Roy. Statist. Soc. Ser. B 58(3)<\/i>, 481 493.<\/li>\n<li>Long, J. Scott (1997). Regression Models for Categorical and Limited Dependent Variables.\nThousand Oaks, CA: Sage Publications.<\/li>\n<\/ul>\n<p><!--?php include \"stat\/footer.htm\"; ?--><\/p>\n","protected":false},"excerpt":{"rendered":"<p>Zero-truncated poisson regression is used to model count data for which the value zero cannot occur. This page uses the following packages. Make sure that you can load them before&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/zero-truncated-poisson\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":21,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-939","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/939","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=939"}],"version-history":[{"count":3,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/939\/revisions"}],"predecessor-version":[{"id":23676,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/939\/revisions\/23676"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=939"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}