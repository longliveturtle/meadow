{"id":922,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-logit-regression\/"},"modified":"2024-06-24T11:05:51","modified_gmt":"2024-06-24T18:05:51","slug":"logit-regression","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/logit-regression\/","title":{"rendered":"Logit Regression |  R Data Analysis Examples"},"content":{"rendered":"<p>&nbsp;<\/p>\n<style type=\"text\/css\">\n.knitr.inline {<br \/>  background-color: #f7f7f7;<br \/>  border:solid 1px #B0B0B0;<br \/>}<br \/>.error {<br \/>\tfont-weight: bold;<br \/>\tcolor: #FF0000;<br \/>},<br \/>.warning {<br \/>\tfont-weight: bold;<br \/>}<br \/>.message {<br \/>\tfont-style: italic;<br \/>}<br \/>.source, .output, .warning, .error, .message {<br \/>\tpadding: 0em 1em;<br \/>  border:solid 1px #F7F7F7;<br \/>}<br \/>.source {<br \/>  background-color: #f5f5f5;<br \/>}<br \/>.rimage.left {<br \/>  text-align: left;<br \/>}<br \/>.rimage.right {<br \/>  text-align: right;<br \/>}<br \/>.rimage.center {<br \/>  text-align: center;<br \/>}<br \/>.hl.num {<br \/>  color: #AF0F91;<br \/>}<br \/>.hl.str {<br \/>  color: #317ECC;<br \/>}<br \/>.hl.com {<br \/>  color: #AD95AF;<br \/>  font-style: italic;<br \/>}<br \/>.hl.opt {<br \/>  color: #000000;<br \/>}<br \/>.hl.std {<br \/>  color: #585858;<br \/>}<br \/>.hl.kwa {<br \/>  color: #295F94;<br \/>  font-weight: bold;<br \/>}<br \/>.hl.kwb {<br \/>  color: #B05A65;<br \/>}<br \/>.hl.kwc {<br \/>  color: #55aa55;<br \/>}<br \/>.hl.kwd {<br \/>  color: #BC5A65;<br \/>  font-weight: bold;<br \/>}<br \/><\/style>\n<p><!--?php include \"stat\/header.htm\"; ?--><\/p>\n<p>Logistic regression, also called a logit model, is used to model dichotomous\noutcome variables. In the logit model the log odds of the outcome is modeled as a linear\ncombination of the predictor variables.<\/p>\n<p>This page uses the following packages. Make sure that you can load\nthem before trying to run the examples on this page. If you do not have\na package installed, run: <code>install.packages(\"packagename\")<\/code>, or\nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div id=\"unnamed-chunk-2\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">library<\/span><span class=\"hl std\">(aod)<\/span>\r\n<span class=\"hl kwd\">library<\/span><span class=\"hl std\">(ggplot2)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><strong>Version info: <\/strong>Code for this page was tested in R version 3.0.2 (2013-09-25)\nOn: 2013-12-16\nWith: knitr 1.5; ggplot2 0.9.3.1; aod 1.3\n<\/code><\/p>\n<p><strong>Please note:<\/strong> The purpose of this page is to show how to use various data analysis commands.\nIt does not cover all aspects of the research process which researchers are expected to do. In\nparticular, it does not cover data cleaning and checking, verification of assumptions, model\ndiagnostics and potential follow-up analyses.<\/p>\n<h2>Examples<\/h2>\n<p><strong>Example 1<\/strong>. Suppose that we are interested in the factors\nthat influence whether a political candidate wins an election. The\noutcome (response) variable is binary (0\/1); win or lose.\nThe predictor variables of interest are the amount of money spent on the campaign, the\namount of time spent campaigning negatively and whether or not the candidate is an\nincumbent.<\/p>\n<p><strong>Example 2<\/strong>. A researcher is interested in how variables, such as GRE (Graduate Record Exam scores),\nGPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate\nschool. The response variable, admit\/don&#8217;t admit, is a binary variable.<\/p>\n<h2>Description of the data<\/h2>\n<p>For our data analysis below, we are going to expand on Example 2 about getting\ninto graduate school. We have generated hypothetical data, which\ncan be obtained from our website from within R. Note that <em>R requires forward slashes<\/em>\n(\/) not back slashes () when specifying a file location even if the file is\non your hard drive.<\/p>\n<div id=\"unnamed-chunk-3\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">mydata<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">read.csv<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/binary.csv\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl com\">## view the first few rows of the data<\/span>\r\n<span class=\"hl kwd\">head<\/span><span class=\"hl std\">(mydata)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   admit gre  gpa rank\r\n## 1     0 380 3.61    3\r\n## 2     1 660 3.67    3\r\n## 3     1 800 4.00    1\r\n## 4     1 640 3.19    4\r\n## 5     0 520 2.93    4\r\n## 6     1 760 3.00    2\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>This dataset has a binary response (outcome, dependent) variable called <code>admit<\/code>.\nThere are three predictor variables: <code>gre<\/code>, <code>gpa<\/code> and <code>rank<\/code>. We will treat the\nvariables <code>gre<\/code> and <code>gpa<\/code> as continuous. The variable <code>rank<\/code> takes on the\nvalues 1 through 4. Institutions with a rank of 1 have the highest prestige,\nwhile those with a rank of 4 have the lowest. We can get basic descriptives for the entire\ndata set by using <code>summary<\/code>. To get the standard deviations, we use <code>sapply<\/code> to apply\nthe <code>sd<\/code> function to each variable in the dataset.<\/p>\n<div id=\"unnamed-chunk-4\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(mydata)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##      admit            gre           gpa            rank     \r\n##  Min.   :0.000   Min.   :220   Min.   :2.26   Min.   :1.00  \r\n##  1st Qu.:0.000   1st Qu.:520   1st Qu.:3.13   1st Qu.:2.00  \r\n##  Median :0.000   Median :580   Median :3.40   Median :2.00  \r\n##  Mean   :0.318   Mean   :588   Mean   :3.39   Mean   :2.48  \r\n##  3rd Qu.:1.000   3rd Qu.:660   3rd Qu.:3.67   3rd Qu.:3.00  \r\n##  Max.   :1.000   Max.   :800   Max.   :4.00   Max.   :4.00\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">sapply<\/span><span class=\"hl std\">(mydata, sd)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   admit     gre     gpa    rank \r\n##   0.466 115.517   0.381   0.944\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## two-way contingency table of categorical outcome and predictors we want<\/span>\r\n<span class=\"hl com\">## to make sure there are not 0 cells<\/span>\r\n<span class=\"hl kwd\">xtabs<\/span><span class=\"hl std\">(<\/span><span class=\"hl opt\">~<\/span><span class=\"hl std\">admit<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">rank,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= mydata)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##      rank\r\n## admit  1  2  3  4\r\n##     0 28 97 93 55\r\n##     1 33 54 28 12\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<h2>Analysis methods you might consider<\/h2>\n<p>Below is a list of some analysis methods you may have encountered.\nSome of the methods listed are quite reasonable while others have either\nfallen out of favor or have limitations.<\/p>\n<ul>\n<li>Logistic regression, the focus of this page.<\/li>\n<li>Probit regression. Probit analysis will produce results similar\nlogistic regression. The choice of probit versus logit depends largely on\nindividual preferences.<\/li>\n<li>OLS regression. When used with a binary response variable, this model is known\nas a linear probability model and can be used as a way to\ndescribe conditional probabilities. However, the errors (i.e., residuals)\nfrom the linear probability model violate the homoskedasticity and\nnormality of errors assumptions of OLS\nregression, resulting in invalid standard errors and hypothesis tests. For\na more thorough discussion of these and other problems with the linear\nprobability model, see Long (1997, p. 38-40).<\/li>\n<li>Two-group discriminant function analysis. A multivariate method for\ndichotomous outcome variables.<\/li>\n<li>Hotelling&#8217;s T<sup>2<\/sup>. The 0\/1 outcome is turned into the\ngrouping variable, and the former predictors are turned into outcome\nvariables. This will produce an overall test of significance but will not\ngive individual coefficients for each variable, and it is unclear the extent\nto which each &#8220;predictor&#8221; is adjusted for the impact of the other\n&#8220;predictors.&#8221;<\/li>\n<\/ul>\n<h2>Using the logit model<\/h2>\n<p>The code below estimates a logistic regression model using the <code>glm<\/code> (generalized linear model)\nfunction. First, we convert <code>rank<\/code> to a factor to indicate that rank should be\ntreated as a categorical variable.<\/p>\n<div id=\"unnamed-chunk-5\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">mydata<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">rank<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">factor<\/span><span class=\"hl std\">(mydata<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">rank)<\/span>\r\n<span class=\"hl std\">mylogit<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">glm<\/span><span class=\"hl std\">(admit<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">gre<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">gpa<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">rank,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= mydata,<\/span> <span class=\"hl kwc\">family<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"binomial\"<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Since we gave our model a name (<code>mylogit<\/code>), R will not produce any\noutput from our regression. In order to get the results we use the summary\ncommand:<\/p>\n<div id=\"unnamed-chunk-6\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(mylogit)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## glm(formula = admit ~ gre + gpa + rank, family = \"binomial\", \r\n##     data = mydata)\r\n## \r\n## Deviance Residuals: \r\n##    Min      1Q  Median      3Q     Max  \r\n## -1.627  -0.866  -0.639   1.149   2.079  \r\n## \r\n## Coefficients:\r\n##             Estimate Std. Error z value Pr(&gt;|z|)    \r\n## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***\r\n## gre          0.00226    0.00109    2.07  0.03847 *  \r\n## gpa          0.80404    0.33182    2.42  0.01539 *  \r\n## rank2       -0.67544    0.31649   -2.13  0.03283 *  \r\n## rank3       -1.34020    0.34531   -3.88  0.00010 ***\r\n## rank4       -1.55146    0.41783   -3.71  0.00020 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n## \r\n## (Dispersion parameter for binomial family taken to be 1)\r\n## \r\n##     Null deviance: 499.98  on 399  degrees of freedom\r\n## Residual deviance: 458.52  on 394  degrees of freedom\r\n## AIC: 470.5\r\n## \r\n## Number of Fisher Scoring iterations: 4\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<ul>\n<li>In the output above, the first thing we see is the call,\nthis is R reminding us what the model we ran was, what options we specified, etc.<\/li>\n<li>Next we see the deviance residuals, which are a measure of model fit. This part\nof output shows the distribution of the deviance residuals for individual cases used\nin the model. Below we discuss how to use summaries of the deviance statistic to assess model fit.<\/li>\n<li>The next part of the output shows the coefficients, their standard errors, the z-statistic (sometimes\ncalled a Wald z-statistic), and the associated p-values. Both <code>gre<\/code> and <code>gpa<\/code> are statistically\nsignificant, as are the three terms for <code>rank<\/code>. The logistic regression coefficients\ngive the change in the log odds of the outcome for a one unit increase in the predictor variable.<\/p>\n<ul>\n<li>For every one unit change in <code>gre<\/code>, the log odds of admission (versus non-admission)\nincreases by <code class=\"knitr inline\">0.002<\/code>.<\/li>\n<li>For a one unit increase in<code> gpa<\/code>, the log odds of being admitted to graduate\nschool increases by <code class=\"knitr inline\">0.804<\/code>.<\/li>\n<li>The indicator variables for <code>rank<\/code> have a slightly different\ninterpretation. For example, having attended an undergraduate institution with <code>rank<\/code> of 2,\nversus an institution with a <code>rank<\/code> of 1, changes the log odds of admission by\n<code class=\"knitr inline\">-0.675<\/code>.<\/li>\n<\/ul>\n<\/li>\n<li>Below the table of coefficients are fit indices, including the null and deviance residuals and the AIC.\nLater we show an example of how you can use these values to help assess model fit.<\/li>\n<\/ul>\n<p>We can use the <code>confint<\/code> function to obtain confidence\nintervals for the coefficient estimates. Note that for logistic models,\nconfidence intervals are based on the profiled log-likelihood function.\nWe can also get CIs based on just the standard errors by using the default method.<\/p>\n<div id=\"unnamed-chunk-7\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## CIs using profiled log-likelihood<\/span>\r\n<span class=\"hl kwd\">confint<\/span><span class=\"hl std\">(mylogit)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## Waiting for profiling to be done...\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                 2.5 %   97.5 %\r\n## (Intercept) -6.271620 -1.79255\r\n## gre          0.000138  0.00444\r\n## gpa          0.160296  1.46414\r\n## rank2       -1.300889 -0.05675\r\n## rank3       -2.027671 -0.67037\r\n## rank4       -2.400027 -0.75354\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## CIs using standard errors<\/span>\r\n<span class=\"hl kwd\">confint.default<\/span><span class=\"hl std\">(mylogit)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                2.5 %   97.5 %\r\n## (Intercept) -6.22424 -1.75572\r\n## gre          0.00012  0.00441\r\n## gpa          0.15368  1.45439\r\n## rank2       -1.29575 -0.05513\r\n## rank3       -2.01699 -0.66342\r\n## rank4       -2.37040 -0.73253\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can test for an overall effect of <code>rank<\/code> using the <code>wald.test<\/code>\nfunction of the <code>aod<\/code> library. The\norder in which the coefficients are given in the table of coefficients is the\nsame as the order of the terms in the model. This is important because the <code>\nwald.test<\/code> function refers to the coefficients by their order in the model.\nWe use the <code>wald.test<\/code> function. <code>b<\/code>\nsupplies the coefficients, while <code>Sigma<\/code> supplies the variance covariance\nmatrix of the error terms, finally <code>Terms<\/code> tells R which terms in the model\nare to be tested, in this case, terms 4, 5, and 6, are the three terms for the\nlevels of <code>rank<\/code>.<\/p>\n<div id=\"unnamed-chunk-8\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">wald.test<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">b<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(mylogit),<\/span> <span class=\"hl kwc\">Sigma<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">vcov<\/span><span class=\"hl std\">(mylogit),<\/span> <span class=\"hl kwc\">Terms<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">4<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">6<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## Wald test:\r\n## ----------\r\n## \r\n## Chi-squared test:\r\n## X2 = 20.9, df = 3, P(&gt; X2) = 0.00011\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The chi-squared test statistic of 20.9, with three degrees of freedom is\nassociated with a p-value of 0.00011 indicating that the overall effect of <code>\nrank<\/code> is statistically significant.<\/p>\n<p>We can also test additional hypotheses about the differences in the\ncoefficients for the different levels of rank. Below we\ntest that the coefficient for <code>rank<\/code>=2 is equal to the coefficient for <code>rank<\/code>=3.\nThe first line of code below creates a vector <code>l<\/code> that defines the test we\nwant to perform. In this case, we want to test the difference (subtraction) of\nthe terms for <code>rank<\/code>=2 and <code>rank<\/code>=3 (i.e., the 4th and 5th terms in the\nmodel). To contrast these two terms, we multiply one of them by 1, and the other\nby -1. The other terms in the model are not involved in the test, so they are\nmultiplied by 0. The second line of code below uses <code>L=l<\/code> to tell R that we\nwish to base the test on the vector <code>l<\/code> (rather than using the Terms option\nas we did above).<\/p>\n<div id=\"unnamed-chunk-9\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">l<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">0<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">0<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">0<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">,<\/span> <span class=\"hl opt\">-<\/span><span class=\"hl num\">1<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">0<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl kwd\">wald.test<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">b<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(mylogit),<\/span> <span class=\"hl kwc\">Sigma<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">vcov<\/span><span class=\"hl std\">(mylogit),<\/span> <span class=\"hl kwc\">L<\/span> <span class=\"hl std\">= l)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## Wald test:\r\n## ----------\r\n## \r\n## Chi-squared test:\r\n## X2 = 5.5, df = 1, P(&gt; X2) = 0.019\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The chi-squared test statistic of 5.5 with 1 degree of freedom is associated with\na p-value of 0.019, indicating that the difference between the coefficient for <code>rank<\/code>=2\nand the coefficient for <code>rank<\/code>=3 is statistically significant.<\/p>\n<p>You can also exponentiate the coefficients and interpret them as\nodds-ratios. R will do this computation for you.\nTo get the exponentiated coefficients, you tell R that you want\nto exponentiate (<code>exp<\/code>), and that the object you want to exponentiate is\ncalled coefficients and it is part of mylogit (<code>coef(mylogit)<\/code>). We can use\nthe same logic to get odds ratios and their confidence intervals, by exponentiating\nthe confidence intervals from before. To put it all in one table, we use <code>cbind<\/code> to\nbind the coefficients and confidence intervals column-wise.<\/p>\n<div id=\"unnamed-chunk-10\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## odds ratios only<\/span>\r\n<span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(mylogit))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## (Intercept)         gre         gpa       rank2       rank3       rank4 \r\n##      0.0185      1.0023      2.2345      0.5089      0.2618      0.2119\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## odds ratios and 95% CI<\/span>\r\n<span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">OR<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(mylogit),<\/span> <span class=\"hl kwd\">confint<\/span><span class=\"hl std\">(mylogit)))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## Waiting for profiling to be done...\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                 OR   2.5 % 97.5 %\r\n## (Intercept) 0.0185 0.00189  0.167\r\n## gre         1.0023 1.00014  1.004\r\n## gpa         2.2345 1.17386  4.324\r\n## rank2       0.5089 0.27229  0.945\r\n## rank3       0.2618 0.13164  0.512\r\n## rank4       0.2119 0.09072  0.471\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Now we can say that for a one unit increase in <code>gpa<\/code>, the odds of being\nadmitted to graduate school (versus not being admitted) increase by a factor of\n2.23. For more information on interpreting odds ratios see our FAQ page\n<a href=\"https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faq-how-do-i-interpret-odds-ratios-in-logistic-regression\/\">How do I interpret odds ratios in logistic regression?<\/a>\n. Note that while R produces it, the odds ratio for the intercept is not generally interpreted.<\/p>\n<p>You can also use predicted probabilities to help you understand the model.\nPredicted probabilities can be computed for both categorical and continuous\npredictor variables. In order to create\npredicted probabilities we first need to create a new data frame with the values\nwe want the independent variables to take on to create our predictions.<\/p>\n<p>We will start by calculating the predicted probability of admission at each\nvalue of rank, holding <code>gre<\/code> and <code>gpa<\/code> at their means. First we create\nand view the data frame.<\/p>\n<div id=\"unnamed-chunk-11\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">newdata1<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">with<\/span><span class=\"hl std\">(mydata,<\/span> <span class=\"hl kwd\">data.frame<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">gre<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(gre),<\/span> <span class=\"hl kwc\">gpa<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(gpa),<\/span> <span class=\"hl kwc\">rank<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">factor<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">4<\/span><span class=\"hl std\">)))<\/span>\r\n\r\n<span class=\"hl com\">## view data frame<\/span>\r\n<span class=\"hl std\">newdata1<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   gre  gpa rank\r\n## 1 588 3.39    1\r\n## 2 588 3.39    2\r\n## 3 588 3.39    3\r\n## 4 588 3.39    4\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><em>These objects must have the same names as the variables in your logistic\nregression above (e.g. in this example the mean for <code>gre<\/code> must be named\ngre).<\/em> Now that we have the data frame we want to use to calculate the predicted\nprobabilities, we can tell R to create the predicted probabilities. The first\nline of code below is quite compact, we will break it apart to discuss what\nvarious components do. The <code>newdata1$rankP <\/code>tells R that we\nwant to create a new variable in the dataset (data frame) <code>newdata1<\/code> called\n<code>rankP<\/code>, the rest of the command tells R that the values of <code>rankP<\/code>\nshould be predictions made using the <code>predict( )<\/code> function. The options\nwithin the parentheses tell R that the predictions should be based on the analysis <code>mylogit<\/code>\nwith values of the predictor variables coming from <code>newdata1<\/code> and that the type of prediction\nis a predicted probability (<code>type=\"response\"<\/code>). The second line of the code\nlists the values in the data frame <code>newdata1<\/code>. Although not\nparticularly pretty, this is a table of predicted probabilities.<\/p>\n<div id=\"unnamed-chunk-12\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">newdata1<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">rankP<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">predict<\/span><span class=\"hl std\">(mylogit,<\/span> <span class=\"hl kwc\">newdata<\/span> <span class=\"hl std\">= newdata1,<\/span> <span class=\"hl kwc\">type<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"response\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl std\">newdata1<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   gre  gpa rank rankP\r\n## 1 588 3.39    1 0.517\r\n## 2 588 3.39    2 0.352\r\n## 3 588 3.39    3 0.219\r\n## 4 588 3.39    4 0.185\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>In the above output we see that the predicted probability of being accepted\ninto a graduate program is 0.52 for students from the highest prestige undergraduate institutions\n(<code>rank<\/code>=1), and 0.18 for students from the lowest ranked institutions (<code>rank<\/code>=4), holding\n<code>gre<\/code> and <code>gpa<\/code> at their means.\nWe can do something very similar to create a table of predicted probabilities\nvarying the value of <code>gre<\/code> and <code>rank<\/code>. We are going to plot these, so we will create\n100 values of <code>gre<\/code> between 200 and 800, at each value of rank (i.e., 1, 2, 3, and 4).<\/p>\n<div id=\"unnamed-chunk-13\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">newdata2<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">with<\/span><span class=\"hl std\">(mydata,<\/span> <span class=\"hl kwd\">data.frame<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">gre<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">rep<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">seq<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">from<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">200<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">to<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">800<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">length.out<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">100<\/span><span class=\"hl std\">),<\/span>\r\n    <span class=\"hl num\">4<\/span><span class=\"hl std\">),<\/span> <span class=\"hl kwc\">gpa<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(gpa),<\/span> <span class=\"hl kwc\">rank<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">factor<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">rep<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">4<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">each<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">100<\/span><span class=\"hl std\">))))<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The code to generate the predicted probabilities (the first line below)\nis the same as before, except we are also going to ask for standard errors\nso we can plot a confidence interval. We get the estimates on the\nlink scale and back transform both the predicted values and confidence\nlimits into probabilities.<\/p>\n<div id=\"unnamed-chunk-14\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">newdata3<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(newdata2,<\/span> <span class=\"hl kwd\">predict<\/span><span class=\"hl std\">(mylogit,<\/span> <span class=\"hl kwc\">newdata<\/span> <span class=\"hl std\">= newdata2,<\/span> <span class=\"hl kwc\">type<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"link\"<\/span><span class=\"hl std\">,<\/span>\r\n    <span class=\"hl kwc\">se<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">TRUE<\/span><span class=\"hl std\">))<\/span>\r\n<span class=\"hl std\">newdata3<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">within<\/span><span class=\"hl std\">(newdata3, {<\/span>\r\n    <span class=\"hl std\">PredictedProb<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">plogis<\/span><span class=\"hl std\">(fit)<\/span>\r\n    <span class=\"hl std\">LL<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">plogis<\/span><span class=\"hl std\">(fit<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">(<\/span><span class=\"hl num\">1.96<\/span> <span class=\"hl opt\">*<\/span> <span class=\"hl std\">se.fit))<\/span>\r\n    <span class=\"hl std\">UL<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">plogis<\/span><span class=\"hl std\">(fit<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">(<\/span><span class=\"hl num\">1.96<\/span> <span class=\"hl opt\">*<\/span> <span class=\"hl std\">se.fit))<\/span>\r\n<span class=\"hl std\">})<\/span>\r\n\r\n<span class=\"hl com\">## view first few rows of final dataset<\/span>\r\n<span class=\"hl kwd\">head<\/span><span class=\"hl std\">(newdata3)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   gre  gpa rank    fit se.fit residual.scale    UL    LL PredictedProb\r\n## 1 200 3.39    1 -0.811  0.515              1 0.549 0.139         0.308\r\n## 2 206 3.39    1 -0.798  0.509              1 0.550 0.142         0.311\r\n## 3 212 3.39    1 -0.784  0.503              1 0.551 0.145         0.313\r\n## 4 218 3.39    1 -0.770  0.498              1 0.551 0.149         0.316\r\n## 5 224 3.39    1 -0.757  0.492              1 0.552 0.152         0.319\r\n## 6 230 3.39    1 -0.743  0.487              1 0.553 0.155         0.322\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>It can also be helpful to use graphs of predicted probabilities\nto understand and\/or present the model. We will use the <code>ggplot2<\/code>\npackage for graphing. Below we make a plot with the predicted probabilities,\nand 95% confidence intervals.<\/p>\n<div id=\"unnamed-chunk-15\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(newdata3,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span> <span class=\"hl std\">= gre,<\/span> <span class=\"hl kwc\">y<\/span> <span class=\"hl std\">= PredictedProb))<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl kwd\">geom_ribbon<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">ymin<\/span> <span class=\"hl std\">= LL,<\/span>\r\n    <span class=\"hl kwc\">ymax<\/span> <span class=\"hl std\">= UL,<\/span> <span class=\"hl kwc\">fill<\/span> <span class=\"hl std\">= rank),<\/span> <span class=\"hl kwc\">alpha<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">0.2<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl kwd\">geom_line<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">colour<\/span> <span class=\"hl std\">= rank),<\/span>\r\n    <span class=\"hl kwc\">size<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/logit-unnamed-chunk-15.png\" alt=\"Predicted probabilities plot\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>We may also wish to see measures of how well our model fits. This can be\nparticularly useful when comparing competing models. The output produced by <code>\nsummary(mylogit)<\/code> included indices of fit (shown below the coefficients), including the null and\ndeviance residuals and the AIC. One measure of model fit is the significance of\nthe overall model. This test asks whether the model with predictors fits\nsignificantly better than a model with just an intercept (i.e., a null model).\nThe test statistic is the difference between the residual deviance for the model\nwith predictors and the null model. The test statistic is distributed\nchi-squared with degrees of freedom equal to the differences in degrees of freedom between\nthe current and the null model (i.e., the number of predictor variables in the\nmodel). To find the difference in deviance for the two models (i.e., the test\nstatistic) we can use the command:<\/p>\n<div id=\"unnamed-chunk-16\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">with<\/span><span class=\"hl std\">(mylogit, null.deviance<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">deviance)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 41.5\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The degrees of freedom for the difference between the two models is equal to the number of\npredictor variables in the mode, and can be obtained using:<\/p>\n<div id=\"unnamed-chunk-17\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">with<\/span><span class=\"hl std\">(mylogit, df.null<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">df.residual)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 5\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Finally, the p-value can be obtained using:<\/p>\n<div id=\"unnamed-chunk-18\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">with<\/span><span class=\"hl std\">(mylogit,<\/span> <span class=\"hl kwd\">pchisq<\/span><span class=\"hl std\">(null.deviance<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">deviance, df.null<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">df.residual,<\/span> <span class=\"hl kwc\">lower.tail<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">FALSE<\/span><span class=\"hl std\">))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 7.58e-08\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The chi-square of 41.46 with 5 degrees of freedom and an associated p-value of\nless than 0.001 tells us that our model as a whole fits\nsignificantly better than an empty model. This is sometimes called a likelihood\nratio test (the deviance residual is -2*log likelihood).\nTo see the model&#8217;s log likelihood, we type:<\/p>\n<div id=\"unnamed-chunk-19\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">logLik<\/span><span class=\"hl std\">(mylogit)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## 'log Lik.' -229 (df=6)\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<h2>Things to consider<\/h2>\n<ul>\n<li>Empty cells or small cells: You should check for empty or small\ncells by doing a crosstab between categorical predictors and the outcome\nvariable. If a cell has very few cases (a small cell), the model may\nbecome unstable or it might not run at all.<\/li>\n<li>Separation or quasi-separation (also called perfect prediction), a\ncondition in which the outcome does not vary at some levels of the\nindependent variables. See our page <a href=\"https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faqwhat-is-complete-or-quasi-complete-separation-in-logisticprobit-regression-and-how-do-we-deal-with-them\/\">\nFAQ: What is complete or quasi-complete separation in logistic\/probit\nregression and how do we deal with them?<\/a> for information on models with\nperfect prediction.<\/li>\n<li>Sample size: Both logit and probit models require more cases than\nOLS regression because they use maximum likelihood estimation techniques. It\nis sometimes possible to estimate models for binary outcomes in datasets\nwith only a small number of cases using exact logistic regression. It is also important to keep in mind that\nwhen the outcome is rare, even if the overall dataset is large, it can be\ndifficult to estimate a logit model.<\/li>\n<li>Pseudo-R-squared: Many different measures of psuedo-R-squared\nexist. They all attempt to provide information similar to that provided by\nR-squared in OLS regression; however, none of them can be interpreted\nexactly as R-squared in OLS regression is interpreted. For a discussion of\nvarious pseudo-R-squareds see Long and Freese (2006) or our FAQ page\n<a href=\"https:\/\/stats.idre.ucla.edu\/other\/mult-pkg\/faq\/general\/faq-what-are-pseudo-r-squareds\/\">What are\npseudo R-squareds?<\/a><\/li>\n<li>Diagnostics: The diagnostics for logistic regression are different\nfrom those for OLS regression. For a discussion of model diagnostics for\nlogistic regression, see Hosmer and Lemeshow (2000, Chapter 5). Note that\ndiagnostics done for logistic regression are similar to those done for probit regression.<\/li>\n<\/ul>\n<h2>References<\/h2>\n<p>Hosmer, D. &amp; Lemeshow, S. (2000). Applied Logistic Regression (Second Edition).\nNew York: John Wiley &amp; Sons, Inc.<\/p>\n<p>Long, J. Scott &amp; Freese, Jeremy. (2006). Regression Models for Categorical Dependent Variables Using Stata, Second Edition. College Station, TX:\u00a0 Stata Press.<\/p>\n<p>Long, J. Scott (1997). Regression Models for Categorical and Limited Dependent Variables.\nThousand Oaks, CA: Sage Publications.<\/p>\n<h2>See also<\/h2>\n<ul>\n<li>R Online Manual <a href=\"http:\/\/stat.ethz.ch\/R-manual\/R-patched\/library\/stats\/html\/glm.html\"><code>glm<\/code><\/a><\/li>\n<li><a href=\"\/books\/#Logistic Regression and Related Methods\">\nStat Books for Loan, Logistic Regression and Limited Dependent Variables<\/a><\/li>\n<li>Everitt, B. S. and Hothorn, T.<a href=\"http:\/\/cran.r-project.org\/web\/packages\/HSAUR\/\">\nA Handbook of Statistical Analyses Using R<\/a><\/li>\n<\/ul>\n<p><!--?php include \"stat\/footer.htm\"; ?--><\/p>\n","protected":false},"excerpt":{"rendered":"<p>&nbsp; Logistic regression, also called a logit model, is used to model dichotomous outcome variables. In the logit model the log odds of the outcome is modeled as a linear&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/logit-regression\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":4,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-922","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/922","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=922"}],"version-history":[{"count":6,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/922\/revisions"}],"predecessor-version":[{"id":37445,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/922\/revisions\/37445"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=922"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}