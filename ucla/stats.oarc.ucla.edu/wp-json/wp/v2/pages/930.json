{"id":930,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-robust-regression\/"},"modified":"2018-09-20T07:42:55","modified_gmt":"2018-09-20T14:42:55","slug":"robust-regression","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/robust-regression\/","title":{"rendered":"Robust Regression |  R Data Analysis Examples"},"content":{"rendered":"<p><!--[mathjax]--><\/p>\n<p>Robust regression is an alternative to least squares regression\nwhen data are contaminated with outliers or influential observations, and it can also be used\nfor the purpose of detecting influential observations.<\/p>\n<p>This page uses the following packages. Make sure that you can load\nthem before trying to run the examples on this page. If you do not have\na package installed, run: <code>install.packages(\"packagename\")<\/code>, or\nif you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div id=\"unnamed-chunk-2\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">require<\/span><span class=\"hl std\">(foreign)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(MASS)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><strong>Version info: <\/strong>Code for this page was tested in R version 3.1.1 (2014-07-10)\nOn: 2014-09-29\nWith: MASS 7.3-33; foreign 0.8-61; knitr 1.6; boot 1.3-11; ggplot2 1.0.0; dplyr 0.2; nlme 3.1-117\n<\/code><\/p>\n<p><strong>Please note:<\/strong> The purpose of this page is to show how to use various\ndata analysis commands. It does not cover all aspects of the research process\nwhich researchers are expected to do. In particular, it does not cover data\ncleaning and checking, verification of assumptions, model diagnostics or\npotential follow-up analyses.<\/p>\n<h2>Introduction<\/h2>\n<p>Let&#8217;s begin our discussion on robust regression with some terms in linear\nregression.<\/p>\n<p><strong>Residual<\/strong>: The difference between the predicted value (based on the\nregression equation) and the actual, observed value.<\/p>\n<p><strong>Outlier<\/strong>: In linear regression, an outlier is an observation with\nlarge residual. In other words, it is an observation whose dependent-variable\nvalue is unusual given its value on the predictor variables. An outlier may\nindicate a sample peculiarity or may indicate a data entry error or other\nproblem.<\/p>\n<p><strong>Leverage<\/strong>: An observation with an extreme value on a predictor\nvariable is a point with high leverage. Leverage is a measure of how far an\nindependent variable deviates from its mean. High leverage points can have a\ngreat amount of effect on the estimate of regression coefficients.<\/p>\n<p><strong>Influence<\/strong>: An observation is said to be influential if removing the\nobservation substantially changes the estimate of the regression coefficients.\nInfluence can be thought of as the product of leverage and outlierness.<\/p>\n<p><strong>Cook&#8217;s distance<\/strong> (or Cook&#8217;s D): A measure that combines the information\nof leverage and residual of the observation.<\/p>\n<p>Robust regression can be used in any situation in which you would use least\nsquares regression. When fitting a least squares regression, we might find some\noutliers or high leverage data points. We have decided that these data points\nare not data entry errors, neither they are from a different population than\nmost of our data. So we have no compelling reason to exclude them from the\nanalysis. Robust regression might be a good strategy since it is a compromise\nbetween excluding these points entirely from the analysis and including all the\ndata points and treating all them equally in OLS regression. The idea of robust\nregression is to weigh the observations differently based on how well behaved\nthese observations are. Roughly speaking, it is a form of weighted and\nreweighted least squares regression.<\/p>\n<p>The <code>rlm<\/code> command in the <code>MASS<\/code> package command implements several versions of robust\nregression. In this page, we will show M-estimation with Huber and bisquare\nweighting. These two are very standard. M-estimation defines a weight function\nsuch that the estimating equation becomes \\(\\sum_{i=1}^{n}w_{i}(y_{i} &#8211; x&#8217;b)x&#8217;_{i} = 0\\).\nBut the weights depend on the residuals and the residuals on the weights. The equation is solved using <strong>I<\/strong>teratively\n<strong>R<\/strong>eweighted <strong>L<\/strong>east <strong>S<\/strong>quares (IRLS).\nFor example, the coefficient matrix at iteration <i>j<\/i> is\n\\(B_{j} = [X&#8217;W_{j-1}X]^{-1}X&#8217;W_{j-1}Y\\)\nwhere the subscripts indicate the matrix at a particular iteration (<i>not<\/i> rows or columns).\nThe process continues until it converges. In Huber weighting,\nobservations with small residuals get a weight of 1 and the larger the residual,\nthe smaller the weight. This is defined by the weight function<\/p>\n<p>\\begin{equation}\nw(e) =\n\\left\\{\n\\begin{array}{rl}\n1 \\quad \\mbox{for} \\quad |e| \\leq k \\\\ \\dfrac{k}{|e|} \\quad \\mbox{for} \\quad |e| &gt; k \\\\\n\\end{array}\n\\right.\n\\end{equation}<\/p>\n<p>With bisquare weighting, all cases with a non-zero\nresidual get down-weighted at least a little.<\/p>\n<h2>Description of the example data<\/h2>\n<p>For our data analysis below, we will use the crime dataset that appears in\n<i>Statistical Methods for Social Sciences, Third Edition<\/i>\nby Alan Agresti and Barbara Finlay (Prentice Hall, 1997). The variables are\nstate id (<code>sid<\/code>), state name (<code>state<\/code>), violent crimes per 100,000\npeople (<code>crime<\/code>), murders per 1,000,000 (<code>murder<\/code>), the percent of\nthe population living in metropolitan areas (<code>pctmetro<\/code>), the percent of\nthe population that is white (<code>pctwhite<\/code>), percent of population with a\nhigh school education or above (<code>pcths<\/code>), percent of population living\nunder poverty line (<code>poverty<\/code>), and percent of population that are single\nparents (<code>single<\/code>). It has 51 observations. We are going to use <code>poverty<\/code>\nand <code>single<\/code> to predict <code>crime<\/code>.<\/p>\n<div id=\"unnamed-chunk-3\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">cdata<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">read.dta<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/crime.dta\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(cdata)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##       sid          state               crime          murder     \r\n##  Min.   : 1.0   Length:51          Min.   :  82   Min.   : 1.60  \r\n##  1st Qu.:13.5   Class :character   1st Qu.: 326   1st Qu.: 3.90  \r\n##  Median :26.0   Mode  :character   Median : 515   Median : 6.80  \r\n##  Mean   :26.0                      Mean   : 613   Mean   : 8.73  \r\n##  3rd Qu.:38.5                      3rd Qu.: 773   3rd Qu.:10.35  \r\n##  Max.   :51.0                      Max.   :2922   Max.   :78.50  \r\n##     pctmetro        pctwhite        pcths         poverty    \r\n##  Min.   : 24.0   Min.   :31.8   Min.   :64.3   Min.   : 8.0  \r\n##  1st Qu.: 49.5   1st Qu.:79.3   1st Qu.:73.5   1st Qu.:10.7  \r\n##  Median : 69.8   Median :87.6   Median :76.7   Median :13.1  \r\n##  Mean   : 67.4   Mean   :84.1   Mean   :76.2   Mean   :14.3  \r\n##  3rd Qu.: 84.0   3rd Qu.:92.6   3rd Qu.:80.1   3rd Qu.:17.4  \r\n##  Max.   :100.0   Max.   :98.5   Max.   :86.6   Max.   :26.4  \r\n##      single    \r\n##  Min.   : 8.4  \r\n##  1st Qu.:10.1  \r\n##  Median :10.9  \r\n##  Mean   :11.3  \r\n##  3rd Qu.:12.1  \r\n##  Max.   :22.1\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<h3>Using robust regression analysis<\/h3>\n<p>In most cases, we begin by running an OLS regression and doing some\ndiagnostics. We will begin by running an OLS regression and looking at\ndiagnostic plots examining residuals, fitted values, Cook&#8217;s distance, and leverage.<\/p>\n<div id=\"unnamed-chunk-4\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(ols<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">lm<\/span><span class=\"hl std\">(crime<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">poverty<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">single,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= cdata))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:\r\n## lm(formula = crime ~ poverty + single, data = cdata)\r\n## \r\n## Residuals:\r\n##    Min     1Q Median     3Q    Max \r\n## -811.1 -114.3  -22.4  121.9  689.8 \r\n## \r\n## Coefficients:\r\n##             Estimate Std. Error t value Pr(&gt;|t|)    \r\n## (Intercept) -1368.19     187.21   -7.31  2.5e-09 ***\r\n## poverty         6.79       8.99    0.76     0.45    \r\n## single        166.37      19.42    8.57  3.1e-11 ***\r\n## ---\r\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n## \r\n## Residual standard error: 244 on 48 degrees of freedom\r\n## Multiple R-squared:  0.707,\tAdjusted R-squared:  0.695 \r\n## F-statistic:   58 on 2 and 48 DF,  p-value: 1.58e-13\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">opar<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">par<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">mfrow<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">2<\/span><span class=\"hl std\">,<\/span><span class=\"hl num\">2<\/span><span class=\"hl std\">),<\/span> <span class=\"hl kwc\">oma<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">0<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">0<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">1.1<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">0<\/span><span class=\"hl std\">))<\/span>\r\n<span class=\"hl kwd\">plot<\/span><span class=\"hl std\">(ols,<\/span> <span class=\"hl kwc\">las<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/rreg-unnamed-chunk-4.png\" alt=\"plot of chunk unnamed-chunk-4\" width=\"500px\" height=\"500px\" \/><\/div>\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">par<\/span><span class=\"hl std\">(opar)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>From these plots, we can identify observations 9, 25, and 51 as possibly\nproblematic to our model. We can look at these observations to see which states\nthey represent.<\/p>\n<div id=\"unnamed-chunk-5\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">cdata[<\/span><span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">9<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">25<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">51<\/span><span class=\"hl std\">),<\/span> <span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">2<\/span><span class=\"hl std\">]<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##    sid state\r\n## 9    9    fl\r\n## 25  25    ms\r\n## 51  51    dc\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>DC, Florida and Mississippi have either high leverage or\nlarge residuals. We can display the observations that have relatively\nlarge values of Cook&#8217;s D. A conventional cut-off point is \\({4}\/{n}\\),\nwhere \\(n\\) is the number of observations in the data set. We\nwill use this criterion to select the values to display.<\/p>\n<div id=\"unnamed-chunk-6\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">d1<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cooks.distance<\/span><span class=\"hl std\">(ols)<\/span>\r\n<span class=\"hl std\">r<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">stdres<\/span><span class=\"hl std\">(ols)<\/span>\r\n<span class=\"hl std\">a<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(cdata, d1, r)<\/span>\r\n<span class=\"hl std\">a[d1<\/span> <span class=\"hl opt\">&gt;<\/span> <span class=\"hl num\">4<\/span><span class=\"hl opt\">\/<\/span><span class=\"hl num\">51<\/span><span class=\"hl std\">, ]<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##    sid state crime murder pctmetro pctwhite pcths poverty single     d1\r\n## 1    1    ak   761    9.0     41.8     75.2  86.6     9.1   14.3 0.1255\r\n## 9    9    fl  1206    8.9     93.0     83.5  74.4    17.8   10.6 0.1426\r\n## 25  25    ms   434   13.5     30.7     63.3  64.3    24.7   14.7 0.6139\r\n## 51  51    dc  2922   78.5    100.0     31.8  73.1    26.4   22.1 2.6363\r\n##         r\r\n## 1  -1.397\r\n## 9   2.903\r\n## 25 -3.563\r\n## 51  2.616\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We probably should drop DC to begin with since it is not even a state. We\ninclude it in the analysis just to show that it has large Cook&#8217;s D and\ndemonstrate how it will be handled by <code>rlm<\/code>. Now we will look at\nthe residuals. We will\ngenerate a new variable called <code>absr1<\/code>, which is the absolute value of the\nresiduals (because the sign of the residual doesn&#8217;t matter). We then print the\nten observations with the highest absolute residual values.<\/p>\n<div id=\"unnamed-chunk-7\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">rabs<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">abs<\/span><span class=\"hl std\">(r)<\/span>\r\n<span class=\"hl std\">a<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(cdata, d1, r, rabs)<\/span>\r\n<span class=\"hl std\">asorted<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">a[<\/span><span class=\"hl kwd\">order<\/span><span class=\"hl std\">(<\/span><span class=\"hl opt\">-<\/span><span class=\"hl std\">rabs), ]<\/span>\r\n<span class=\"hl std\">asorted[<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">10<\/span><span class=\"hl std\">, ]<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##    sid state crime murder pctmetro pctwhite pcths poverty single      d1\r\n## 25  25    ms   434   13.5     30.7     63.3  64.3    24.7   14.7 0.61387\r\n## 9    9    fl  1206    8.9     93.0     83.5  74.4    17.8   10.6 0.14259\r\n## 51  51    dc  2922   78.5    100.0     31.8  73.1    26.4   22.1 2.63625\r\n## 46  46    vt   114    3.6     27.0     98.4  80.8    10.0   11.0 0.04272\r\n## 26  26    mt   178    3.0     24.0     92.6  81.0    14.9   10.8 0.01676\r\n## 21  21    me   126    1.6     35.7     98.5  78.8    10.7   10.6 0.02233\r\n## 1    1    ak   761    9.0     41.8     75.2  86.6     9.1   14.3 0.12548\r\n## 31  31    nj   627    5.3    100.0     80.8  76.7    10.9    9.6 0.02229\r\n## 14  14    il   960   11.4     84.0     81.0  76.2    13.6   11.5 0.01266\r\n## 20  20    md   998   12.7     92.8     68.9  78.4     9.7   12.0 0.03570\r\n##         r  rabs\r\n## 25 -3.563 3.563\r\n## 9   2.903 2.903\r\n## 51  2.616 2.616\r\n## 46 -1.742 1.742\r\n## 26 -1.461 1.461\r\n## 21 -1.427 1.427\r\n## 1  -1.397 1.397\r\n## 31  1.354 1.354\r\n## 14  1.338 1.338\r\n## 20  1.287 1.287\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Now let&#8217;s run our first robust regression. Robust regression is done by\niterated re-weighted least squares (IRLS). The command for running robust regression\nis <code>rlm<\/code> in the <code>MASS<\/code> package. There are several weighting functions\nthat can be used for IRLS. We are\ngoing to first use the Huber weights in this example. We will then look at\nthe final weights created by the IRLS process. This can be very\nuseful.<\/p>\n<div id=\"unnamed-chunk-8\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(rr.huber<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">rlm<\/span><span class=\"hl std\">(crime<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">poverty<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">single,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= cdata))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call: rlm(formula = crime ~ poverty + single, data = cdata)\r\n## Residuals:\r\n##    Min     1Q Median     3Q    Max \r\n## -846.1 -125.8  -16.5  119.2  679.9 \r\n## \r\n## Coefficients:\r\n##             Value     Std. Error t value  \r\n## (Intercept) -1423.037   167.590     -8.491\r\n## poverty         8.868     8.047      1.102\r\n## single        168.986    17.388      9.719\r\n## \r\n## Residual standard error: 182 on 48 degrees of freedom\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">hweights<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">data.frame<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">state<\/span> <span class=\"hl std\">= cdata<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">state,<\/span> <span class=\"hl kwc\">resid<\/span> <span class=\"hl std\">= rr.huber<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">resid,<\/span> <span class=\"hl kwc\">weight<\/span> <span class=\"hl std\">= rr.huber<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">w)<\/span>\r\n<span class=\"hl std\">hweights2<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">hweights[<\/span><span class=\"hl kwd\">order<\/span><span class=\"hl std\">(rr.huber<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">w), ]<\/span>\r\n<span class=\"hl std\">hweights2[<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">15<\/span><span class=\"hl std\">, ]<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##    state   resid weight\r\n## 25    ms -846.09 0.2890\r\n## 9     fl  679.94 0.3595\r\n## 46    vt -410.48 0.5956\r\n## 51    dc  376.34 0.6494\r\n## 26    mt -356.14 0.6865\r\n## 21    me -337.10 0.7252\r\n## 31    nj  331.12 0.7384\r\n## 14    il  319.10 0.7661\r\n## 1     ak -313.16 0.7807\r\n## 20    md  307.19 0.7958\r\n## 19    ma  291.21 0.8395\r\n## 18    la -266.96 0.9159\r\n## 2     al  105.40 1.0000\r\n## 3     ar   30.54 1.0000\r\n## 4     az  -43.25 1.0000\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can see that roughly, as the absolute residual goes down, the weight goes up. In other words,\ncases with a large residuals tend to be down-weighted. This output shows us that the\nobservation for Mississippi will be down-weighted the most.\u00a0Florida will\nalso be substantially down-weighted. All observations not shown above have\na weight of 1. In OLS regression, all\ncases have a weight of 1. Hence, the more cases in the robust regression\nthat have a weight close to one, the closer the results of the OLS and robust\nregressions.<\/p>\n<p>Next, let&#8217;s run the same model, but using the bisquare weighting function.\nAgain, we can look at the weights.<\/p>\n<div id=\"unnamed-chunk-9\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">rr.bisquare<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">rlm<\/span><span class=\"hl std\">(crime<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">poverty<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">single,<\/span> <span class=\"hl kwc\">data<\/span><span class=\"hl std\">=cdata,<\/span> <span class=\"hl kwc\">psi<\/span> <span class=\"hl std\">= psi.bisquare)<\/span>\r\n<span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(rr.bisquare)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call: rlm(formula = crime ~ poverty + single, data = cdata, psi = psi.bisquare)\r\n## Residuals:\r\n##    Min     1Q Median     3Q    Max \r\n##   -906   -141    -15    115    668 \r\n## \r\n## Coefficients:\r\n##             Value     Std. Error t value  \r\n## (Intercept) -1535.334   164.506     -9.333\r\n## poverty        11.690     7.899      1.480\r\n## single        175.930    17.068     10.308\r\n## \r\n## Residual standard error: 202 on 48 degrees of freedom\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">biweights<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">data.frame<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">state<\/span> <span class=\"hl std\">= cdata<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">state,<\/span> <span class=\"hl kwc\">resid<\/span> <span class=\"hl std\">= rr.bisquare<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">resid,<\/span> <span class=\"hl kwc\">weight<\/span> <span class=\"hl std\">= rr.bisquare<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">w)<\/span>\r\n<span class=\"hl std\">biweights2<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">biweights[<\/span><span class=\"hl kwd\">order<\/span><span class=\"hl std\">(rr.bisquare<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">w), ]<\/span>\r\n<span class=\"hl std\">biweights2[<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">15<\/span><span class=\"hl std\">, ]<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##    state  resid   weight\r\n## 25    ms -905.6 0.007653\r\n## 9     fl  668.4 0.252871\r\n## 46    vt -402.8 0.671495\r\n## 26    mt -360.9 0.731137\r\n## 31    nj  346.0 0.751348\r\n## 18    la -332.7 0.768938\r\n## 21    me -328.6 0.774103\r\n## 1     ak -325.9 0.777662\r\n## 14    il  313.1 0.793659\r\n## 20    md  308.8 0.799066\r\n## 19    ma  297.6 0.812597\r\n## 51    dc  260.6 0.854442\r\n## 50    wy -234.2 0.881661\r\n## 5     ca  201.4 0.911714\r\n## 10    ga -186.6 0.924033\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can see that the weight given to Mississippi is dramatically lower using\nthe bisquare weighting function than the Huber weighting function and the\nparameter estimates from these two different weighting methods differ. When comparing the results of a regular OLS\nregression and a robust regression, if the results are very different, you will\nmost likely want to use the results from the robust regression. Large\ndifferences suggest that the model parameters are being highly influenced by\noutliers. Different\nfunctions have advantages and drawbacks. Huber weights can have difficulties\nwith severe outliers, and bisquare weights can have difficulties converging or\nmay yield multiple solutions.<\/p>\n<p>As you can see, the results from the two analyses are fairly different,\nespecially with respect to the coefficients of <code>single<\/code> and the constant\n(<code>intercept<\/code>).\nWhile normally we are not interested in the constant, if you had centered one or\nboth of the predictor variables, the constant would be useful. On the\nother hand, you will notice that <code>poverty<\/code> is not statistically significant\nin either analysis, whereas <code>single<\/code> is significant in both analyses.<\/p>\n<h2>Things to consider<\/h2>\n<ul>\n<li>Robust regression does not address issues of heterogeneity of variance.\nThis problem can be addressed by using functions in the <code>sandwich<\/code> package\nafter the <code>lm<\/code> function.<\/li>\n<li>The examples shown here have presented <code>R<\/code> code for M estimation. There are\nother estimation options available in <code>rlm<\/code> and other R commands and\npackages: Least trimmed squares using\n<code>ltsReg<\/code> in the <code>robustbase<\/code> package and MM using <code>rlm<\/code>.<\/li>\n<\/ul>\n<h2>References<\/h2>\n<ul>\n<li>Li, G. 1985. <i>Robust regression<\/i>. In <i>Exploring Data Tables, Trends, and Shapes<\/i>,\ned. D. C. Hoaglin, F. Mosteller, and J. W. Tukey, Wiley.<\/li>\n<li>John Fox, <i>Applied regression analysis, linear models, and related models<\/i>,\nSage publications, Inc, 1997<\/li>\n<\/ul>\n<h2>See also<\/h2>\n<ul>\n<li><a href=\"http:\/\/stat.ethz.ch\/R-manual\/R-patched\/library\/MASS\/html\/rlm.html\">\nR documentation for <code>rlm<\/code><\/a><\/li>\n<\/ul>\n","protected":false},"excerpt":{"rendered":"<p>Robust regression is an alternative to least squares regression when data are contaminated with outliers or influential observations, and it can also be used for the purpose of detecting influential&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/robust-regression\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":12,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-930","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/930","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=930"}],"version-history":[{"count":21,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/930\/revisions"}],"predecessor-version":[{"id":30700,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/930\/revisions\/30700"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=930"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}