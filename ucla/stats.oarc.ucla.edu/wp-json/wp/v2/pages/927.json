{"id":927,"date":"2016-02-11T18:29:03","date_gmt":"2016-02-11T18:29:03","guid":{"rendered":"http:\/\/dev2.hpc.ucla.edu\/r\/dae-3\/r-data-analysis-examples-ordinal-logistic-regression\/"},"modified":"2019-07-30T08:55:45","modified_gmt":"2019-07-30T15:55:45","slug":"ordinal-logistic-regression","status":"publish","type":"page","link":"https:\/\/stats.oarc.ucla.edu\/r\/dae\/ordinal-logistic-regression\/","title":{"rendered":"Ordinal Logistic Regression |  R Data Analysis Examples"},"content":{"rendered":"<h2><strong>Introduction<\/strong><\/h2>\n<ul>\n<li>The following page discusses how to use R&#8217;s <code>polr <\/code> package to perform an ordinal logistic regression.<\/li>\n<li>For a more mathematical treatment of the interpretation of results refer to:\u00a0<a href=\"https:\/\/stats.idre.ucla.edu\/r\/faq\/ologit-coefficients\/\">How do I interpret the coefficients in an ordinal logistic regression in R?<\/a><\/li>\n<\/ul>\n<h3>Preparation<\/h3>\n<p>Make sure that you can load the following packages before trying to run the examples on this page. If you do not have\na package installed, run: <code>install.packages(\"packagename\")<\/code>, or if you see the version is out of date, run: <code>update.packages()<\/code>.<\/p>\n<div id=\"unnamed-chunk-2\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">require<\/span><span class=\"hl std\">(foreign)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(ggplot2)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(MASS)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(Hmisc)<\/span>\r\n<span class=\"hl kwd\">require<\/span><span class=\"hl std\">(reshape2)<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p><code class=\"knitr inline\"><strong>Version info: <\/strong>Code for this page was tested in R version 3.1.1 (2014-07-10)\nOn: 2014-08-21\nWith: reshape2 1.4; Hmisc 3.14-4; Formula 1.1-2; survival 2.37-7; lattice 0.20-29; MASS 7.3-33; ggplot2 1.0.0; foreign 0.8-61; knitr 1.6\n<\/code><\/p>\n<p><strong>Please note:<\/strong> The purpose of this page is to show how to use various data\nanalysis commands. It does not cover all aspects of the research process which\nresearchers are expected to do. In particular, it does not cover data\ncleaning and checking, verification of assumptions, model diagnostics or\npotential follow-up analyses.<\/p>\n<h2>Examples of ordinal logistic regression<\/h2>\n<p>Example 1: A marketing research firm wants to investigate what factors influence the size of soda (small, medium, large or\nextra large) that people order at a fast-food chain. These factors may include what type of sandwich is ordered (burger or chicken), whether or not fries are also ordered, and age of the consumer. While the outcome variable, size of soda, is obviously ordered, the difference between the various sizes is not consistent. The difference between small and medium is 10 ounces, between medium and large 8, and between large and extra large 12.<\/p>\n<p>Example 2: A researcher is interested in what factors influence medaling in Olympic swimming. Relevant predictors include at training hours, diet, age, and popularity of swimming in the athlete&#8217;s home country. The researcher believes that the distance between gold and silver is larger than the distance between silver and bronze.<\/p>\n<p>Example 3: A study looks at factors that influence the decision of whether to apply to graduate school. College juniors are asked if they are\nunlikely, somewhat likely, or very likely to apply to graduate school.\nHence, our outcome variable has three categories. Data on parental educational status, whether the undergraduate institution is\npublic or private, and current GPA is also collected. The researchers have reason to believe that the &#8220;distances&#8221; between these three\npoints are not equal. For example, the &#8220;distance&#8221; between &#8220;unlikely&#8221; and &#8220;somewhat likely&#8221; may be shorter than the distance between &#8220;somewhat likely&#8221; and &#8220;very likely&#8221;.<\/p>\n<h2>Description of the Data<\/h2>\n<p>For our data analysis below, we are going to expand on Example 3 about applying to graduate school. We have simulated some data for this\nexample and it can be obtained from our website:<\/p>\n<div id=\"unnamed-chunk-3\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><strong><span class=\"hl std\">dat<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">read.dta<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"https:\/\/stats.idre.ucla.edu\/stat\/data\/ologit.dta\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl kwd\">head<\/span><span class=\"hl std\">(dat)<\/span>\r\n<\/strong><\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##             apply pared public  gpa\r\n## 1     very likely     0      0 3.26\r\n## 2 somewhat likely     1      0 3.21\r\n## 3        unlikely     1      1 3.94\r\n## 4 somewhat likely     0      0 2.81\r\n## 5 somewhat likely     0      0 2.53\r\n## 6        unlikely     0      1 2.59\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>This hypothetical data set has a three level variable called\n<code>apply<\/code>, with levels &#8220;unlikely&#8221;, &#8220;somewhat likely&#8221;, and &#8220;very likely&#8221;, coded 1, 2, and 3, respectively, that we will use as our outcome variable.\nWe also have three variables that we will use as predictors: <code>pared<\/code>,\nwhich is a 0\/1 variable indicating whether at least one parent has a graduate degree;\n<code>public<\/code>, which is a 0\/1 variable where 1 indicates that the\nundergraduate institution is public and 0 private, and\n<code>gpa<\/code>, which is the student&#8217;s grade point average.\nLet&#8217;s start with the descriptive statistics of these variables.<\/p>\n<div id=\"unnamed-chunk-4\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## one at a time, table apply, pared, and public<\/span>\r\n<a href=\"https:\/\/stats.idre.ucla.edu\/r\/faq\/ologit-coefficients\/\"><span class=\"hl kwd\">lapply<\/span><span class=\"hl std\">(dat[,<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"apply\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"pared\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"public\"<\/span><span class=\"hl std\">)], table)<\/span>\r\n<\/a><\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## $apply\r\n## \r\n##        unlikely somewhat likely     very likely \r\n##             220             140              40 \r\n## \r\n## $pared\r\n## \r\n##   0   1 \r\n## 337  63 \r\n## \r\n## $public\r\n## \r\n##   0   1 \r\n## 343  57\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## three way cross tabs (xtabs) and flatten the table<\/span>\r\n<span class=\"hl kwd\">ftable<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">xtabs<\/span><span class=\"hl std\">(<\/span><span class=\"hl opt\">~<\/span> <span class=\"hl std\">public<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">apply<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">pared,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= dat))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                        pared   0   1\r\n## public apply                        \r\n## 0      unlikely              175  14\r\n##        somewhat likely        98  26\r\n##        very likely            20  10\r\n## 1      unlikely               25   6\r\n##        somewhat likely        12   4\r\n##        very likely             7   3\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(dat<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">gpa)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \r\n##    1.90    2.72    2.99    3.00    3.27    4.00\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">sd<\/span><span class=\"hl std\">(dat<\/span><span class=\"hl opt\">$<\/span><span class=\"hl std\">gpa)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## [1] 0.3979\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can also examine the distribution of <code>gpa<\/code> at every level of <code>apply<\/code>and broken down by <code>public<\/code> and <code>pared<\/code>. This creates a 2 x 2 grid\nwith a boxplot of gpa for every level of apply, for particular values of <code>pared<\/code>and <code>public<\/code>. To better see the data, we also add the raw data points on top of the box plots, with a small amount of noise (often called &#8220;jitter&#8221;) and 50% transparency so they do not overwhelm the boxplots. Finally, in addition to the cells, we plot all of the marginal relationships. The margins make the final plot a 3 x 3 grid. In the\nlower right hand corner, is the overall relationship between <code>apply<\/code> and <code>gpa<\/code> which appears slightly positive. To do this, we use the <code>ggplot2<\/code> package.<\/p>\n<div id=\"unnamed-chunk-5\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(dat,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span> <span class=\"hl std\">= apply,<\/span> <span class=\"hl kwc\">y<\/span> <span class=\"hl std\">= gpa))<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_boxplot<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">size<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">.75<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_jitter<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">alpha<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">.5<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">facet_grid<\/span><span class=\"hl std\">(pared<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">public,<\/span> <span class=\"hl kwc\">margins<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">TRUE<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">theme<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">axis.text.x<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">element_text<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">angle<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">45<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">hjust<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">vjust<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">))<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ologit-unnamed-chunk-5.png\" alt=\"Boxplot with jittered raw data values faceted by parental education (hi vs. low) and schooltype (public vs. private)\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<h2>Analysis methods you might consider<\/h2>\n<p>Below is a list of some analysis methods you may have encountered. Some of the methods listed are quite reasonable while others have either\nfallen out of favor or have limitations.<\/p>\n<ul>\n<li>Ordered logistic regression: the focus of this page.<\/li>\n<li>OLS regression: This analysis is problematic because the assumptions of OLS are violated when it is used with a non-interval\noutcome variable.<\/li>\n<li>ANOVA: If you use only one continuous predictor, you could &#8220;flip&#8221; the model around so that, say, <code>gpa<\/code> was the\noutcome variable and <code>apply<\/code> was the predictor variable. Then you could run a one-way ANOVA.\nThis isn&#8217;t a bad thing to do if you only have one predictor variable (from the logistic model), and it is continuous.<\/li>\n<li>Multinomial logistic regression: This is similar to doing ordered logistic regression, except that it is assumed that there is no order to the categories of the outcome variable (i.e., the categories are nominal). The downside of this approach is that the information contained in the ordering is lost.<\/li>\n<li>Ordered probit regression: This is very, very similar to running an ordered logistic regression. The main difference is in the\ninterpretation of the coefficients.<\/li>\n<\/ul>\n<h2>Ordered logistic regression<\/h2>\n<p>Below we use the <code>polr<\/code> command from the <code>MASS<\/code> package to estimate an ordered logistic regression model. The command name comes from proportional odds logistic regression, highlighting the proportional odds assumption in our model. <code>polr<\/code> uses the standard formula interface in <code>R<\/code> for specifying a regression model with outcome followed by predictors. We also specify <code>Hess=TRUE<\/code> to have the model return the observed information matrix from optimization (called the Hessian) which is used to get standard errors.<\/p>\n<h3>Definitions<\/h3>\n<p>To understand how to interpret the coefficients, first let&#8217;s establish some notation and review the concepts involved in ordinal logistic regression. Let $Y$ be an ordinal outcome with $J$ categories. Then $P(Y \\le j)$ is the cumulative probability of $Y$ less than or equal to a specific category $j = 1, \\cdots, J-1$. The odds of being less than or equal a particular category can be defined as<\/p>\n<p>$$\\frac{P(Y \\le j)}{P(Y&gt;j)}$$<\/p>\n<p>for $j=1,\\cdots, J-1$ since $P(Y &gt; J) = 0$ and dividing by zero is undefined. The <strong>log odds<\/strong>\u00a0 is also known as the <strong>logit<\/strong>, so that<\/p>\n<p>$$log \\frac{P(Y \\le j)}{P(Y&gt;j)} = logit (P(Y \\le j)).$$<\/p>\n<p>In R&#8217;s <code>polr<\/code> the ordinal logistic regression model is parameterized as<\/p>\n<p>$$logit (P(Y \\le j)) = \\beta_{j0} &#8211; \\eta_{1}x_1 &#8211; \\cdots &#8211; \\eta_{p} x_p.$$<\/p>\n<p>Then we can fit the following ordinal logistic regression model:<\/p>\n<div id=\"unnamed-chunk-6\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## fit ordered logit model and store results 'm'<\/span>\r\n<span class=\"hl std\">m<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">polr<\/span><span class=\"hl std\">(apply<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">pared<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">public<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">gpa,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= dat,<\/span> <span class=\"hl kwc\">Hess<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">TRUE<\/span><span class=\"hl std\">)<\/span>\r\n\r\n<span class=\"hl com\">## view a summary of the model<\/span>\r\n<span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(m)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## Call:\r\n## polr(formula = apply ~ pared + public + gpa, data = dat, Hess = TRUE)\r\n## \r\n## Coefficients:\r\n##          Value Std. Error t value\r\n## pared   1.0477      0.266   3.942\r\n## public -0.0588      0.298  -0.197\r\n## gpa     0.6159      0.261   2.363\r\n## \r\n## Intercepts:\r\n##                             Value  Std. Error t value\r\n## unlikely|somewhat likely     2.204  0.780      2.827 \r\n## somewhat likely|very likely  4.299  0.804      5.345 \r\n## \r\n## Residual Deviance: 717.02 \r\n## AIC: 727.02\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The estimated model can be written as:<\/p>\n<p>$$\n\\begin{eqnarray}\nlogit (\\hat{P}(Y \\le 1)) &amp; = &amp; 2.20 &#8211; 1.05*PARED &#8211; (-0.06)*PUBLIC &#8211; 0.616*GPA \\\\\nlogit (\\hat{P}(Y \\le 2)) &amp; = &amp; 4.30 &#8211; 1.05*PARED &#8211; (-0.06)*PUBLIC &#8211; 0.616*GPA\n\\end{eqnarray}\n$$<\/p>\n<p>In the output above, we see<\/p>\n<ul>\n<li>Call, this is <code>R<\/code> reminding us what type of model we ran, what options we specified, etc.<\/li>\n<li>Next we see the usual regression output coefficient table including the value of each coefficient, standard errors, and t value, which is simply the ratio of the coefficient to its standard error. There is no significance test by default.<\/li>\n<li>Next we see the estimates for the two intercepts, which are sometimes called cutpoints. The intercepts indicate where the latent variable is cut to make the three groups that we observe in our data. Note that this latent variable is continuous. In general,\nthese are not used in the interpretation of the results. The cutpoints are closely related to thresholds, which are reported by other statistical packages.<\/li>\n<li>Finally, we see the residual deviance, -2 * Log Likelihood of the model as well\nas the AIC. Both the deviance and AIC are useful for model comparison.<\/li>\n<\/ul>\n<p>Some people are not satisfied without a p value. One way to calculate a p-value in this case is by comparing the t-value against the standard normal distribution, like a z test. Of course this is only true with infinite degrees of freedom, but is reasonably approximated by large samples, becoming increasingly biased as sample size decreases. This approach is used in other software packages such as <code>Stata<\/code> and is trivial to do. First we store the coefficient table, then calculate the p-values and combine back with the table.<\/p>\n<div id=\"unnamed-chunk-7\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## store table<\/span>\r\n<span class=\"hl std\">(ctable<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(m)))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                                Value Std. Error t value\r\n## pared                        1.04769     0.2658  3.9418\r\n## public                      -0.05879     0.2979 -0.1974\r\n## gpa                          0.61594     0.2606  2.3632\r\n## unlikely|somewhat likely     2.20391     0.7795  2.8272\r\n## somewhat likely|very likely  4.29936     0.8043  5.3453\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## calculate and store p values<\/span>\r\n<span class=\"hl std\">p<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">pnorm<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">abs<\/span><span class=\"hl std\">(ctable[,<\/span> <span class=\"hl str\">\"t value\"<\/span><span class=\"hl std\">]),<\/span> <span class=\"hl kwc\">lower.tail<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">FALSE<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">*<\/span> <span class=\"hl num\">2<\/span>\r\n\r\n<span class=\"hl com\">## combined table<\/span>\r\n<span class=\"hl std\">(ctable<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(ctable,<\/span> <span class=\"hl str\">\"p value\"<\/span> <span class=\"hl std\">= p))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##                                Value Std. Error t value   p value\r\n## pared                        1.04769     0.2658  3.9418 8.087e-05\r\n## public                      -0.05879     0.2979 -0.1974 8.435e-01\r\n## gpa                          0.61594     0.2606  2.3632 1.812e-02\r\n## unlikely|somewhat likely     2.20391     0.7795  2.8272 4.696e-03\r\n## somewhat likely|very likely  4.29936     0.8043  5.3453 9.027e-08\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can also get confidence intervals for the parameter estimates. These can be obtained either by profiling the likelihood function or by using the standard errors and assuming a normal distribution. Note that profiled CIs are not symmetric (although they are usually close to symmetric). If the 95% CI does not cross 0, the parameter estimate is statistically significant.<\/p>\n<div id=\"unnamed-chunk-8\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">(ci<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">confint<\/span><span class=\"hl std\">(m))<\/span> <span class=\"hl com\"># default method gives profiled CIs<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"message\">\n<pre class=\"knitr r\">## Waiting for profiling to be done...\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##          2.5 % 97.5 %\r\n## pared   0.5282 1.5722\r\n## public -0.6522 0.5191\r\n## gpa     0.1076 1.1309\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">confint.default<\/span><span class=\"hl std\">(m)<\/span> <span class=\"hl com\"># CIs assuming normality<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##          2.5 % 97.5 %\r\n## pared   0.5268  1.569\r\n## public -0.6426  0.525\r\n## gpa     0.1051  1.127\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The CIs for both <code>pared<\/code> and <code>gpa<\/code> do not include 0; <code>public<\/code> does. The estimates in the output are given in units of ordered logits, or\nordered log odds. So for <code>pared<\/code>, we would say that for a one unit increase in <code>pared<\/code> (i.e., going from 0 to 1), we expect a 1.05 increase in\nthe expected value of <code>apply<\/code> on the log odds scale, given all of the other variables in the model are held constant. For <code>gpa<\/code>, we would say that for a one unit increase in <code>gpa<\/code>, we would expect a 0.62 increase in the expected value of <code>apply<\/code> in the log odds scale, given that all of the other variables in the model are held constant.<\/p>\n<p>The coefficients from the model can be somewhat difficult to interpret because they are scaled in terms of logs. Another way to interpret logistic regression models is to convert the coefficients into odds ratios. To get the OR and confidence intervals, we just exponentiate the estimates and confidence intervals.<\/p>\n<div id=\"unnamed-chunk-9\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## odds ratios<\/span>\r\n<span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(m))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##  pared public    gpa \r\n## 2.8511 0.9429 1.8514\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl com\">## OR and CI<\/span>\r\n<span class=\"hl kwd\">exp<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">OR<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">coef<\/span><span class=\"hl std\">(m), ci))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##            OR  2.5 % 97.5 %\r\n## pared  2.8511 1.6958  4.817\r\n## public 0.9429 0.5209  1.681\r\n## gpa    1.8514 1.1136  3.098\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>These coefficients are called proportional odds ratios and we would interpret these pretty much as we would odds ratios from a binary\nlogistic regression.<\/p>\n<h3>Interpreting the odds ratio<\/h3>\n<p>There are many equivalent interpretations of the odds ratio based on how the probability is defined and the direction of the odds. For a detailed justification, refer to <a href=\"https:\/\/stats.idre.ucla.edu\/r\/faq\/ologit-coefficients\/\">How do I interpret the coefficients in an ordinal logistic regression in R?<\/a> The <strong>(*)<\/strong> symbol below denotes the easiest interpretation among the choices.<\/p>\n<h4>Parental Education<\/h4>\n<ol>\n<li><strong>(*)<\/strong> For students whose parents <em>did\u00a0<\/em>attend college, the odds of being <i>more <\/i>likely (i.e., <em>very<\/em> or <em>somewhat<\/em> likely versus unlikely) to apply is 2.85 times that of students whose parents did not go to college, holding constant all other variables.<\/li>\n<li>For students whose parents did <em>not<\/em> attend college, the odds of being <i>less <\/i>likely to apply\u00a0(i.e., <em>unlikely<\/em> versus somewhat or very likely) is 2.85 times that of students whose parents did go to college, holding constant all other variables.<\/li>\n<\/ol>\n<h4>School Type<\/h4>\n<ol>\n<li>For students in public school, the odds of being <i>more <\/i>likely\u00a0(i.e., <em>very<\/em> or <em>somewhat<\/em> likely versus unlikely) to apply is 5.71% lower [i.e., (1 -0.943) x 100%] than private school students, holding constant all other variables.\n<ul>\n<li><strong>(*) <\/strong>For students in private school, the odds of being <em>more\u00a0<\/em>likely to apply is 1.06 times [i.e., 1\/0.943] that of public school students, holding constant all other variables (positive odds ratio).<\/li>\n<\/ul>\n<\/li>\n<li>For students in private school, the odds of being <em>less\u00a0<\/em>likely to apply\u00a0(i.e., <em>unlikely<\/em> versus somewhat or very likely) is 5.71% lower than public school students, holding constant all other variables.\n<ul>\n<li>For students in public school, the odds of being\u00a0<em>less\u00a0<\/em>likely to apply is 1.06 times that of private school students, holding constant all other variables (positive odds ratio).<\/li>\n<\/ul>\n<\/li>\n<\/ol>\n<h4>GPA<\/h4>\n<ol>\n<li><strong>(*)<\/strong> For every one unit <em>in<\/em><i>crease <\/i>in student&#8217;s GPA the odds of being <i>more <\/i>likely to apply (<em>very<\/em> or <em>somewhat<\/em> likely versus unlikely) is multiplied 1.85 times (i.e., increases 85%), holding constant all other variables.<\/li>\n<li>For every one unit <i>decrease <\/i>in student&#8217;s GPA the odds of being <i>less <\/i>likely to apply (<em>unlikely<\/em> versus somewhat or very likely) is multiplied 1.85 times, holding constant all other variables.<\/li>\n<\/ol>\n<h3>Proportional odds assumption<\/h3>\n<p>One of the assumptions underlying ordinal logistic (and ordinal probit) regression is that the relationship between each pair of outcome groups is the same. In other words, ordinal logistic regression assumes that the coefficients that describe the relationship between, say, the lowest versus all higher categories of the response variable are the same as those that describe the relationship between the next lowest category and all higher categories, etc. This is called the proportional odds assumption or the parallel regression assumption. Because the relationship between all pairs of groups is the same, there is only one set of coefficients.<\/p>\n<p>If this was not the case, we would need different sets of coefficients in the model to describe the relationship between each pair of outcome groups. Thus, in order to asses the appropriateness of our model, we need to evaluate whether the proportional odds assumption is tenable. Statistical tests to do this are available in some software packages. However, these tests have been criticized for having a tendency to reject the null hypothesis (that the sets of coefficients are the same), and hence, indicate that there the parallel slopes assumption does not hold, in cases where the assumption does hold (see Harrell 2001 p. 335). We were unable to locate a facility in R to perform any of the tests commonly used to test the parallel slopes assumption. However, Harrell does recommend a graphical method for assessing the parallel\nslopes assumption. The values displayed in this graph are essentially (linear) predictions from a logit model, used to model the probability that y is greater than or equal to a given value (for each level of y), using one predictor (x) variable at a time. In order create this graph, you will need the <code>Hmisc<\/code> library.<\/p>\n<p>The code below contains two commands (the first command falls on multiple lines) and is used to create this graph to test the proportional odds assumption. Basically, we will graph predicted logits from individual logistic regressions with a single predictor where the outcome groups are defined by either apply &gt;= 2 and apply &gt;= 3. If the difference between predicted logits for varying levels of a predictor, say <code>pared<\/code>, are the same whether the outcome is defined by apply &gt;= 2 or apply &gt;=3, then we can be confident that the proportional odds assumption holds. In other words, if the difference between logits for <code>pared<\/code> = 0 and <code>pared<\/code> = 1 is the same when the outcome is apply &gt;= 2 as the difference when the outcome is apply &gt;= 3, then the proportional odds assumption likely holds.<\/p>\n<p>The first command creates the function that estimates the values that will be graphed. The first line of this command tells R that <code>sf<\/code> is a function, and that this function takes one argument, which we label y. The <code>sf<\/code> function will calculate the log odds of being greater than or equal to each value of the target variable. For our purposes, we would like the log odds of apply being greater than or equal to 2, and then greater than or equal to 3. Depending on the number of categories in your dependent variable, and the coding of your variables, you\nmay have to edit this function. Below the function is configured for a <em>y<\/em> variable with three levels, 1, 2, 3. If your dependent variable has 4 levels, labeled 1, 2, 3, 4 you would need to add <code>'Y&gt;=4'=qlogis(mean(y &gt;= 4))<\/code> (minus the quotation marks) inside the first set of parentheses. If your dependent variable were coded 0, 1, 2 instead of 1, 2, 3, you would need to edit the code, replacing each instance of 1 with 0, 2 with 1, and so on. Inside the <code>sf<\/code> function we find the <code>qlogis<\/code> function, which transforms a probability to a logit. So, we will basically feed probabilities of apply being greater than 2 or 3 to <code>qlogis<\/code>, and it will return the logit transformations of these probabilites. Inside the <code>qlogis<\/code> function we see that we want the log odds of the mean of <code>y &gt;= 2<\/code>. When we supply a y argument, such as <code>apply<\/code>, to function <code>sf<\/code>, <code>y &gt;= 2<\/code> will evaluate to a 0\/1 (FALSE\/TRUE) vector, and taking the mean of that vector will give you the proportion of or probability that apply &gt;= 2.<\/p>\n<p>The second command below calls the function <code>sf<\/code> on several subsets of the data defined by the predictors. In this statement we see the <code>summary<\/code> function with a formula supplied as the first argument. When R sees a call to <code>summary<\/code> with a formula argument, it will calculate descriptive statistics for the variable on the left side of the formula by groups on the right side of the formula and will return the results in a nice table. By default, <code>summary<\/code> will calculate the mean of the left side variable. So, if we had used the code <code>summary(as.numeric(apply) ~ pared + public + gpa)<\/code> without the <code>fun<\/code> argument, we would get means on apply by pared, then by public, and finally by gpa broken up into 4 equal groups. However, we can override calculation of the mean by supplying our own function, namely <code>sf<\/code> to the <code>fun=<\/code> argument. The final command\nasks R to return the contents to the object <code>s<\/code>, which is a table.<\/p>\n<div id=\"unnamed-chunk-10\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">sf<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwa\">function<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">y<\/span><span class=\"hl std\">) {<\/span>\r\n  <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">'Y&gt;=1'<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">qlogis<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(y<\/span> <span class=\"hl opt\">&gt;=<\/span> <span class=\"hl num\">1<\/span><span class=\"hl std\">)),<\/span>\r\n    <span class=\"hl str\">'Y&gt;=2'<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">qlogis<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(y<\/span> <span class=\"hl opt\">&gt;=<\/span> <span class=\"hl num\">2<\/span><span class=\"hl std\">)),<\/span>\r\n    <span class=\"hl str\">'Y&gt;=3'<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">qlogis<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">mean<\/span><span class=\"hl std\">(y<\/span> <span class=\"hl opt\">&gt;=<\/span> <span class=\"hl num\">3<\/span><span class=\"hl std\">)))<\/span>\r\n<span class=\"hl std\">}<\/span>\r\n\r\n<span class=\"hl std\">(s<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">with<\/span><span class=\"hl std\">(dat,<\/span> <span class=\"hl kwd\">summary<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">as.numeric<\/span><span class=\"hl std\">(apply)<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">pared<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">public<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl std\">gpa,<\/span> <span class=\"hl kwc\">fun<\/span><span class=\"hl std\">=sf)))<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## as.numeric(apply)    N=400\r\n## \r\n## +-------+-----------+---+----+--------+------+\r\n## |       |           |N  |Y&gt;=1|Y&gt;=2    |Y&gt;=3  |\r\n## +-------+-----------+---+----+--------+------+\r\n## |pared  |No         |337|Inf |-0.37834|-2.441|\r\n## |       |Yes        | 63|Inf | 0.76547|-1.347|\r\n## +-------+-----------+---+----+--------+------+\r\n## |public |No         |343|Inf |-0.20479|-2.345|\r\n## |       |Yes        | 57|Inf |-0.17589|-1.548|\r\n## +-------+-----------+---+----+--------+------+\r\n## |gpa    |[1.90,2.73)|102|Inf |-0.39730|-2.773|\r\n## |       |[2.73,3.00)| 99|Inf |-0.26415|-2.303|\r\n## |       |[3.00,3.28)|100|Inf |-0.20067|-2.091|\r\n## |       |[3.28,4.00]| 99|Inf | 0.06062|-1.804|\r\n## +-------+-----------+---+----+--------+------+\r\n## |Overall|           |400|Inf |-0.20067|-2.197|\r\n## +-------+-----------+---+----+--------+------+\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>The table above displays the (linear) predicted values we would get if we regressed our\ndependent variable on our predictor variables one at a time, without the\nparallel slopes assumption. We can evaluate the parallel slopes assumption by running\na series of binary logistic regressions with varying cutpoints on the dependent variable and checking the equality of coefficients across cutpoints. We thus relax the parallel slopes assumption to checks its tenability. To accomplish this, we transform the original, ordinal, dependent variable into a new, binary, dependent variable which is equal to zero if the original, ordinal dependent variable (here <code>apply<\/code>) is less than some value <em>a<\/em>, and 1 if the\nordinal variable is greater than or equal to <em>a<\/em> (note, this is what the ordinal\nregression model coefficients represent as well). This is done for k-1 levels of\nthe ordinal variable and is executed by the <code>as.numeric(apply) &gt;= <em>a<\/em><\/code> coding below. The first line of code estimates the effect of <code>pared<\/code> on choosing &#8220;unlikely&#8221; applying versus &#8220;somewhat likely&#8221; or &#8220;very likely&#8221;. The second line of code estimates the effect of <code>pared<\/code> on choosing &#8220;unlikely&#8221; or &#8220;somewhat likely&#8221; applying versus &#8220;very likely&#8221; applying.\nLooking at the intercept for this model (-0.3783), we see that it matches the\npredicted value in the cell for <code>pared<\/code> equal to &#8220;no&#8221; in the column for Y&gt;=1, the value below it, for\n<code>pared<\/code> equals &#8220;yes&#8221; is equal to the intercept plus the coefficient for\n<code>pared<\/code> (i.e. -0.3783 + 1.1438 = 0.765).<\/p>\n<div id=\"unnamed-chunk-11\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">glm<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">I<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">as.numeric<\/span><span class=\"hl std\">(apply)<\/span> <span class=\"hl opt\">&gt;=<\/span> <span class=\"hl num\">2<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">pared,<\/span> <span class=\"hl kwc\">family<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"binomial\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= dat)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:  glm(formula = I(as.numeric(apply) &gt;= 2) ~ pared, family = \"binomial\", \r\n##     data = dat)\r\n## \r\n## Coefficients:\r\n## (Intercept)        pared  \r\n##      -0.378        1.144  \r\n## \r\n## Degrees of Freedom: 399 Total (i.e. Null);  398 Residual\r\n## Null Deviance:\t    551 \r\n## Residual Deviance: 534 \tAIC: 538\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">glm<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">I<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">as.numeric<\/span><span class=\"hl std\">(apply)<\/span> <span class=\"hl opt\">&gt;=<\/span> <span class=\"hl num\">3<\/span><span class=\"hl std\">)<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">pared,<\/span> <span class=\"hl kwc\">family<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"binomial\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">data<\/span> <span class=\"hl std\">= dat)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## \r\n## Call:  glm(formula = I(as.numeric(apply) &gt;= 3) ~ pared, family = \"binomial\", \r\n##     data = dat)\r\n## \r\n## Coefficients:\r\n## (Intercept)        pared  \r\n##       -2.44         1.09  \r\n## \r\n## Degrees of Freedom: 399 Total (i.e. Null);  398 Residual\r\n## Null Deviance:\t    260 \r\n## Residual Deviance: 252 \tAIC: 256\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>We can use the values in this table to help us assess whether\nthe proportional odds assumption is reasonable for our model. (Note,\nthe table is reproduced below, as well as above.) For example, when <code>pared<\/code> is\nequal to &#8220;no&#8221; the difference between the predicted value for apply greater than or equal to\ntwo and apply greater than or equal to three is roughly 2 (-0.378 &#8211; -2.440 = 2.062).\nFor <code>pared<\/code> equal to &#8220;yes&#8221; the difference in predicted values for apply greater\nthan or equal to two and apply greater than or equal to three is also roughly 2 (0.765 &#8211; -1.347 = 2.112).\nThis suggests that the parallel slopes assumption is reasonable (these differences are what graph below are plotting). Turning our attention to the predictions with public\nas a predictor variable, we see that when <code>public<\/code> is set to &#8220;no&#8221; the difference in\npredictions for apply greater than or equal to two, versus apply greater than or equal to\nthree is about 2.14 (-0.204 &#8211; -2.345 = 2.141). When <code>public<\/code> is set to &#8220;yes&#8221;\nthe difference between the coefficients is about 1.37 (-0.175 &#8211; -1.547 = 1.372). The\ndifferences in the distance between the two sets of coefficients (2.14 vs. 1.37) may suggest\nthat the parallel slopes assumption does not hold for the predictor <code>public<\/code>. That\nwould indicate that the effect of attending a public versus private school is different for\nthe transition from &#8220;unlikely&#8221; to &#8220;somewhat likely&#8221; and &#8220;somewhat likely&#8221; to &#8220;very likely.&#8221;<\/p>\n<p>The plot command below tells R that the object we wish to plot is <code>s<\/code>. The command <code>\nwhich=1:3<\/code> is a list of values indicating levels of y should be included in\nthe plot. If your dependent variable had more than three levels you would need\nto change the 3 to the <em>number<\/em> of categories (e.g., 4 for a four category\nvariable, even if it is numbered 0, 1, 2, 3). The command <code>pch=1:3<\/code> selects\nthe markers to use, and is optional, as are <code>xlab='logit'<\/code> which labels the\nx-axis, and <code>main=' '<\/code> which sets the main label for the graph to blank.\nIf the proportional odds assumption holds, for each predictor variable,\ndistance between the symbols for each set of categories of the dependent\nvariable, should remain similar. To help demonstrate this, we normalized all the first\nset of coefficients to be zero so there is a common reference point. Looking\nat the coefficients for the variable <code>pared<\/code> we see that the distance between the\ntwo sets of coefficients is similar. In contrast, the distances\nbetween the estimates for <code>public<\/code> are different (i.e., the markers are much\nfurther apart on the second line than on the first), suggesting that the proportional\nodds assumption may not hold.<\/p>\n<div id=\"unnamed-chunk-12\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">s[,<\/span> <span class=\"hl num\">4<\/span><span class=\"hl std\">]<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">s[,<\/span> <span class=\"hl num\">4<\/span><span class=\"hl std\">]<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">s[,<\/span> <span class=\"hl num\">3<\/span><span class=\"hl std\">]<\/span>\r\n<span class=\"hl std\">s[,<\/span> <span class=\"hl num\">3<\/span><span class=\"hl std\">]<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl std\">s[,<\/span> <span class=\"hl num\">3<\/span><span class=\"hl std\">]<\/span> <span class=\"hl opt\">-<\/span> <span class=\"hl std\">s[,<\/span> <span class=\"hl num\">3<\/span><span class=\"hl std\">]<\/span>\r\n<span class=\"hl std\">s<\/span> <span class=\"hl com\"># print<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">## as.numeric(apply)    N=400\r\n## \r\n## +-------+-----------+---+----+----+------+\r\n## |       |           |N  |Y&gt;=1|Y&gt;=2|Y&gt;=3  |\r\n## +-------+-----------+---+----+----+------+\r\n## |pared  |No         |337|Inf |0   |-2.062|\r\n## |       |Yes        | 63|Inf |0   |-2.113|\r\n## +-------+-----------+---+----+----+------+\r\n## |public |No         |343|Inf |0   |-2.140|\r\n## |       |Yes        | 57|Inf |0   |-1.372|\r\n## +-------+-----------+---+----+----+------+\r\n## |gpa    |[1.90,2.73)|102|Inf |0   |-2.375|\r\n## |       |[2.73,3.00)| 99|Inf |0   |-2.038|\r\n## |       |[3.00,3.28)|100|Inf |0   |-1.890|\r\n## |       |[3.28,4.00]| 99|Inf |0   |-1.864|\r\n## +-------+-----------+---+----+----+------+\r\n## |Overall|           |400|Inf |0   |-1.997|\r\n## +-------+-----------+---+----+----+------+\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">plot<\/span><span class=\"hl std\">(s,<\/span> <span class=\"hl kwc\">which<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">3<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">pch<\/span><span class=\"hl std\">=<\/span><span class=\"hl num\">1<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">3<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">xlab<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">'logit'<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">main<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">' '<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">xlim<\/span><span class=\"hl std\">=<\/span><span class=\"hl kwd\">range<\/span><span class=\"hl std\">(s[,<\/span><span class=\"hl num\">3<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">4<\/span><span class=\"hl std\">]))<\/span>\r\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"rimage default\"><img decoding=\"async\" class=\"plot\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/ologit-unnamed-chunk-12.png\" alt=\"Plot viewing proportional odds assumption\" width=\"500px\" height=\"500px\" \/><\/div>\n<\/div>\n<p>Once we are done assessing whether the assumptions of our model hold,\nwe can obtain predicted probabilities, which are usually easier to\nunderstand than either the coefficients or the odds ratios. For example, we can vary\n<code>gpa<\/code> for each level of <code>pared<\/code> and <code>public<\/code> and calculate\nthe probability of being in each category of apply. We do this by creating a new\ndataset of all the values to use for prediction.<\/p>\n<div id=\"unnamed-chunk-13\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">newdat<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">data.frame<\/span><span class=\"hl std\">(<\/span>\r\n  <span class=\"hl kwc\">pared<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">rep<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">0<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">1<\/span><span class=\"hl std\">,<\/span> <span class=\"hl num\">200<\/span><span class=\"hl std\">),<\/span>\r\n  <span class=\"hl kwc\">public<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">rep<\/span><span class=\"hl std\">(<\/span><span class=\"hl num\">0<\/span><span class=\"hl opt\">:<\/span><span class=\"hl num\">1<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">each<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">200<\/span><span class=\"hl std\">),<\/span>\r\n  <span class=\"hl kwc\">gpa<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">rep<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwd\">seq<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">from<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">1.9<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">to<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">4<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">length.out<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl num\">100<\/span><span class=\"hl std\">),<\/span> <span class=\"hl num\">4<\/span><span class=\"hl std\">))<\/span>\r\n\r\n<span class=\"hl std\">newdat<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">cbind<\/span><span class=\"hl std\">(newdat,<\/span> <span class=\"hl kwd\">predict<\/span><span class=\"hl std\">(m, newdat,<\/span> <span class=\"hl kwc\">type<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"probs\"<\/span><span class=\"hl std\">))<\/span>\r\n\r\n<span class=\"hl com\">##show first few rows<\/span>\r\n<span class=\"hl kwd\">head<\/span><span class=\"hl std\">(newdat)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   pared public   gpa unlikely somewhat likely very likely\r\n## 1     0      0 1.900   0.7376          0.2205     0.04192\r\n## 2     1      0 1.921   0.4932          0.3946     0.11221\r\n## 3     0      0 1.942   0.7325          0.2245     0.04299\r\n## 4     1      0 1.964   0.4867          0.3985     0.11484\r\n## 5     0      0 1.985   0.7274          0.2285     0.04407\r\n## 6     1      0 2.006   0.4802          0.4023     0.11753\r\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<p>Now we can reshape the data long with the <code>reshape2<\/code> package and plot\nall of the predicted probabilities for the different conditions. We plot the\npredicted probilities, connected with a line, colored by level of the outcome,\n<code>apply<\/code>, and facetted by level of <code>pared<\/code> and <code>public<\/code>. We also\nuse a custom label function, to add clearer labels showing what each column and row\nof the plot represent.<\/p>\n<div id=\"unnamed-chunk-14\" class=\"chunk\">\n<div class=\"rcode\">\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl std\">lnewdat<\/span> <span class=\"hl kwb\">&lt;-<\/span> <span class=\"hl kwd\">melt<\/span><span class=\"hl std\">(newdat,<\/span> <span class=\"hl kwc\">id.vars<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl kwd\">c<\/span><span class=\"hl std\">(<\/span><span class=\"hl str\">\"pared\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"public\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl str\">\"gpa\"<\/span><span class=\"hl std\">),<\/span>\r\n  <span class=\"hl kwc\">variable.name<\/span> <span class=\"hl std\">=<\/span> <span class=\"hl str\">\"Level\"<\/span><span class=\"hl std\">,<\/span> <span class=\"hl kwc\">value.name<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"Probability\"<\/span><span class=\"hl std\">)<\/span>\r\n<span class=\"hl com\">## view first few rows<\/span>\r\n<span class=\"hl kwd\">head<\/span><span class=\"hl std\">(lnewdat)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"output\">\n<pre class=\"knitr r\">##   pared public   gpa    Level Probability\r\n## 1     0      0 1.900 unlikely      0.7376\r\n## 2     1      0 1.921 unlikely      0.4932\r\n## 3     0      0 1.942 unlikely      0.7325\r\n## 4     1      0 1.964 unlikely      0.4867\r\n## 5     0      0 1.985 unlikely      0.7274\r\n## 6     1      0 2.006 unlikely      0.4802\r\n<\/pre>\n<\/div>\n<div class=\"source\">\n<pre class=\"knitr r\"><span class=\"hl kwd\">ggplot<\/span><span class=\"hl std\">(lnewdat,<\/span> <span class=\"hl kwd\">aes<\/span><span class=\"hl std\">(<\/span><span class=\"hl kwc\">x<\/span> <span class=\"hl std\">= gpa,<\/span> <span class=\"hl kwc\">y<\/span> <span class=\"hl std\">= Probability,<\/span> <span class=\"hl kwc\">colour<\/span> <span class=\"hl std\">= Level))<\/span> <span class=\"hl opt\">+<\/span>\r\n  <span class=\"hl kwd\">geom_line<\/span><span class=\"hl std\">()<\/span> <span class=\"hl opt\">+<\/span> <span class=\"hl kwd\">facet_grid<\/span><span class=\"hl std\">(pared<\/span> <span class=\"hl opt\">~<\/span> <span class=\"hl std\">public,<\/span> <span class=\"hl kwc\">labeller<\/span><span class=\"hl std\">=<\/span><span class=\"hl str\">\"label_both\"<\/span><span class=\"hl std\">)<\/span>\r\n<\/pre>\n<\/div>\n<div class=\"rimage default\"><a href=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/gpapublic.png\">\n<img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-27070\" src=\"https:\/\/stats.idre.ucla.edu\/wp-content\/uploads\/2016\/02\/gpapublic.png\" alt=\"predicted probability plot for public versus private by gpa\" width=\"664\" height=\"406\" srcset=\"https:\/\/stats.oarc.ucla.edu\/wp-content\/uploads\/2016\/02\/gpapublic.png 1022w, https:\/\/stats.oarc.ucla.edu\/wp-content\/uploads\/2016\/02\/gpapublic-300x183.png 300w, https:\/\/stats.oarc.ucla.edu\/wp-content\/uploads\/2016\/02\/gpapublic-768x470.png 768w\" sizes=\"auto, (max-width: 664px) 100vw, 664px\" \/><\/a><\/div>\n<h2>Things to consider<\/h2>\n<ul>\n<li>Perfect prediction: Perfect prediction means that one value of a predictor variable is\nassociated with only one value of the response variable. If this\nhappens, Stata will usually issue a note at the top of the output and will\ndrop the cases so that the model can run.<\/li>\n<li>Sample size: Both ordered logistic and ordered probit, using\nmaximum likelihood estimates, require sufficient sample size. How big\nis big is a topic of some debate, but they almost always require more cases than OLS regression.<\/li>\n<li>Empty cells or small cells: You should check for empty or small\ncells by doing a crosstab between categorical predictors and\nthe outcome variable. If a cell has very few cases, the\nmodel may become unstable or it might not run at all.<\/li>\n<li>Pseudo-R-squared: There is no exact analog of the R-squared found\nin OLS. There are many versions of pseudo-R-squares. Please see\nLong and Freese 2005 for more details and explanations of various\npseudo-R-squares.<\/li>\n<li>Diagnostics: Doing diagnostics for non-linear models is difficult, and ordered logit\/probit models are even more difficult than binary models. For a discussion of model diagnostics for logistic regression, see Hosmer and Lemeshow (2000, Chapter 5). Note that diagnostics done for logistic regression are similar to those done for probit regression.<\/li>\n<\/ul>\n<h2>References<\/h2>\n<ul>\n<li>Agresti, A. (1996) <a href=\"\/examples\/icda\/\">An Introduction to Categorical Data\nAnalysis<\/a>. New York: John Wiley &amp; Sons, Inc<\/li>\n<li>Agresti, A. (2002) <a href=\"http:\/\/www.stat.ufl.edu\/%7Eaa\/cda\/cda.html\">Categorical Data Analysis,\nSecond Edition<\/a>. Hoboken, New Jersey: John Wiley &amp; Sons, Inc.<\/li>\n<li>Harrell, F. E, (2001) <a href=\"http:\/\/www.springer.com\/gb\/book\/9781441929181\"> Regression Modeling Strategies<\/a>. New York: Springer-Verlag.<\/li>\n<li>Liao, T. F. (1994) <a href=\"http:\/\/www.sagepub.com\/booksProdDesc.nav?prodId=Book4631&amp;\">Interpreting Probability\nModels: Logit, Probit, and Other Generalized Linear Models<\/a>. Thousand Oaks, CA: Sage Publications, Inc.<\/li>\n<li>Powers, D. and Xie, Yu. Statistical Methods for Categorical Data Analysis.\u00a0 Bingley, UK: Emerald Group Publishing Limited.<\/li>\n<\/ul>\n<\/div>\n<\/div>\n","protected":false},"excerpt":{"rendered":"<p>Introduction The following page discusses how to use R&#8217;s polr package to perform an ordinal logistic regression. For a more mathematical treatment of the interpretation of results refer to:\u00a0How do&#8230;<br><a class=\"moretag\" href=\"https:\/\/stats.oarc.ucla.edu\/r\/dae\/ordinal-logistic-regression\/\"> Read More<\/a><\/p>\n","protected":false},"author":1,"featured_media":0,"parent":916,"menu_order":9,"comment_status":"closed","ping_status":"closed","template":"","meta":{"_genesis_hide_title":false,"_genesis_hide_breadcrumbs":false,"_genesis_hide_singular_image":false,"_genesis_hide_footer_widgets":false,"_genesis_custom_body_class":"","_genesis_custom_post_class":"","_genesis_layout":"","footnotes":""},"class_list":["post-927","page","type-page","status-publish","entry"],"_links":{"self":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/927","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/comments?post=927"}],"version-history":[{"count":55,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/927\/revisions"}],"predecessor-version":[{"id":32708,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/927\/revisions\/32708"}],"up":[{"embeddable":true,"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/pages\/916"}],"wp:attachment":[{"href":"https:\/\/stats.oarc.ucla.edu\/wp-json\/wp\/v2\/media?parent=927"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}